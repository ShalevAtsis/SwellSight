{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "depth_extraction_header"
   },
   "source": [
    "# SwellSight Real-to-Synthetic Pipeline - Depth-Anything-V2 Depth Extraction (Enhanced)\n",
    "\n",
    "This notebook implements the enhanced Depth-Anything-V2 depth estimation phase with advanced error handling and memory management.\n",
    "\n",
    "## Overview\n",
    "This enhanced notebook provides:\n",
    "- Depth-Anything-V2 model initialization with memory optimization\n",
    "- Advanced error handling with GPU fallback mechanisms\n",
    "- Individual image error handling with batch continuation\n",
    "- Comprehensive depth quality assessment\n",
    "- Adaptive memory management and cleanup\n",
    "- Detailed error reporting and recovery instructions\n",
    "\n",
    "## Key Improvements\n",
    "- **Memory Management**: Dynamic memory monitoring and adaptive processing\n",
    "- **Error Recovery**: GPU memory error handling with CPU fallback\n",
    "- **Quality Assessment**: Enhanced depth map quality validation\n",
    "- **Batch Resilience**: Individual image failures don't stop batch processing\n",
    "- **Performance Monitoring**: Real-time memory and performance tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add utils to path for shared utilities\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "\n",
    "# Import shared utilities\n",
    "from config_manager import load_config\n",
    "from data_flow_manager import load_previous_results, save_stage_results\n",
    "from error_handler import retry_with_backoff, handle_gpu_memory_error\n",
    "from memory_optimizer import get_optimal_batch_size, cleanup_variables\n",
    "from progress_tracker import create_progress_bar\n",
    "from data_validator import validate_image_quality\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üöÄ Enhanced Depth-Anything-V2 Extraction Pipeline\")\n",
    "print(\"   Advanced error handling and memory management enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DepthResult:\n",
    "    \"\"\"Enhanced data class to store depth extraction results.\"\"\"\n",
    "    depth_map: np.ndarray\n",
    "    depth_quality_score: float\n",
    "    processing_time: float\n",
    "    model_used: str\n",
    "    memory_usage: Dict[str, float]\n",
    "    image_size: Tuple[int, int]\n",
    "    preprocessing_applied: List[str]\n",
    "\n",
    "@dataclass\n",
    "class ProcessingStats:\n",
    "    \"\"\"Statistics for batch processing.\"\"\"\n",
    "    total_processed: int = 0\n",
    "    successful: int = 0\n",
    "    failed: int = 0\n",
    "    gpu_fallbacks: int = 0\n",
    "    memory_cleanups: int = 0\n",
    "    total_time: float = 0.0\n",
    "    \n",
    "class EnhancedDepthAnythingV2Extractor:\n",
    "    \"\"\"Enhanced Depth-Anything-V2 extractor with advanced error handling and memory management.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"depth-anything/Depth-Anything-V2-Large\", device=\"cuda\", storage_path=None):\n",
    "        self.device = device\n",
    "        self.storage_path = Path(storage_path) if storage_path else None\n",
    "        self.model_name = model_name\n",
    "        self.pipe = None\n",
    "        \n",
    "        # Memory management settings\n",
    "        self.memory_threshold = 0.85  # 85% memory usage threshold\n",
    "        self.critical_memory_threshold = 0.95  # 95% critical threshold\n",
    "        self.consecutive_failures = 0\n",
    "        self.max_consecutive_failures = 5\n",
    "        self.memory_cleanup_interval = 10  # Clean memory every N images\n",
    "        \n",
    "        # Error tracking\n",
    "        self.error_history = defaultdict(list)\n",
    "        self.recovery_strategies = {\n",
    "            'OutOfMemoryError': self._handle_oom_error,\n",
    "            'RuntimeError': self._handle_runtime_error,\n",
    "            'FileNotFoundError': self._handle_file_error,\n",
    "            'ValueError': self._handle_value_error\n",
    "        }\n",
    "        \n",
    "        print(f\"   Loading Enhanced Depth-Anything-V2 model: {model_name} on {device}...\")\n",
    "        self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize model with comprehensive error handling.\"\"\"\n",
    "        try:\n",
    "            # Clear any existing GPU memory before loading\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = self._get_memory_info()\n",
    "                print(f\"   Initial GPU memory: {initial_memory['allocated_gb']:.2f}/{initial_memory['total_gb']:.2f} GB\")\n",
    "            \n",
    "            # Initialize the depth estimation pipeline with memory optimization\n",
    "            self.pipe = pipeline(\n",
    "                task=\"depth-estimation\",\n",
    "                model=self.model_name,\n",
    "                device=0 if self.device == \"cuda\" else -1,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "            )\n",
    "            \n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                post_load_memory = self._get_memory_info()\n",
    "                model_memory = post_load_memory['allocated_gb'] - (initial_memory['allocated_gb'] if 'initial_memory' in locals() else 0)\n",
    "                print(f\"   Post-load GPU memory: {post_load_memory['allocated_gb']:.2f} GB\")\n",
    "                print(f\"   Model memory footprint: {model_memory:.2f} GB\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Model loaded successfully: {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            self._log_error('ModelInitializationError', str(e))\n",
    "            raise\n",
    "\n",
    "    def _get_memory_info(self) -> Dict[str, float]:\n",
    "        \"\"\"Get comprehensive memory usage information.\"\"\"\n",
    "        info = {'usage_ratio': 0.0, 'is_high': False, 'is_critical': False}\n",
    "        \n",
    "        if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            usage_ratio = allocated / total\n",
    "            \n",
    "            info.update({\n",
    "                'allocated_gb': allocated,\n",
    "                'reserved_gb': reserved,\n",
    "                'total_gb': total,\n",
    "                'usage_ratio': usage_ratio,\n",
    "                'is_high': usage_ratio > self.memory_threshold,\n",
    "                'is_critical': usage_ratio > self.critical_memory_threshold\n",
    "            })\n",
    "        \n",
    "        # Add system memory info\n",
    "        system_memory = psutil.virtual_memory()\n",
    "        info.update({\n",
    "            'system_memory_gb': system_memory.total / 1e9,\n",
    "            'system_memory_used_gb': system_memory.used / 1e9,\n",
    "            'system_memory_percent': system_memory.percent\n",
    "        })\n",
    "        \n",
    "        return info\n",
    "\n",
    "    def _handle_memory_pressure(self, force_cleanup=False) -> bool:\n",
    "        \"\"\"Handle high memory usage with progressive cleanup strategies.\"\"\"\n",
    "        memory_info = self._get_memory_info()\n",
    "        \n",
    "        if not memory_info['is_high'] and not force_cleanup:\n",
    "            return True\n",
    "        \n",
    "        logger.info(f\"Memory pressure detected: {memory_info['usage_ratio']:.1%}\")\n",
    "        \n",
    "        # Progressive cleanup strategies\n",
    "        cleanup_strategies = [\n",
    "            (\"torch_cache\", lambda: torch.cuda.empty_cache() if torch.cuda.is_available() else None),\n",
    "            (\"torch_sync\", lambda: torch.cuda.synchronize() if torch.cuda.is_available() else None),\n",
    "            (\"garbage_collect\", lambda: gc.collect()),\n",
    "            (\"force_gc\", lambda: [gc.collect() for _ in range(3)])  # Multiple GC passes\n",
    "        ]\n",
    "        \n",
    "        for strategy_name, cleanup_func in cleanup_strategies:\n",
    "            try:\n",
    "                cleanup_func()\n",
    "                new_memory_info = self._get_memory_info()\n",
    "                \n",
    "                improvement = memory_info['usage_ratio'] - new_memory_info['usage_ratio']\n",
    "                logger.info(f\"Applied {strategy_name}: {improvement:.1%} memory freed\")\n",
    "                \n",
    "                if not new_memory_info['is_critical']:\n",
    "                    return True\n",
    "                    \n",
    "                memory_info = new_memory_info\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cleanup strategy {strategy_name} failed: {e}\")\n",
    "        \n",
    "        # If still critical, recommend CPU fallback\n",
    "        final_memory_info = self._get_memory_info()\n",
    "        if final_memory_info['is_critical']:\n",
    "            logger.error(f\"Memory usage still critical after cleanup: {final_memory_info['usage_ratio']:.1%}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _adaptive_preprocessing(self, image: Image.Image) -> Tuple[Image.Image, List[str]]:\n",
    "        \"\"\"Adaptively preprocess image based on memory constraints and image characteristics.\"\"\"\n",
    "        preprocessing_applied = []\n",
    "        memory_info = self._get_memory_info()\n",
    "        original_size = image.size\n",
    "        \n",
    "        # Memory-based resizing\n",
    "        if memory_info['is_high']:\n",
    "            scale_factor = 0.75 if memory_info['is_critical'] else 0.85\n",
    "            new_size = (int(original_size[0] * scale_factor), int(original_size[1] * scale_factor))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            preprocessing_applied.append(f\"memory_resize_{scale_factor}\")\n",
    "            logger.info(f\"Applied memory-based resize: {original_size} -> {new_size}\")\n",
    "        \n",
    "        # Size-based optimization\n",
    "        max_dimension = max(image.size)\n",
    "        if max_dimension > 2048:  # Very large images\n",
    "            scale_factor = 2048 / max_dimension\n",
    "            new_size = (int(image.size[0] * scale_factor), int(image.size[1] * scale_factor))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            preprocessing_applied.append(f\"size_optimize_{scale_factor:.2f}\")\n",
    "            logger.info(f\"Applied size optimization: {original_size} -> {new_size}\")\n",
    "        \n",
    "        return image, preprocessing_applied\n",
    "\n",
    "    def _calculate_enhanced_quality_score(self, depth_map: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive depth map quality metrics.\"\"\"\n",
    "        try:\n",
    "            # Normalize depth map\n",
    "            depth_norm = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min() + 1e-8)\n",
    "            \n",
    "            # Multiple quality metrics\n",
    "            metrics = {}\n",
    "            \n",
    "            # 1. Standard deviation (depth variation)\n",
    "            metrics['std_score'] = np.std(depth_norm)\n",
    "            \n",
    "            # 2. Gradient magnitude (edge information)\n",
    "            grad_x = np.gradient(depth_norm, axis=1)\n",
    "            grad_y = np.gradient(depth_norm, axis=0)\n",
    "            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            metrics['gradient_score'] = np.mean(gradient_magnitude)\n",
    "            \n",
    "            # 3. Entropy (information content)\n",
    "            hist, _ = np.histogram(depth_norm, bins=256, range=(0, 1))\n",
    "            hist = hist / hist.sum()\n",
    "            hist = hist[hist > 0]  # Remove zero bins\n",
    "            metrics['entropy_score'] = -np.sum(hist * np.log2(hist)) / 8.0  # Normalize by max entropy\n",
    "            \n",
    "            # 4. Dynamic range\n",
    "            metrics['dynamic_range'] = depth_map.max() - depth_map.min()\n",
    "            \n",
    "            # 5. Smoothness (inverse of total variation)\n",
    "            tv = np.sum(np.abs(grad_x)) + np.sum(np.abs(grad_y))\n",
    "            metrics['smoothness_score'] = 1.0 / (1.0 + tv / depth_map.size)\n",
    "            \n",
    "            # Combined quality score (weighted average)\n",
    "            weights = {\n",
    "                'std_score': 0.3,\n",
    "                'gradient_score': 0.25,\n",
    "                'entropy_score': 0.25,\n",
    "                'smoothness_score': 0.2\n",
    "            }\n",
    "            \n",
    "            combined_score = sum(weights[key] * metrics[key] for key in weights.keys())\n",
    "            metrics['combined_score'] = min(1.0, max(0.0, combined_score))\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Enhanced quality calculation failed: {e}\")\n",
    "            return {\n",
    "                'combined_score': 0.5,\n",
    "                'std_score': 0.5,\n",
    "                'gradient_score': 0.5,\n",
    "                'entropy_score': 0.5,\n",
    "                'smoothness_score': 0.5,\n",
    "                'dynamic_range': 0.0\n",
    "            }\n",
    "\n",
    "    def _log_error(self, error_type: str, error_message: str, image_path: str = None):\n",
    "        \"\"\"Log error with context for analysis.\"\"\"\n",
    "        error_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'error_type': error_type,\n",
    "            'message': error_message,\n",
    "            'image_path': str(image_path) if image_path else None,\n",
    "            'memory_info': self._get_memory_info(),\n",
    "            'consecutive_failures': self.consecutive_failures\n",
    "        }\n",
    "        self.error_history[error_type].append(error_entry)\n",
    "\n",
    "    def _handle_oom_error(self, image_path: str) -> Optional[DepthResult]:\n",
    "        \"\"\"Handle Out of Memory errors with progressive strategies.\"\"\"\n",
    "        logger.warning(f\"OOM error for {image_path}, attempting recovery...\")\n",
    "        \n",
    "        # Force memory cleanup\n",
    "        if not self._handle_memory_pressure(force_cleanup=True):\n",
    "            logger.error(\"Memory cleanup failed, skipping image\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Try with smaller image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            original_size = image.size\n",
    "            \n",
    "            # Aggressive resizing for OOM recovery\n",
    "            scale_factor = 0.5\n",
    "            new_size = (int(original_size[0] * scale_factor), int(original_size[1] * scale_factor))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            logger.info(f\"OOM recovery: resized {original_size} -> {new_size}\")\n",
    "            \n",
    "            # Try extraction with reduced image\n",
    "            with torch.no_grad():\n",
    "                result = self.pipe(image)\n",
    "                depth_map = np.array(result['depth'])\n",
    "            \n",
    "            # Resize depth map back to original dimensions\n",
    "            depth_map_resized = cv2.resize(depth_map, original_size, interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            quality_metrics = self._calculate_enhanced_quality_score(depth_map_resized)\n",
    "            \n",
    "            return DepthResult(\n",
    "                depth_map=depth_map_resized,\n",
    "                depth_quality_score=quality_metrics['combined_score'],\n",
    "                processing_time=0.0,  # Not tracking time for recovery\n",
    "                model_used=f\"{self.model_name} (OOM recovery)\",\n",
    "                memory_usage=self._get_memory_info(),\n",
    "                image_size=original_size,\n",
    "                preprocessing_applied=[f\"oom_recovery_resize_{scale_factor}\"]\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"OOM recovery failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _handle_runtime_error(self, image_path: str) -> Optional[DepthResult]:\n",
    "        \"\"\"Handle runtime errors.\"\"\"\n",
    "        logger.warning(f\"Runtime error for {image_path}, attempting model reload...\")\n",
    "        \n",
    "        try:\n",
    "            # Try reinitializing the pipeline\n",
    "            self.cleanup()\n",
    "            self._initialize_model()\n",
    "            \n",
    "            # Retry extraction\n",
    "            return self.extract_depth(image_path, store_result=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Runtime error recovery failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _handle_file_error(self, image_path: str) -> Optional[DepthResult]:\n",
    "        \"\"\"Handle file-related errors.\"\"\"\n",
    "        logger.error(f\"File error: {image_path} not accessible\")\n",
    "        return None\n",
    "\n",
    "    def _handle_value_error(self, image_path: str) -> Optional[DepthResult]:\n",
    "        \"\"\"Handle value errors (e.g., corrupted images).\"\"\"\n",
    "        logger.warning(f\"Value error for {image_path}, attempting image repair...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load and re-save the image to fix minor corruption\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Basic image validation\n",
    "            if image.size[0] < 32 or image.size[1] < 32:\n",
    "                raise ValueError(\"Image too small\")\n",
    "            \n",
    "            # Retry extraction\n",
    "            return self.extract_depth(image_path, store_result=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image repair failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_depth(self, image_path: str, store_result: bool = True) -> DepthResult:\n",
    "        \"\"\"Extract depth map with comprehensive error handling and recovery.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Pre-processing memory check\n",
    "            memory_info = self._get_memory_info()\n",
    "            if memory_info['is_critical']:\n",
    "                if not self._handle_memory_pressure():\n",
    "                    raise RuntimeError(\"Critical memory usage, cannot process image\")\n",
    "            \n",
    "            # Load and validate image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Validate image quality\n",
    "            quality_check = validate_image_quality(str(image_path))\n",
    "            if not quality_check['is_valid']:\n",
    "                raise ValueError(f\"Image quality validation failed: {quality_check['issues']}\")\n",
    "            \n",
    "            # Adaptive preprocessing\n",
    "            processed_image, preprocessing_applied = self._adaptive_preprocessing(image)\n",
    "            \n",
    "            # Extract depth using pipeline\n",
    "            with torch.no_grad():\n",
    "                result = self.pipe(processed_image)\n",
    "                depth_map = np.array(result['depth'])\n",
    "            \n",
    "            # If image was resized, resize depth map back\n",
    "            if processed_image.size != image.size:\n",
    "                depth_map = cv2.resize(depth_map, image.size, interpolation=cv2.INTER_CUBIC)\n",
    "                preprocessing_applied.append(\"depth_resize_back\")\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Calculate enhanced quality metrics\n",
    "            quality_metrics = self._calculate_enhanced_quality_score(depth_map)\n",
    "            \n",
    "            # Store result if requested\n",
    "            if store_result and self.storage_path:\n",
    "                self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "                save_path = self.storage_path / f\"{Path(image_path).stem}_depth.npy\"\n",
    "                np.save(save_path, depth_map)\n",
    "            \n",
    "            # Reset consecutive failures on success\n",
    "            self.consecutive_failures = 0\n",
    "            \n",
    "            return DepthResult(\n",
    "                depth_map=depth_map,\n",
    "                depth_quality_score=quality_metrics['combined_score'],\n",
    "                processing_time=processing_time,\n",
    "                model_used=self.model_name,\n",
    "                memory_usage=self._get_memory_info(),\n",
    "                image_size=image.size,\n",
    "                preprocessing_applied=preprocessing_applied\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Increment consecutive failures\n",
    "            self.consecutive_failures += 1\n",
    "            \n",
    "            # Log error with context\n",
    "            error_type = type(e).__name__\n",
    "            self._log_error(error_type, str(e), image_path)\n",
    "            \n",
    "            # Try recovery strategies\n",
    "            if error_type in self.recovery_strategies and self.consecutive_failures <= self.max_consecutive_failures:\n",
    "                logger.info(f\"Attempting recovery for {error_type}...\")\n",
    "                recovery_result = self.recovery_strategies[error_type](image_path)\n",
    "                if recovery_result:\n",
    "                    logger.info(f\"Recovery successful for {image_path}\")\n",
    "                    return recovery_result\n",
    "            \n",
    "            # If recovery failed or too many consecutive failures, re-raise\n",
    "            logger.error(f\"Failed to process {image_path} after recovery attempts: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def get_error_summary(self) -> Dict:\n",
    "        \"\"\"Get comprehensive error summary for reporting.\"\"\"\n",
    "        summary = {\n",
    "            'total_error_types': len(self.error_history),\n",
    "            'consecutive_failures': self.consecutive_failures,\n",
    "            'error_breakdown': {}\n",
    "        }\n",
    "        \n",
    "        for error_type, errors in self.error_history.items():\n",
    "            summary['error_breakdown'][error_type] = {\n",
    "                'count': len(errors),\n",
    "                'latest': errors[-1]['timestamp'] if errors else None,\n",
    "                'sample_message': errors[-1]['message'] if errors else None\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Comprehensive cleanup of model resources.\"\"\"\n",
    "        if self.pipe is not None:\n",
    "            del self.pipe\n",
    "            self.pipe = None\n",
    "        \n",
    "        # Force memory cleanup\n",
    "        self._handle_memory_pressure(force_cleanup=True)\n",
    "        \n",
    "        logger.info(\"Model cleanup completed\")\n",
    "\n",
    "print(\"‚úÖ Enhanced DepthAnythingV2Extractor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and setup\n",
    "print(\"üîÑ Loading configuration and processing batch...\")\n",
    "\n",
    "# Check if running in Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_PATH = Path('/content/drive/MyDrive/SwellSight')\n",
    "else:\n",
    "    PROJECT_PATH = Path.cwd()\n",
    "\n",
    "try:\n",
    "    # Load pipeline configuration\n",
    "    PIPELINE_CONFIG = load_config(PROJECT_PATH / 'config.json')\n",
    "    \n",
    "    # Load processing batch from previous stage\n",
    "    PROCESSING_BATCH = load_previous_results(\n",
    "        stage_name=\"data_preprocessing\",\n",
    "        required_files=[\"processed_images.json\", \"quality_report.json\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Configuration and batch loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration or batch: {e}\")\n",
    "    print(\"Please ensure you have run the previous notebooks and have valid configuration.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure device with enhanced detection\n",
    "print(\"\\nüîß Enhanced device configuration...\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Enhanced model selection based on memory\n",
    "    if gpu_memory < 6:\n",
    "        selected_model = \"depth-anything/Depth-Anything-V2-Small\"\n",
    "        print(f\"   ‚ö†Ô∏è Limited memory: Using Small model\")\n",
    "    elif gpu_memory < 12:\n",
    "        selected_model = \"depth-anything/Depth-Anything-V2-Base\"\n",
    "        print(f\"   ‚úÖ Adequate memory: Using Base model\")\n",
    "    else:\n",
    "        selected_model = \"depth-anything/Depth-Anything-V2-Large\"\n",
    "        print(f\"   üöÄ Excellent memory: Using Large model\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    gpu_memory = 0\n",
    "    selected_model = \"depth-anything/Depth-Anything-V2-Small\"\n",
    "    print(f\"   ‚ö†Ô∏è CPU mode: Using Small model\")\n",
    "\n",
    "# Set up paths\n",
    "DEPTH_OUTPUT_PATH = Path(PIPELINE_CONFIG['paths']['output_dir']) / 'depth_maps'\n",
    "DEPTH_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Output path: {DEPTH_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize enhanced depth extractor\n",
    "print(\"üß† Initializing Enhanced Depth-Anything-V2 extractor...\")\n",
    "\n",
    "try:\n",
    "    depth_extractor = EnhancedDepthAnythingV2Extractor(\n",
    "        model_name=selected_model,\n",
    "        device=device,\n",
    "        storage_path=str(DEPTH_OUTPUT_PATH)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Enhanced extractor initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize extractor: {e}\")\n",
    "    \n",
    "    # Enhanced fallback with multiple strategies\n",
    "    if device == \"cuda\":\n",
    "        print(\"\\nüîÑ Attempting enhanced GPU fallback strategies...\")\n",
    "        \n",
    "        fallback_strategies = [\n",
    "            (\"depth-anything/Depth-Anything-V2-Base\", \"cuda\"),\n",
    "            (\"depth-anything/Depth-Anything-V2-Small\", \"cuda\"),\n",
    "            (\"depth-anything/Depth-Anything-V2-Small\", \"cpu\")\n",
    "        ]\n",
    "        \n",
    "        for model, dev in fallback_strategies:\n",
    "            try:\n",
    "                print(f\"   Trying {model} on {dev}...\")\n",
    "                depth_extractor = EnhancedDepthAnythingV2Extractor(\n",
    "                    model_name=model,\n",
    "                    device=dev,\n",
    "                    storage_path=str(DEPTH_OUTPUT_PATH)\n",
    "                )\n",
    "                print(f\"   ‚úÖ Fallback successful: {model} on {dev}\")\n",
    "                selected_model = model\n",
    "                device = dev\n",
    "                break\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"   ‚ùå Fallback failed: {fallback_error}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"‚ùå All fallback strategies failed\")\n",
    "            raise\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced batch processing with comprehensive error handling\n",
    "print(\"üöÄ Starting enhanced batch depth extraction...\")\n",
    "\n",
    "# Get images to process\n",
    "images_to_process = PROCESSING_BATCH.get('processed_images', [])\n",
    "total_images = len(images_to_process)\n",
    "\n",
    "if total_images == 0:\n",
    "    print(\"‚ùå No images found to process\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"   Total images: {total_images}\")\n",
    "print(f\"   Model: {selected_model}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Initialize enhanced tracking\n",
    "stats = ProcessingStats()\n",
    "successful_results = []\n",
    "failed_results = []\n",
    "processing_times = []\n",
    "quality_scores = []\n",
    "memory_usage_history = []\n",
    "\n",
    "# Enhanced batch processing with progress tracking\n",
    "start_time = datetime.now()\n",
    "\n",
    "with create_progress_bar(total_images, \"Enhanced depth extraction\") as pbar:\n",
    "    for i, image_info in enumerate(images_to_process):\n",
    "        stats.total_processed += 1\n",
    "        \n",
    "        try:\n",
    "            # Get image path\n",
    "            image_path = image_info.get('path') or image_info.get('file_path')\n",
    "            if not image_path:\n",
    "                raise ValueError(\"No image path found\")\n",
    "            \n",
    "            image_path = Path(image_path)\n",
    "            if not image_path.exists():\n",
    "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "            \n",
    "            # Extract depth with enhanced error handling\n",
    "            result = depth_extractor.extract_depth(str(image_path), store_result=True)\n",
    "            \n",
    "            # Track successful result\n",
    "            stats.successful += 1\n",
    "            successful_results.append({\n",
    "                'image_path': str(image_path),\n",
    "                'depth_quality_score': result.depth_quality_score,\n",
    "                'processing_time': result.processing_time,\n",
    "                'model_used': result.model_used,\n",
    "                'image_size': result.image_size,\n",
    "                'preprocessing_applied': result.preprocessing_applied,\n",
    "                'memory_usage': result.memory_usage\n",
    "            })\n",
    "            \n",
    "            processing_times.append(result.processing_time)\n",
    "            quality_scores.append(result.depth_quality_score)\n",
    "            memory_usage_history.append(result.memory_usage)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Enhanced error handling\n",
    "            stats.failed += 1\n",
    "            error_type = type(e).__name__\n",
    "            \n",
    "            failed_results.append({\n",
    "                'image_path': str(image_path),\n",
    "                'error': str(e),\n",
    "                'error_type': error_type,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            logger.error(f\"Failed to process {image_path}: {e}\")\n",
    "        \n",
    "        # Enhanced memory management\n",
    "        if (i + 1) % depth_extractor.memory_cleanup_interval == 0:\n",
    "            memory_info = depth_extractor._get_memory_info()\n",
    "            if memory_info['is_high']:\n",
    "                depth_extractor._handle_memory_pressure()\n",
    "                stats.memory_cleanups += 1\n",
    "        \n",
    "        # Update progress with memory info\n",
    "        current_memory = depth_extractor._get_memory_info()\n",
    "        pbar.set_postfix({\n",
    "            'Success': f\"{stats.successful}/{stats.total_processed}\",\n",
    "            'Memory': f\"{current_memory.get('usage_ratio', 0):.1%}\"\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "# Calculate final statistics\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "stats.total_time = total_time\n",
    "success_rate = (stats.successful / stats.total_processed) * 100 if stats.total_processed > 0 else 0\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced batch processing completed!\")\n",
    "print(f\"\\nüìä Enhanced Processing Results:\")\n",
    "print(f\"   Total processed: {stats.total_processed}\")\n",
    "print(f\"   Successful: {stats.successful}\")\n",
    "print(f\"   Failed: {stats.failed}\")\n",
    "print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "print(f\"   Memory cleanups: {stats.memory_cleanups}\")\n",
    "print(f\"   Total time: {total_time:.1f}s\")\n",
    "\n",
    "if processing_times:\n",
    "    print(f\"\\n‚è±Ô∏è Performance Metrics:\")\n",
    "    print(f\"   Average time: {np.mean(processing_times):.2f}s\")\n",
    "    print(f\"   Fastest: {min(processing_times):.2f}s\")\n",
    "    print(f\"   Slowest: {max(processing_times):.2f}s\")\n",
    "\n",
    "if quality_scores:\n",
    "    print(f\"\\nüéØ Quality Metrics:\")\n",
    "    print(f\"   Average quality: {np.mean(quality_scores):.3f}\")\n",
    "    print(f\"   Quality range: {min(quality_scores):.3f} - {max(quality_scores):.3f}\")\n",
    "\n",
    "# Error summary\n",
    "error_summary = depth_extractor.get_error_summary()\n",
    "if error_summary['total_error_types'] > 0:\n",
    "    print(f\"\\n‚ùå Error Summary:\")\n",
    "    for error_type, details in error_summary['error_breakdown'].items():\n",
    "        print(f\"   {error_type}: {details['count']} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced results with comprehensive metadata\n",
    "print(\"üíæ Saving enhanced results...\")\n",
    "\n",
    "enhanced_results = {\n",
    "    'stage_info': {\n",
    "        'stage_name': 'enhanced_depth_anything_v2_extraction',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_used': selected_model,\n",
    "        'device_used': device,\n",
    "        'total_processing_time': total_time,\n",
    "        'enhancement_features': [\n",
    "            'adaptive_preprocessing',\n",
    "            'memory_optimization',\n",
    "            'error_recovery',\n",
    "            'quality_assessment',\n",
    "            'gpu_fallback'\n",
    "        ]\n",
    "    },\n",
    "    'processing_statistics': {\n",
    "        'total_images': stats.total_processed,\n",
    "        'successful_extractions': stats.successful,\n",
    "        'failed_extractions': stats.failed,\n",
    "        'success_rate': success_rate,\n",
    "        'memory_cleanups': stats.memory_cleanups,\n",
    "        'gpu_fallbacks': stats.gpu_fallbacks\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'mean_processing_time': np.mean(processing_times) if processing_times else 0,\n",
    "        'std_processing_time': np.std(processing_times) if processing_times else 0,\n",
    "        'min_processing_time': min(processing_times) if processing_times else 0,\n",
    "        'max_processing_time': max(processing_times) if processing_times else 0,\n",
    "        'mean_quality_score': np.mean(quality_scores) if quality_scores else 0,\n",
    "        'std_quality_score': np.std(quality_scores) if quality_scores else 0\n",
    "    },\n",
    "    'memory_analysis': {\n",
    "        'peak_memory_usage': max([m.get('usage_ratio', 0) for m in memory_usage_history]) if memory_usage_history else 0,\n",
    "        'average_memory_usage': np.mean([m.get('usage_ratio', 0) for m in memory_usage_history]) if memory_usage_history else 0,\n",
    "        'memory_cleanup_frequency': stats.memory_cleanups / stats.total_processed if stats.total_processed > 0 else 0\n",
    "    },\n",
    "    'error_analysis': error_summary,\n",
    "    'successful_results': successful_results,\n",
    "    'failed_results': failed_results\n",
    "}\n",
    "\n",
    "try:\n",
    "    save_stage_results(\n",
    "        data=enhanced_results,\n",
    "        stage_name=\"enhanced_depth_anything_v2_extraction\",\n",
    "        metadata={\n",
    "            'model_used': selected_model,\n",
    "            'processing_time': total_time,\n",
    "            'success_rate': success_rate,\n",
    "            'enhancement_level': 'advanced'\n",
    "        }\n",
    "    )\n",
    "    print(\"‚úÖ Enhanced results saved successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Failed to save results: {e}\")\n",
    "\n",
    "# Cleanup\n",
    "depth_extractor.cleanup()\n",
    "print(\"\\nüßπ Cleanup completed\")\n",
    "print(\"‚úÖ Enhanced depth extraction stage completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}