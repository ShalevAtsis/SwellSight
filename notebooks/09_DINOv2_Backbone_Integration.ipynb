{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SwellSight Wave Analysis - DINOv2 Backbone Integration\n",
        "\n",
        "This notebook implements DINOv2 self-supervised vision transformer as the feature extraction backbone for wave analysis.\n",
        "\n",
        "## Overview\n",
        "This notebook provides:\n",
        "- DINOv2 ViT-L/14 model loading and configuration\n",
        "- 4-channel input adaptation (RGB + Depth)\n",
        "- Feature extraction with frozen backbone\n",
        "- Feature quality validation and visualization\n",
        "- Integration tests with beach cam images\n",
        "\n",
        "## DINOv2 Architecture\n",
        "- **Model**: Vision Transformer Large (ViT-L/14)\n",
        "- **Input**: 4 channels (RGB + Depth), 518x518 resolution\n",
        "- **Output**: 1024-dimensional feature vectors\n",
        "- **Training**: Frozen backbone (preserves pre-trained knowledge)\n",
        "\n",
        "## Prerequisites\n",
        "- Complete execution of notebooks 01-05\n",
        "- Depth maps available from notebook 03\n",
        "- Beach cam images available\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Add src to path for production modules\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "print(\"\ud83d\udce6 Importing SwellSight production modules...\")\n",
        "\n",
        "# Import production modules\n",
        "from src.swellsight.models.backbone import DINOv2Backbone\n",
        "from src.swellsight.utils.hardware import HardwareManager\n",
        "from src.swellsight.utils.error_handler import error_handler, retry_with_backoff\n",
        "from src.swellsight.utils.performance import PerformanceOptimizer\n",
        "from src.swellsight.utils.config import load_config\n",
        "\n",
        "print(\"\u2705 Production modules loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"\\n\ud83d\udd27 Loading configuration...\")\n",
        "\n",
        "# Load pipeline configuration\n",
        "config = load_config(\"config.json\")\n",
        "\n",
        "print(f\"\u2705 Configuration loaded: {config['pipeline']['name']}\")\n",
        "print(f\"   Version: {config['pipeline']['version']}\")\n",
        "\n",
        "# Set up paths\n",
        "DATA_DIR = Path(config['paths']['data_dir'])\n",
        "OUTPUT_DIR = Path(config['paths']['output_dir'])\n",
        "DEPTH_MAPS_DIR = OUTPUT_DIR / \"depth_maps\"\n",
        "FEATURES_DIR = OUTPUT_DIR / \"dinov2_features\"\n",
        "\n",
        "# Create output directory\n",
        "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n\ud83d\udcc1 Working directories:\")\n",
        "print(f\"   Data: {DATA_DIR}\")\n",
        "print(f\"   Depth maps: {DEPTH_MAPS_DIR}\")\n",
        "print(f\"   Features output: {FEATURES_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hardware Detection and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd0d Detecting hardware configuration...\")\n",
        "\n",
        "# Initialize hardware manager\n",
        "hardware_manager = HardwareManager()\n",
        "hw_info = hardware_manager.hardware_info\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 Hardware Configuration:\")\n",
        "print(f\"   Device: {hw_info.device_type}\")\n",
        "print(f\"   Name: {hw_info.device_name}\")\n",
        "print(f\"   Memory: {hw_info.memory_total_gb:.1f} GB total\")\n",
        "\n",
        "if hw_info.device_type == \"cuda\":\n",
        "    print(f\"   CUDA Version: {hw_info.cuda_version}\")\n",
        "    print(f\"   Compute Capability: {hw_info.compute_capability}\")\n",
        "    \n",
        "    # Check memory requirements for DINOv2\n",
        "    if hw_info.memory_total_gb < 8:\n",
        "        print(\"\\n\u26a0\ufe0f  Warning: Less than 8GB GPU memory\")\n",
        "        print(\"   Consider using smaller model variant or CPU\")\n",
        "    else:\n",
        "        print(\"\\n\u2705 Sufficient GPU memory for DINOv2 ViT-L/14\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  Running on CPU - processing will be slower\")\n",
        "\n",
        "# Store device configuration\n",
        "device = hw_info.device_type\n",
        "print(f\"\\n\u2705 Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sub-task 7.1: DINOv2 Backbone Loading and Adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83e\udde0 Sub-task 7.1: Loading DINOv2 backbone...\")\n",
        "print(\"\\nInitializing DINOv2 ViT-L/14 with:\")\n",
        "print(\"   - Model: dinov2_vitl14\")\n",
        "print(\"   - Feature dimension: 1024\")\n",
        "print(\"   - Frozen backbone: True\")\n",
        "print(\"   - Input channels: 4 (RGB + Depth)\")\n",
        "\n",
        "try:\n",
        "    # Initialize DINOv2 backbone\n",
        "    backbone = DINOv2Backbone(\n",
        "        model_name=\"dinov2_vitl14\",\n",
        "        freeze=True\n",
        "    )\n",
        "    \n",
        "    # Move to device\n",
        "    backbone = backbone.to(device)\n",
        "    backbone.eval()  # Set to evaluation mode\n",
        "    \n",
        "    print(\"\\n\u2705 DINOv2 backbone loaded successfully!\")\n",
        "    print(f\"   Feature dimension: {backbone.get_feature_dim()}\")\n",
        "    print(f\"   Input channels: {backbone.input_channels}\")\n",
        "    print(f\"   Target resolution: {backbone.target_size}\")\n",
        "    print(f\"   Frozen: {backbone.freeze}\")\n",
        "    \n",
        "    # Verify backbone is frozen\n",
        "    trainable_params = sum(p.numel() for p in backbone._backbone.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in backbone._backbone.parameters())\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Parameter Status:\")\n",
        "    print(f\"   Total parameters: {total_params:,}\")\n",
        "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"   Frozen parameters: {total_params - trainable_params:,}\")\n",
        "    \n",
        "    if trainable_params == 0:\n",
        "        print(\"   \u2705 Backbone is properly frozen\")\n",
        "    else:\n",
        "        print(f\"   \u26a0\ufe0f  Warning: {trainable_params} parameters are trainable\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Failed to load DINOv2 backbone: {e}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"   1. Check internet connection for model download\")\n",
        "    print(\"   2. Verify torch hub cache directory\")\n",
        "    print(\"   3. Try clearing torch hub cache\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sub-task 7.2: 4-Channel Input Adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd27 Sub-task 7.2: Testing 4-channel input adaptation...\")\n",
        "\n",
        "# Create test input (RGB + Depth)\n",
        "batch_size = 2\n",
        "test_input = torch.randn(batch_size, 4, 518, 518).to(device)\n",
        "\n",
        "print(f\"\\nTest input shape: {test_input.shape}\")\n",
        "print(f\"   Batch size: {test_input.shape[0]}\")\n",
        "print(f\"   Channels: {test_input.shape[1]} (RGB + Depth)\")\n",
        "print(f\"   Resolution: {test_input.shape[2]}x{test_input.shape[3]}\")\n",
        "\n",
        "try:\n",
        "    # Test forward pass\n",
        "    with torch.no_grad():\n",
        "        features = backbone(test_input)\n",
        "    \n",
        "    print(f\"\\n\u2705 4-channel input adaptation successful!\")\n",
        "    print(f\"   Output shape: {features.shape}\")\n",
        "    print(f\"   Feature dimension: {features.shape[1]}\")\n",
        "    \n",
        "    # Verify feature dimension\n",
        "    expected_dim = 1024\n",
        "    if features.shape[1] == expected_dim:\n",
        "        print(f\"   \u2705 Feature dimension matches expected: {expected_dim}\")\n",
        "    else:\n",
        "        print(f\"   \u26a0\ufe0f  Feature dimension mismatch: got {features.shape[1]}, expected {expected_dim}\")\n",
        "    \n",
        "    # Test with different resolutions\n",
        "    print(\"\\n\ud83d\udd0d Testing automatic resizing...\")\n",
        "    test_resolutions = [(256, 256), (512, 512), (1024, 1024)]\n",
        "    \n",
        "    for h, w in test_resolutions:\n",
        "        test_input_resized = torch.randn(1, 4, h, w).to(device)\n",
        "        with torch.no_grad():\n",
        "            features_resized = backbone(test_input_resized)\n",
        "        print(f\"   Input {h}x{w} -> Output {features_resized.shape[1]}-dim: \u2705\")\n",
        "    \n",
        "    print(\"\\n\u2705 All resolution tests passed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c 4-channel input adaptation failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sub-task 7.3: Feature Extraction and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd0d Sub-task 7.3: Feature extraction with real beach cam images...\")\n",
        "\n",
        "# Find available beach cam images and depth maps\n",
        "image_dir = DATA_DIR / \"real\" / \"images\"\n",
        "image_files = list(image_dir.glob(\"*.jpg\"))[:10]  # Process first 10 images\n",
        "\n",
        "if not image_files:\n",
        "    print(\"\u274c No beach cam images found\")\n",
        "    print(f\"   Please ensure images are in: {image_dir}\")\n",
        "else:\n",
        "    print(f\"\\n\ud83d\udcca Found {len(image_files)} beach cam images\")\n",
        "    print(f\"   Processing first {min(len(image_files), 10)} images...\")\n",
        "    \n",
        "    # Storage for results\n",
        "    extraction_results = []\n",
        "    feature_vectors = []\n",
        "    \n",
        "    print(\"\\n\ud83d\ude80 Extracting features...\")\n",
        "    \n",
        "    for img_path in tqdm(image_files, desc=\"Extracting features\"):\n",
        "        try:\n",
        "            # Load image\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image_np = np.array(image)\n",
        "            \n",
        "            # Load corresponding depth map\n",
        "            depth_path = DEPTH_MAPS_DIR / f\"{img_path.stem}_depth.npy\"\n",
        "            \n",
        "            if not depth_path.exists():\n",
        "                print(f\"\\n\u26a0\ufe0f  Depth map not found for {img_path.name}, skipping...\")\n",
        "                continue\n",
        "            \n",
        "            depth_map = np.load(depth_path)\n",
        "            \n",
        "            # Prepare 4-channel input (RGB + Depth)\n",
        "            # Resize image to match depth map if needed\n",
        "            if image_np.shape[:2] != depth_map.shape:\n",
        "                from PIL import Image as PILImage\n",
        "                image_resized = PILImage.fromarray(image_np).resize(\n",
        "                    (depth_map.shape[1], depth_map.shape[0]),\n",
        "                    PILImage.BILINEAR\n",
        "                )\n",
        "                image_np = np.array(image_resized)\n",
        "            \n",
        "            # Normalize image to [0, 1]\n",
        "            image_normalized = image_np.astype(np.float32) / 255.0\n",
        "            \n",
        "            # Normalize depth to [0, 1]\n",
        "            depth_normalized = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min() + 1e-8)\n",
        "            depth_normalized = depth_normalized.astype(np.float32)\n",
        "            \n",
        "            # Stack RGB + Depth\n",
        "            rgbd_input = np.concatenate([\n",
        "                image_normalized,\n",
        "                depth_normalized[..., np.newaxis]\n",
        "            ], axis=-1)\n",
        "            \n",
        "            # Convert to tensor [1, 4, H, W]\n",
        "            rgbd_tensor = torch.from_numpy(rgbd_input).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                features = backbone(rgbd_tensor)\n",
        "            \n",
        "            # Store results\n",
        "            features_np = features.cpu().numpy()[0]\n",
        "            feature_vectors.append(features_np)\n",
        "            \n",
        "            extraction_results.append({\n",
        "                'image_path': str(img_path),\n",
        "                'depth_path': str(depth_path),\n",
        "                'feature_shape': features.shape,\n",
        "                'feature_mean': float(features_np.mean()),\n",
        "                'feature_std': float(features_np.std()),\n",
        "                'feature_min': float(features_np.min()),\n",
        "                'feature_max': float(features_np.max())\n",
        "            })\n",
        "            \n",
        "            # Save features\n",
        "            feature_save_path = FEATURES_DIR / f\"{img_path.stem}_features.npy\"\n",
        "            np.save(feature_save_path, features_np)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to process {img_path.name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n\u2705 Feature extraction completed!\")\n",
        "    print(f\"   Processed: {len(extraction_results)} images\")\n",
        "    print(f\"   Features saved to: {FEATURES_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Quality Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if extraction_results:\n",
        "    print(\"\ud83d\udcca Feature Quality Validation...\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    feature_means = [r['feature_mean'] for r in extraction_results]\n",
        "    feature_stds = [r['feature_std'] for r in extraction_results]\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcc8 Feature Statistics:\")\n",
        "    print(f\"   Mean across images: {np.mean(feature_means):.4f} \u00b1 {np.std(feature_means):.4f}\")\n",
        "    print(f\"   Std across images: {np.mean(feature_stds):.4f} \u00b1 {np.std(feature_stds):.4f}\")\n",
        "    \n",
        "    # Check feature diversity\n",
        "    if len(feature_vectors) > 1:\n",
        "        feature_matrix = np.stack(feature_vectors)\n",
        "        \n",
        "        # Calculate pairwise cosine similarities\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        similarities = cosine_similarity(feature_matrix)\n",
        "        \n",
        "        # Get upper triangle (excluding diagonal)\n",
        "        upper_tri = similarities[np.triu_indices_from(similarities, k=1)]\n",
        "        \n",
        "        print(f\"\\n\ud83d\udd0d Feature Similarity Analysis:\")\n",
        "        print(f\"   Mean similarity: {upper_tri.mean():.4f}\")\n",
        "        print(f\"   Std similarity: {upper_tri.std():.4f}\")\n",
        "        print(f\"   Min similarity: {upper_tri.min():.4f}\")\n",
        "        print(f\"   Max similarity: {upper_tri.max():.4f}\")\n",
        "        \n",
        "        if upper_tri.mean() < 0.95:\n",
        "            print(\"   \u2705 Features show good diversity\")\n",
        "        else:\n",
        "            print(\"   \u26a0\ufe0f  Features may be too similar\")\n",
        "    \n",
        "    # Validate feature dimension\n",
        "    expected_dim = 1024\n",
        "    actual_dim = feature_vectors[0].shape[0]\n",
        "    \n",
        "    print(f\"\\n\u2705 Feature Dimension Validation:\")\n",
        "    print(f\"   Expected: {expected_dim}\")\n",
        "    print(f\"   Actual: {actual_dim}\")\n",
        "    \n",
        "    if actual_dim == expected_dim:\n",
        "        print(\"   \u2705 Feature dimension matches specification\")\n",
        "    else:\n",
        "        print(f\"   \u274c Feature dimension mismatch!\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  No features extracted for validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if feature_vectors:\n",
        "    print(\"\ud83d\udcca Visualizing feature distributions...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Plot 1: Feature distribution for first image\n",
        "    axes[0, 0].hist(feature_vectors[0], bins=50, alpha=0.7, edgecolor='black')\n",
        "    axes[0, 0].set_title('Feature Distribution (First Image)')\n",
        "    axes[0, 0].set_xlabel('Feature Value')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Feature statistics across images\n",
        "    feature_means = [r['feature_mean'] for r in extraction_results]\n",
        "    feature_stds = [r['feature_std'] for r in extraction_results]\n",
        "    \n",
        "    axes[0, 1].scatter(feature_means, feature_stds, alpha=0.6)\n",
        "    axes[0, 1].set_title('Feature Statistics Across Images')\n",
        "    axes[0, 1].set_xlabel('Mean Feature Value')\n",
        "    axes[0, 1].set_ylabel('Std Feature Value')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Feature similarity heatmap\n",
        "    if len(feature_vectors) > 1:\n",
        "        feature_matrix = np.stack(feature_vectors)\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        similarities = cosine_similarity(feature_matrix)\n",
        "        \n",
        "        im = axes[1, 0].imshow(similarities, cmap='viridis', aspect='auto')\n",
        "        axes[1, 0].set_title('Feature Similarity Matrix')\n",
        "        axes[1, 0].set_xlabel('Image Index')\n",
        "        axes[1, 0].set_ylabel('Image Index')\n",
        "        plt.colorbar(im, ax=axes[1, 0])\n",
        "    \n",
        "    # Plot 4: Feature range across dimensions\n",
        "    feature_matrix = np.stack(feature_vectors)\n",
        "    feature_ranges = feature_matrix.max(axis=0) - feature_matrix.min(axis=0)\n",
        "    \n",
        "    axes[1, 1].plot(feature_ranges, alpha=0.7)\n",
        "    axes[1, 1].set_title('Feature Range Across Dimensions')\n",
        "    axes[1, 1].set_xlabel('Feature Dimension')\n",
        "    axes[1, 1].set_ylabel('Range (Max - Min)')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FEATURES_DIR / 'feature_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n\u2705 Visualization saved to: {FEATURES_DIR / 'feature_analysis.png'}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  No features available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results and Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udcbe Saving results and metadata...\")\n",
        "\n",
        "# Prepare metadata\n",
        "metadata = {\n",
        "    'notebook': '09_DINOv2_Backbone_Integration',\n",
        "    'model': {\n",
        "        'name': 'dinov2_vitl14',\n",
        "        'feature_dim': 1024,\n",
        "        'input_channels': 4,\n",
        "        'frozen': True\n",
        "    },\n",
        "    'processing': {\n",
        "        'total_images': len(extraction_results),\n",
        "        'successful_extractions': len(feature_vectors),\n",
        "        'device': device\n",
        "    },\n",
        "    'feature_statistics': {\n",
        "        'mean_feature_mean': float(np.mean([r['feature_mean'] for r in extraction_results])) if extraction_results else 0,\n",
        "        'mean_feature_std': float(np.mean([r['feature_std'] for r in extraction_results])) if extraction_results else 0\n",
        "    },\n",
        "    'extraction_results': extraction_results\n",
        "}\n",
        "\n",
        "# Save metadata\n",
        "metadata_path = FEATURES_DIR / 'dinov2_metadata.json'\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"\u2705 Metadata saved to: {metadata_path}\")\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DINOV2 BACKBONE INTEGRATION SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: dinov2_vitl14\")\n",
        "print(f\"Feature Dimension: 1024\")\n",
        "print(f\"Input Channels: 4 (RGB + Depth)\")\n",
        "print(f\"Backbone Frozen: True\")\n",
        "print(f\"\\nProcessing Results:\")\n",
        "print(f\"   Images Processed: {len(extraction_results)}\")\n",
        "print(f\"   Features Extracted: {len(feature_vectors)}\")\n",
        "print(f\"   Output Directory: {FEATURES_DIR}\")\n",
        "print(f\"\\n\u2705 All sub-tasks completed successfully!\")\n",
        "print(f\"   \u2705 7.1: DINOv2 backbone loaded and adapted\")\n",
        "print(f\"   \u2705 7.2: 4-channel input adaptation verified\")\n",
        "print(f\"   \u2705 7.3: Feature extraction and validation completed\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}