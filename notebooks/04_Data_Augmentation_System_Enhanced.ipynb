{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "augmentation_header"
   },
   "source": [
    "# SwellSight Pipeline - Enhanced Data Augmentation System\n",
    "\n",
    "This enhanced notebook implements FLUX.1-dev optimized augmentation parameter generation with improved validation, memory optimization, and progress tracking.\n",
    "\n",
    "## Key Improvements\n",
    "- **FLUX.1-dev Optimization**: Parameter ranges and distributions optimized for FLUX.1-dev model\n",
    "- **Enhanced Validation**: Comprehensive parameter validation with quality checks\n",
    "- **Memory Optimization**: Dynamic memory monitoring and efficient parameter generation\n",
    "- **Progress Tracking**: Real-time progress with memory usage display\n",
    "- **Configuration Snapshots**: Reproducibility through configuration preservation\n",
    "- **Parameter Diversity Analysis**: Advanced diversity metrics and reporting\n",
    "\n",
    "## Pipeline Integration\n",
    "This notebook implements Step 4 of the enhanced pipeline:\n",
    "1. **Load Configuration**: Enhanced config loading with hardware adaptation\n",
    "2. **Parameter System**: FLUX-optimized parameter generator with validation\n",
    "3. **Batch Generation**: Memory-efficient parameter set creation\n",
    "4. **Quality Control**: Advanced parameter validation and quality checks\n",
    "5. **Diversity Analysis**: Comprehensive parameter space coverage analysis\n",
    "6. **Export & Snapshot**: Results export with configuration preservation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys",
    "import json",
    "import logging",
    "from pathlib import Path",
    "from datetime import datetime",
    "import numpy as np",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "from tqdm.auto import tqdm",
    "import random",
    "from collections import defaultdict",
    "import warnings",
    "import gc",
    "import psutil",
    "warnings.filterwarnings('ignore')",
    "",
    "# Add src to path for production module imports",
    "src_path = Path.cwd() / \"src\"",
    "if str(src_path) not in sys.path:",
    "    sys.path.insert(0, str(src_path))",
    "",
    "# Import production modules",
    "try:",
    "    from swellsight.utils.config import load_config, validate_config",
    "    from swellsight.utils.error_handler import ErrorHandler, retry_with_backoff",
    "    from swellsight.utils.hardware import HardwareManager",
    "    from swellsight.utils.performance import PerformanceOptimizer, OptimizationConfig",
    "    from swellsight.utils.monitoring import SystemMonitor",
    "    from swellsight.data.augmentation import WaveAugmentation, AugmentationConfig",
    "    from swellsight.evaluation.data_quality import DataQualityAssessor",
    "    ",
    "    print(\"âœ… Production modules loaded\")",
    "    PRODUCTION_MODULES_AVAILABLE = True",
    "    ",
    "    # Initialize production utilities",
    "    error_handler = ErrorHandler()",
    "    hardware_manager = HardwareManager()",
    "    perf_optimizer = PerformanceOptimizer()",
    "    system_monitor = SystemMonitor()",
    "    data_quality_assessor = DataQualityAssessor()",
    "    ",
    "except ImportError as e:",
    "    print(f\"âš ï¸  Production modules unavailable: {e}\")",
    "    print(\"   Some enhanced features may not be available\")",
    "    PRODUCTION_MODULES_AVAILABLE = False",
    "    error_handler = hardware_manager = perf_optimizer = None",
    "    system_monitor = data_quality_assessor = None",
    "",
    "# Configure enhanced logging",
    "logging.basicConfig(",
    "    level=logging.INFO,",
    "    format='%(asctime)s - %(levelname)s - %(message)s'",
    ")",
    "logger = logging.getLogger(__name__)",
    "",
    "print(\"ðŸ”„ Loading enhanced configuration and utilities...\")",
    "print(\"âœ… Enhanced utilities initialized successfully\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enhanced configuration with hardware adaptation\n",
    "print(\"ðŸ”§ Loading enhanced configuration...\")\n",
    "\n",
    "try:\n",
    "    # Load configuration with hardware detection and adaptation\n",
    "    PIPELINE_CONFIG = config_manager.load_config()\n",
    "    \n",
    "    # Validate configuration\n",
    "    validation_result = validate_config(PIPELINE_CONFIG)\n",
    "    if not validation_result['valid']:\n",
    "        logger.warning(\"Configuration validation issues found:\")\n",
    "        for error in validation_result['errors']:\n",
    "            logger.warning(f\"  - {error}\")\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        logger.info(\"Configuration warnings:\")\n",
    "        for warning in validation_result['warnings']:\n",
    "            logger.info(f\"  - {warning}\")\n",
    "    \n",
    "    # Load previous stage results\n",
    "    depth_results = data_flow_manager.load_previous_results(\n",
    "        stage_name=\"depth_extraction\",\n",
    "        required_files=[\"depth_maps.json\", \"depth_quality.json\"]\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Configuration and previous results loaded successfully\")\n",
    "    \n",
    "    # Display configuration summary\n",
    "    print(f\"\\nðŸ“‹ Enhanced Pipeline Configuration:\")\n",
    "    print(f\"   Pipeline: {PIPELINE_CONFIG['pipeline']['name']} v{PIPELINE_CONFIG['pipeline']['version']}\")\n",
    "    print(f\"   Batch size: {PIPELINE_CONFIG['processing']['batch_size']}\")\n",
    "    print(f\"   Quality threshold: {PIPELINE_CONFIG['processing']['quality_threshold']}\")\n",
    "    print(f\"   Memory limit: {PIPELINE_CONFIG['processing']['memory_limit_gb']:.1f}GB\")\n",
    "    \n",
    "    # Display hardware info if available\n",
    "    if 'detected_hardware' in PIPELINE_CONFIG:\n",
    "        hw_info = PIPELINE_CONFIG['detected_hardware']\n",
    "        print(f\"\\nðŸ–¥ï¸  Detected Hardware:\")\n",
    "        print(f\"   GPU Available: {hw_info['gpu_available']}\")\n",
    "        if hw_info['gpu_available']:\n",
    "            print(f\"   GPU: {hw_info['gpu_name']}\")\n",
    "            print(f\"   GPU Memory: {hw_info['gpu_memory_gb']:.1f}GB\")\n",
    "            print(f\"   FLUX Compatibility: {hw_info['flux_compatibility']}\")\n",
    "        print(f\"   CPU Cores: {hw_info['cpu_count']}\")\n",
    "        print(f\"   System Memory: {hw_info['memory_gb']:.1f}GB\")\n",
    "    \n",
    "    # Display depth extraction results\n",
    "    if depth_results:\n",
    "        print(f\"\\nðŸ“Š Depth Extraction Results:\")\n",
    "        print(f\"   Images processed: {len(depth_results.get('images', []))}\")\n",
    "        print(f\"   Average quality: {depth_results.get('average_quality', 0):.3f}\")\n",
    "        print(f\"   Ready for augmentation: {len(depth_results.get('images', []))} images\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading configuration: {e}\")\n",
    "    print(\"âŒ Failed to load configuration. Please check previous pipeline stages.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FLUX-Optimized Parameter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLUXOptimizedParameterSystem:\n",
    "    \"\"\"\n",
    "    Enhanced parameter system optimized for FLUX.1-dev model\n",
    "    with comprehensive validation and quality checks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.flux_config = config['models']['base_model']\n",
    "        self.controlnet_config = config['models']['controlnet_model']\n",
    "        \n",
    "        # FLUX.1-dev optimized parameter ranges\n",
    "        self.parameter_ranges = {\n",
    "            'guidance_scale': {\n",
    "                'min': 1.0,\n",
    "                'max': 5.0,  # FLUX works better with lower guidance\n",
    "                'default': 3.5,\n",
    "                'distribution': 'normal',\n",
    "                'std': 0.8\n",
    "            },\n",
    "            'num_inference_steps': {\n",
    "                'values': [20, 25, 28, 30, 35],  # FLUX optimal step counts\n",
    "                'default': 28,\n",
    "                'weights': [0.1, 0.2, 0.4, 0.2, 0.1]  # Prefer 28 steps\n",
    "            },\n",
    "            'controlnet_conditioning_scale': {\n",
    "                'min': 0.4,\n",
    "                'max': 0.8,  # FLUX ControlNet works better with moderate conditioning\n",
    "                'default': 0.6,\n",
    "                'distribution': 'uniform'\n",
    "            },\n",
    "            'control_guidance_start': {\n",
    "                'min': 0.0,\n",
    "                'max': 0.2,\n",
    "                'default': 0.0,\n",
    "                'distribution': 'uniform'\n",
    "            },\n",
    "            'control_guidance_end': {\n",
    "                'min': 0.8,\n",
    "                'max': 1.0,\n",
    "                'default': 1.0,\n",
    "                'distribution': 'uniform'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Enhanced prompt variations for wave/ocean scenes\n",
    "        self.prompt_variations = [\n",
    "            \"ocean waves crashing on beach, natural lighting, photorealistic\",\n",
    "            \"powerful sea waves, dramatic coastline, high detail\",\n",
    "            \"coastal waves breaking, golden hour lighting, cinematic\",\n",
    "            \"surf waves rolling to shore, clear blue water, realistic\",\n",
    "            \"ocean swells approaching beach, natural colors, detailed\",\n",
    "            \"breaking waves with white foam, sunny day, sharp focus\",\n",
    "            \"sea waves in motion, dynamic water, professional photography\",\n",
    "            \"beach waves at sunset, warm lighting, high resolution\",\n",
    "            \"turbulent ocean waves, stormy atmosphere, dramatic\",\n",
    "            \"gentle waves lapping shore, peaceful scene, soft lighting\"\n",
    "        ]\n",
    "        \n",
    "        # Parameter validation rules\n",
    "        self.validation_rules = {\n",
    "            'guidance_scale': {\n",
    "                'min_value': 0.5,\n",
    "                'max_value': 10.0,\n",
    "                'recommended_min': 1.0,\n",
    "                'recommended_max': 5.0\n",
    "            },\n",
    "            'num_inference_steps': {\n",
    "                'min_value': 10,\n",
    "                'max_value': 50,\n",
    "                'recommended_min': 20,\n",
    "                'recommended_max': 35\n",
    "            },\n",
    "            'controlnet_conditioning_scale': {\n",
    "                'min_value': 0.1,\n",
    "                'max_value': 2.0,\n",
    "                'recommended_min': 0.4,\n",
    "                'recommended_max': 0.8\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_random_parameters(self) -> dict:\n",
    "        \"\"\"Generate FLUX-optimized random parameters\"\"\"\n",
    "        try:\n",
    "            parameters = {}\n",
    "            \n",
    "            # Guidance scale (normal distribution around optimal value)\n",
    "            guidance_config = self.parameter_ranges['guidance_scale']\n",
    "            if guidance_config['distribution'] == 'normal':\n",
    "                guidance = np.random.normal(guidance_config['default'], guidance_config['std'])\n",
    "                guidance = np.clip(guidance, guidance_config['min'], guidance_config['max'])\n",
    "            else:\n",
    "                guidance = np.random.uniform(guidance_config['min'], guidance_config['max'])\n",
    "            parameters['guidance_scale'] = float(guidance)\n",
    "            \n",
    "            # Inference steps (weighted choice)\n",
    "            steps_config = self.parameter_ranges['num_inference_steps']\n",
    "            steps = np.random.choice(steps_config['values'], p=steps_config['weights'])\n",
    "            parameters['num_inference_steps'] = int(steps)\n",
    "            \n",
    "            # ControlNet conditioning scale\n",
    "            conditioning_config = self.parameter_ranges['controlnet_conditioning_scale']\n",
    "            conditioning = np.random.uniform(conditioning_config['min'], conditioning_config['max'])\n",
    "            parameters['controlnet_conditioning_scale'] = float(conditioning)\n",
    "            \n",
    "            # Control guidance timing\n",
    "            start_config = self.parameter_ranges['control_guidance_start']\n",
    "            end_config = self.parameter_ranges['control_guidance_end']\n",
    "            parameters['control_guidance_start'] = float(np.random.uniform(start_config['min'], start_config['max']))\n",
    "            parameters['control_guidance_end'] = float(np.random.uniform(end_config['min'], end_config['max']))\n",
    "            \n",
    "            # Random seed for reproducibility\n",
    "            parameters['seed'] = int(np.random.randint(0, 2**32 - 1))\n",
    "            \n",
    "            # Prompt variation\n",
    "            parameters['prompt_variation'] = np.random.choice(self.prompt_variations)\n",
    "            \n",
    "            return parameters\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating parameters: {e}\")\n",
    "            return self._get_default_parameters()\n",
    "    \n",
    "    def validate_parameters(self, parameters: dict) -> dict:\n",
    "        \"\"\"Validate parameter values and provide quality assessment\"\"\"\n",
    "        validation_result = {\n",
    "            'valid': True,\n",
    "            'issues': [],\n",
    "            'warnings': [],\n",
    "            'quality_score': 1.0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            quality_scores = []\n",
    "            \n",
    "            for param_name, value in parameters.items():\n",
    "                if param_name in self.validation_rules:\n",
    "                    rules = self.validation_rules[param_name]\n",
    "                    \n",
    "                    # Check absolute bounds\n",
    "                    if value < rules['min_value'] or value > rules['max_value']:\n",
    "                        validation_result['valid'] = False\n",
    "                        validation_result['issues'].append(\n",
    "                            f\"{param_name} value {value} outside valid range [{rules['min_value']}, {rules['max_value']}]\"\n",
    "                        )\n",
    "                        quality_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # Check recommended bounds\n",
    "                    if value < rules['recommended_min'] or value > rules['recommended_max']:\n",
    "                        validation_result['warnings'].append(\n",
    "                            f\"{param_name} value {value} outside recommended range [{rules['recommended_min']}, {rules['recommended_max']}]\"\n",
    "                        )\n",
    "                        quality_scores.append(0.7)  # Reduced quality but still valid\n",
    "                    else:\n",
    "                        quality_scores.append(1.0)  # Optimal quality\n",
    "            \n",
    "            # Calculate overall quality score\n",
    "            if quality_scores:\n",
    "                validation_result['quality_score'] = np.mean(quality_scores)\n",
    "            \n",
    "            return validation_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error validating parameters: {e}\")\n",
    "            return {\n",
    "                'valid': False,\n",
    "                'issues': [f\"Validation error: {str(e)}\"],\n",
    "                'warnings': [],\n",
    "                'quality_score': 0.0\n",
    "            }\n",
    "    \n",
    "    def _get_default_parameters(self) -> dict:\n",
    "        \"\"\"Get default parameters as fallback\"\"\"\n",
    "        return {\n",
    "            'guidance_scale': self.parameter_ranges['guidance_scale']['default'],\n",
    "            'num_inference_steps': self.parameter_ranges['num_inference_steps']['default'],\n",
    "            'controlnet_conditioning_scale': self.parameter_ranges['controlnet_conditioning_scale']['default'],\n",
    "            'control_guidance_start': self.parameter_ranges['control_guidance_start']['default'],\n",
    "            'control_guidance_end': self.parameter_ranges['control_guidance_end']['default'],\n",
    "            'seed': 42,\n",
    "            'prompt_variation': self.prompt_variations[0]\n",
    "        }\n",
    "\n",
    "# Initialize FLUX-optimized parameter system\n",
    "print(\"ðŸŽ›ï¸  Initializing FLUX-optimized parameter system...\")\n",
    "\n",
    "try:\n",
    "    param_system = FLUXOptimizedParameterSystem(PIPELINE_CONFIG)\n",
    "    \n",
    "    # Test parameter generation\n",
    "    test_params = param_system.generate_random_parameters()\n",
    "    validation_result = param_system.validate_parameters(test_params)\n",
    "    \n",
    "    print(\"âœ… FLUX-optimized parameter system initialized successfully!\")\n",
    "    print(f\"   Parameter validation: {'âœ… Valid' if validation_result['valid'] else 'âŒ Invalid'}\")\n",
    "    print(f\"   Quality score: {validation_result['quality_score']:.3f}\")\n",
    "    print(f\"   Sample parameters: {test_params}\")\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        print(f\"   Warnings: {len(validation_result['warnings'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize parameter system: {e}\")\n",
    "    print(\"âŒ Parameter system initialization failed\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory-Optimized Parameter Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-optimized batch parameter generation\n",
    "print(\"ðŸŽ² Starting memory-optimized parameter generation...\")\n",
    "\n",
    "# Get images ready for augmentation\n",
    "ready_images = depth_results.get('images', [])\n",
    "if not ready_images:\n",
    "    print(\"âŒ No images ready for augmentation. Please check previous pipeline stages.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get synthetic per real from config\n",
    "synthetic_per_real = PIPELINE_CONFIG['processing'].get('synthetic_per_real', 3)\n",
    "total_parameter_sets = len(ready_images) * synthetic_per_real\n",
    "\n",
    "print(f\"\\nðŸ“Š Generation Configuration:\")\n",
    "print(f\"   Images to process: {len(ready_images)}\")\n",
    "print(f\"   Synthetic variations per image: {synthetic_per_real}\")\n",
    "print(f\"   Total parameter sets to generate: {total_parameter_sets}\")\n",
    "\n",
    "# Monitor memory usage\n",
    "initial_memory = monitor_memory()\n",
    "print(f\"\\nðŸ’¾ Initial Memory Usage:\")\n",
    "print(f\"   System: {initial_memory.get('system_percent', 0):.1f}%\")\n",
    "if 'gpu_percent' in initial_memory:\n",
    "    print(f\"   GPU: {initial_memory.get('gpu_percent', 0):.1f}%\")\n",
    "\n",
    "# Calculate optimal batch size for parameter generation\n",
    "param_memory_per_item = 1024  # Estimated bytes per parameter set\n",
    "optimal_batch_size = memory_optimizer.get_optimal_batch_size(\n",
    "    item_size=param_memory_per_item,\n",
    "    max_batch_size=min(len(ready_images), 100)\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ”§ Memory Optimization:\")\n",
    "print(f\"   Optimal batch size: {optimal_batch_size}\")\n",
    "print(f\"   Estimated memory per parameter set: {param_memory_per_item} bytes\")\n",
    "\n",
    "# Initialize tracking variables\n",
    "all_augmentation_params = []\n",
    "parameter_statistics = defaultdict(list)\n",
    "generation_metadata = []\n",
    "validation_summary = {\n",
    "    'total_generated': 0,\n",
    "    'valid_parameters': 0,\n",
    "    'invalid_parameters': 0,\n",
    "    'quality_warnings': 0,\n",
    "    'average_quality_score': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸš€ Starting parameter generation with memory monitoring...\")\n",
    "\n",
    "# Use memory monitoring context\n",
    "with memory_optimizer.memory_monitor(log_usage=True) as monitor:\n",
    "    # Process images in batches for memory efficiency\n",
    "    for batch_start in tqdm(range(0, len(ready_images), optimal_batch_size), \n",
    "                           desc=\"Processing batches\", unit=\"batch\"):\n",
    "        \n",
    "        batch_end = min(batch_start + optimal_batch_size, len(ready_images))\n",
    "        batch_images = ready_images[batch_start:batch_end]\n",
    "        \n",
    "        # Generate parameters for current batch\n",
    "        batch_params = []\n",
    "        \n",
    "        for i, image_path in enumerate(batch_images):\n",
    "            image_path_obj = Path(image_path)\n",
    "            global_index = batch_start + i\n",
    "            \n",
    "            # Generate multiple parameter sets for this image\n",
    "            for j in range(synthetic_per_real):\n",
    "                try:\n",
    "                    # Generate parameters\n",
    "                    augmentation_params = param_system.generate_random_parameters()\n",
    "                    \n",
    "                    # Validate parameters\n",
    "                    validation_result = param_system.validate_parameters(augmentation_params)\n",
    "                    \n",
    "                    # Create parameter metadata\n",
    "                    param_metadata = {\n",
    "                        'image_path': image_path,\n",
    "                        'image_name': image_path_obj.name,\n",
    "                        'variation_index': j + 1,\n",
    "                        'total_variations': synthetic_per_real,\n",
    "                        'global_index': validation_summary['total_generated'],\n",
    "                        'parameters': augmentation_params,\n",
    "                        'validation': validation_result,\n",
    "                        'generated_timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    batch_params.append(param_metadata)\n",
    "                    \n",
    "                    # Update validation summary\n",
    "                    validation_summary['total_generated'] += 1\n",
    "                    if validation_result['valid']:\n",
    "                        validation_summary['valid_parameters'] += 1\n",
    "                    else:\n",
    "                        validation_summary['invalid_parameters'] += 1\n",
    "                    \n",
    "                    validation_summary['quality_warnings'] += len(validation_result['warnings'])\n",
    "                    \n",
    "                    # Collect statistics for numeric parameters\n",
    "                    for key, value in augmentation_params.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            parameter_statistics[key].append(value)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to generate parameters for {image_path_obj.name} variation {j+1}: {e}\")\n",
    "                    validation_summary['invalid_parameters'] += 1\n",
    "                    continue\n",
    "        \n",
    "        # Add batch to main collection\n",
    "        all_augmentation_params.extend(batch_params)\n",
    "        \n",
    "        # Monitor memory usage during processing\n",
    "        current_memory = monitor_memory()\n",
    "        if current_memory.get('system_percent', 0) > 85:\n",
    "            logger.warning(f\"High memory usage detected: {current_memory.get('system_percent', 0):.1f}%\")\n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "        \n",
    "        # Clear batch variables to free memory\n",
    "        del batch_params\n",
    "        gc.collect()\n",
    "\n",
    "# Calculate final statistics\n",
    "if validation_summary['total_generated'] > 0:\n",
    "    # Calculate average quality score\n",
    "    quality_scores = [param['validation']['quality_score'] for param in all_augmentation_params \n",
    "                     if 'validation' in param]\n",
    "    if quality_scores:\n",
    "        validation_summary['average_quality_score'] = np.mean(quality_scores)\n",
    "\n",
    "print(f\"\\nâœ… Parameter generation completed!\")\n",
    "print(f\"\\nðŸ“Š Generation Results:\")\n",
    "print(f\"   Total parameter sets generated: {validation_summary['total_generated']}\")\n",
    "print(f\"   Valid parameter sets: {validation_summary['valid_parameters']}\")\n",
    "print(f\"   Invalid parameter sets: {validation_summary['invalid_parameters']}\")\n",
    "print(f\"   Quality warnings: {validation_summary['quality_warnings']}\")\n",
    "print(f\"   Average quality score: {validation_summary['average_quality_score']:.3f}\")\n",
    "print(f\"   Success rate: {validation_summary['valid_parameters']/validation_summary['total_generated']*100:.1f}%\")\n",
    "\n",
    "# Final memory check\n",
    "final_memory = monitor_memory()\n",
    "print(f\"\\nðŸ’¾ Final Memory Usage:\")\n",
    "print(f\"   System: {final_memory.get('system_percent', 0):.1f}%\")\n",
    "if 'gpu_percent' in final_memory:\n",
    "    print(f\"   GPU: {final_memory.get('gpu_percent', 0):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}