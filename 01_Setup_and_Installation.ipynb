{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Ox_hEU6-as"
      },
      "source": [
        "# SwellSight Real-to-Synthetic Pipeline - Setup and Installation (Enhanced)\n",
        "\n",
        "This notebook handles environment preparation and dependency management for the SwellSight real-to-synthetic image generation pipeline with integrated utilities.\n",
        "\n",
        "## New Features\n",
        "- ‚ú® Integrated SwellSight utilities for better error handling and progress tracking\n",
        "- üîß Automatic configuration management\n",
        "- üìä Memory optimization and monitoring\n",
        "- üõ°Ô∏è Robust error handling with retry logic\n",
        "- üìà Progress tracking and performance feedback\n",
        "- üöÄ Updated for Depth-Anything-V2 and FLUX.1-dev models\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import SwellSight Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Add utils directory to Python path\n",
        "utils_path = Path.cwd() / \"utils\"\n",
        "if str(utils_path) not in sys.path:\n",
        "    sys.path.insert(0, str(utils_path))\n",
        "\n",
        "try:\n",
        "    # Import SwellSight utilities\n",
        "    from utils import (\n",
        "        load_config, validate_config,\n",
        "        validate_image_quality, validate_depth_map_quality,\n",
        "        get_optimal_batch_size, cleanup_variables, monitor_memory,\n",
        "        retry_with_backoff, handle_gpu_memory_error,\n",
        "        create_progress_bar, display_stage_summary,\n",
        "        save_stage_results, load_previous_results, check_dependencies\n",
        "    )\n",
        "    print(\"‚úÖ SwellSight utilities loaded successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è Could not import utilities: {e}\")\n",
        "    print(\"Continuing with basic functionality...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pipeline configuration\n",
        "try:\n",
        "    config = load_config(\"config.json\")\n",
        "    print(f\"‚úÖ Configuration loaded: {config['pipeline']['name']} v{config['pipeline']['version']}\")\n",
        "    \n",
        "    # Extract commonly used settings\n",
        "    batch_size = config['processing']['batch_size']\n",
        "    quality_threshold = config['processing']['quality_threshold']\n",
        "    data_dir = Path(config['paths']['data_dir'])\n",
        "    output_dir = Path(config['paths']['output_dir'])\n",
        "    checkpoint_dir = Path(config['paths']['checkpoint_dir'])\n",
        "    \n",
        "    print(f\"üìÅ Data directory: {data_dir}\")\n",
        "    print(f\"üìÅ Output directory: {output_dir}\")\n",
        "    print(f\"üìÅ Checkpoint directory: {checkpoint_dir}\")\n",
        "    print(f\"üéØ Quality threshold: {quality_threshold}\")\n",
        "    \n",
        "    # Display model configuration\n",
        "    print(f\"\\nü§ñ Model Configuration:\")\n",
        "    print(f\"  Depth Model: {config['models']['depth_model']}\")\n",
        "    print(f\"  Base Model: {config['models']['base_model']}\")\n",
        "    print(f\"  ControlNet: {config['models']['controlnet_model']}\")\n",
        "    print(f\"  Mixed Precision: {config['models']['mixed_precision']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not load configuration: {e}\")\n",
        "    print(\"Using default settings...\")\n",
        "    \n",
        "    # Fallback configuration\n",
        "    batch_size = \"auto\"\n",
        "    quality_threshold = 0.7\n",
        "    data_dir = Path(\"./data\")\n",
        "    output_dir = Path(\"./outputs\")\n",
        "    checkpoint_dir = Path(\"./checkpoints\")\n",
        "    \n",
        "    config = {\n",
        "        'pipeline': {'name': 'swellsight_pipeline', 'version': '1.0'},\n",
        "        'processing': {'batch_size': batch_size, 'quality_threshold': quality_threshold},\n",
        "        'models': {\n",
        "            'depth_model': 'depth-anything/Depth-Anything-V2-Large',\n",
        "            'base_model': 'black-forest-labs/FLUX.1-dev',\n",
        "            'controlnet_model': 'Shakker-Labs/FLUX.1-dev-ControlNet-Depth',\n",
        "            'mixed_precision': True\n",
        "        },\n",
        "        'paths': {'data_dir': str(data_dir), 'output_dir': str(output_dir), 'checkpoint_dir': str(checkpoint_dir)}\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Enhanced Hardware Detection and GPU Memory Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import platform\n",
        "\n",
        "# Check if running in Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    print(\"üîó Mounting Google Drive...\")\n",
        "    \n",
        "    def mount_drive():\n",
        "        drive.mount('/content/drive')\n",
        "        return True\n",
        "    \n",
        "    try:\n",
        "        # Use retry logic for drive mounting\n",
        "        retry_with_backoff(mount_drive, max_retries=2)\n",
        "        print(\"‚úÖ Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Drive mounting failed: {e}\")\n",
        "\n",
        "# Enhanced hardware detection with FLUX requirements\n",
        "print(\"\\nüîç Hardware Detection:\")\n",
        "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "\n",
        "# GPU detection with FLUX memory requirements\n",
        "flux_memory_requirements = {\n",
        "    'minimum_gb': 8,\n",
        "    'recommended_gb': 16,\n",
        "    'optimal_gb': 24\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    current_gpu = torch.cuda.current_device()\n",
        "    gpu_name = torch.cuda.get_device_name(current_gpu)\n",
        "    gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / (1024**3)\n",
        "    \n",
        "    print(f\"üöÄ GPU Available: {gpu_name}\")\n",
        "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"üî¢ GPU Count: {gpu_count}\")\n",
        "    \n",
        "    # FLUX memory assessment\n",
        "    if gpu_memory >= flux_memory_requirements['optimal_gb']:\n",
        "        print(f\"‚úÖ GPU memory is optimal for FLUX.1-dev ({gpu_memory:.1f} GB >= {flux_memory_requirements['optimal_gb']} GB)\")\n",
        "        flux_compatibility = \"optimal\"\n",
        "    elif gpu_memory >= flux_memory_requirements['recommended_gb']:\n",
        "        print(f\"‚ö†Ô∏è GPU memory is adequate for FLUX.1-dev ({gpu_memory:.1f} GB >= {flux_memory_requirements['recommended_gb']} GB)\")\n",
        "        flux_compatibility = \"adequate\"\n",
        "    elif gpu_memory >= flux_memory_requirements['minimum_gb']:\n",
        "        print(f\"‚ö†Ô∏è GPU memory is minimal for FLUX.1-dev ({gpu_memory:.1f} GB >= {flux_memory_requirements['minimum_gb']} GB)\")\n",
        "        print(\"üí° Consider using smaller batch sizes and mixed precision training\")\n",
        "        flux_compatibility = \"minimal\"\n",
        "    else:\n",
        "        print(f\"‚ùå GPU memory insufficient for FLUX.1-dev ({gpu_memory:.1f} GB < {flux_memory_requirements['minimum_gb']} GB)\")\n",
        "        print(\"üí° Consider using CPU fallback or upgrading GPU\")\n",
        "        flux_compatibility = \"insufficient\"\n",
        "        \n",
        "    # Check for mixed precision support\n",
        "    if torch.cuda.get_device_capability(current_gpu)[0] >= 7:\n",
        "        print(\"‚úÖ GPU supports mixed precision training (Tensor Cores available)\")\n",
        "        mixed_precision_supported = True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è GPU has limited mixed precision support\")\n",
        "        mixed_precision_supported = False\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU available - using CPU\")\n",
        "    print(\"‚ùå FLUX.1-dev requires GPU acceleration for reasonable performance\")\n",
        "    flux_compatibility = \"none\"\n",
        "    mixed_precision_supported = False\n",
        "\n",
        "# Memory monitoring\n",
        "try:\n",
        "    memory_info = monitor_memory()\n",
        "    print(f\"\\nüíª System Memory: {memory_info.get('system_total_gb', 0):.1f} GB total, {memory_info.get('system_percent', 0):.1f}% used\")\n",
        "    if 'gpu_total_gb' in memory_info:\n",
        "        print(f\"üéÆ GPU Memory: {memory_info.get('gpu_total_gb', 0):.1f} GB total, {memory_info.get('gpu_percent', 0):.1f}% used\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not get memory info: {e}\")\n",
        "\n",
        "# Store hardware info for later use\n",
        "hardware_info = {\n",
        "    'gpu_available': torch.cuda.is_available(),\n",
        "    'gpu_memory_gb': gpu_memory if torch.cuda.is_available() else 0,\n",
        "    'flux_compatibility': flux_compatibility,\n",
        "    'mixed_precision_supported': mixed_precision_supported\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Directory Structure with Progress Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "directories_to_create = [\n",
        "    data_dir / \"real\" / \"images\",\n",
        "    data_dir / \"processed\",\n",
        "    data_dir / \"depth_maps\",\n",
        "    data_dir / \"synthetic\",\n",
        "    output_dir,\n",
        "    checkpoint_dir,\n",
        "    Path(\"models\"),\n",
        "    Path(\"logs\")\n",
        "]\n",
        "\n",
        "print(\"üìÅ Creating directory structure...\")\n",
        "progress_bar = create_progress_bar(len(directories_to_create), \"Creating directories\")\n",
        "\n",
        "created_dirs = []\n",
        "for directory in directories_to_create:\n",
        "    try:\n",
        "        directory.mkdir(parents=True, exist_ok=True)\n",
        "        created_dirs.append(str(directory))\n",
        "        progress_bar.update(1)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not create directory {directory}: {e}\")\n",
        "\n",
        "progress_bar.close()\n",
        "print(f\"‚úÖ Created {len(created_dirs)} directories\")\n",
        "\n",
        "# Display directory structure\n",
        "print(\"\\nüìÇ Directory Structure:\")\n",
        "for directory in created_dirs:\n",
        "    print(f\"  üìÅ {directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Install Enhanced Dependencies for Depth-Anything-V2 and FLUX.1-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Define required packages for Depth-Anything-V2 and FLUX.1-dev\n",
        "required_packages = [\n",
        "    # Core PyTorch packages\n",
        "    \"torch>=2.0.0\",\n",
        "    \"torchvision>=0.15.0\",\n",
        "    \"torchaudio>=2.0.0\",\n",
        "    \n",
        "    # Transformers and Diffusers for new models\n",
        "    \"transformers>=4.35.0\",  # Updated for Depth-Anything-V2 support\n",
        "    \"diffusers>=0.24.0\",     # Updated for FLUX.1-dev support\n",
        "    \"accelerate>=0.24.0\",    # Enhanced acceleration for FLUX\n",
        "    \n",
        "    # FLUX-specific dependencies\n",
        "    \"sentencepiece>=0.1.99\", # For FLUX text encoding\n",
        "    \"protobuf>=3.20.0\",      # Protocol buffers for model serialization\n",
        "    \"safetensors>=0.4.0\",    # Safe tensor loading for FLUX models\n",
        "    \n",
        "    # Depth-Anything-V2 dependencies\n",
        "    \"timm>=0.9.0\",           # PyTorch Image Models for backbone\n",
        "    \"einops>=0.7.0\",         # Tensor operations for depth models\n",
        "    \n",
        "    # Core image processing\n",
        "    \"opencv-python>=4.8.0\",\n",
        "    \"Pillow>=10.0.0\",\n",
        "    \"numpy>=1.24.0\",\n",
        "    \n",
        "    # Utilities and monitoring\n",
        "    \"tqdm>=4.65.0\",\n",
        "    \"psutil>=5.9.0\",\n",
        "    \"matplotlib>=3.7.0\",     # For visualization\n",
        "    \"scipy>=1.10.0\",         # Scientific computing\n",
        "    \n",
        "    # Memory and performance optimization\n",
        "    \"xformers>=0.0.22\",      # Memory-efficient attention for FLUX\n",
        "    \"bitsandbytes>=0.41.0\"   # 8-bit optimization support\n",
        "]\n",
        "\n",
        "# Optional packages for enhanced performance\n",
        "optional_packages = [\n",
        "    \"flash-attn>=2.3.0\",     # Flash attention for even better memory efficiency\n",
        "    \"triton>=2.1.0\"          # Triton kernels for optimized operations\n",
        "]\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        raise Exception(f\"Installation failed: {result.stderr}\")\n",
        "    return result.stdout\n",
        "\n",
        "print(\"üì¶ Installing required packages for Depth-Anything-V2 and FLUX.1-dev...\")\n",
        "progress_bar = create_progress_bar(len(required_packages), \"Installing packages\")\n",
        "\n",
        "installed_packages = []\n",
        "failed_packages = []\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        # Use retry logic for package installation\n",
        "        retry_with_backoff(lambda: install_package(package), max_retries=2)\n",
        "        installed_packages.append(package)\n",
        "        progress_bar.update(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è Failed to install {package}: {e}\")\n",
        "        failed_packages.append(package)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "# Try to install optional packages\n",
        "if hardware_info['gpu_available'] and hardware_info['flux_compatibility'] in ['optimal', 'adequate']:\n",
        "    print(\"\\nüì¶ Installing optional performance packages...\")\n",
        "    optional_progress = create_progress_bar(len(optional_packages), \"Installing optional packages\")\n",
        "    \n",
        "    for package in optional_packages:\n",
        "        try:\n",
        "            install_package(package)\n",
        "            installed_packages.append(package)\n",
        "            print(f\"‚úÖ Installed optional package: {package}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Optional package {package} failed: {e}\")\n",
        "        optional_progress.update(1)\n",
        "    \n",
        "    optional_progress.close()\n",
        "\n",
        "# Display installation summary\n",
        "installation_metrics = {\n",
        "    'total_packages': len(required_packages),\n",
        "    'installed_successfully': len([p for p in installed_packages if p in required_packages]),\n",
        "    'failed_installations': len(failed_packages),\n",
        "    'success_rate': len([p for p in installed_packages if p in required_packages]) / len(required_packages)\n",
        "}\n",
        "\n",
        "display_stage_summary(\"Package Installation\", installation_metrics)\n",
        "\n",
        "if failed_packages:\n",
        "    print(\"\\n‚ö†Ô∏è Failed packages:\")\n",
        "    for package in failed_packages:\n",
        "        print(f\"  - {package}\")\n",
        "    print(\"\\nüí° Try installing failed packages manually or check your internet connection.\")\n",
        "    \n",
        "    # Provide specific guidance for FLUX requirements\n",
        "    if any('xformers' in pkg for pkg in failed_packages):\n",
        "        print(\"\\nüîß XFormers installation tips:\")\n",
        "        print(\"  - Ensure you have the correct PyTorch version\")\n",
        "        print(\"  - Try: pip install xformers --index-url https://download.pytorch.org/whl/cu118\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hardware-Adaptive Memory and Batch Size Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate optimal batch size based on available memory and FLUX requirements\n",
        "print(\"üß† Optimizing memory configuration for FLUX.1-dev...\")\n",
        "\n",
        "if batch_size == \"auto\":\n",
        "    try:\n",
        "        # Adjust max batch size based on FLUX compatibility\n",
        "        if hardware_info['flux_compatibility'] == 'optimal':\n",
        "            max_batch_size = 8\n",
        "        elif hardware_info['flux_compatibility'] == 'adequate':\n",
        "            max_batch_size = 4\n",
        "        elif hardware_info['flux_compatibility'] == 'minimal':\n",
        "            max_batch_size = 2\n",
        "        else:\n",
        "            max_batch_size = 1\n",
        "            \n",
        "        optimal_batch_size = get_optimal_batch_size(max_batch_size=max_batch_size)\n",
        "        print(f\"‚úÖ Calculated optimal batch size for FLUX: {optimal_batch_size}\")\n",
        "        \n",
        "        # Additional recommendations based on hardware\n",
        "        if hardware_info['mixed_precision_supported']:\n",
        "            print(\"‚úÖ Mixed precision training recommended and supported\")\n",
        "            config['models']['mixed_precision'] = True\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Mixed precision training not fully supported, using FP32\")\n",
        "            config['models']['mixed_precision'] = False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not calculate optimal batch size: {e}\")\n",
        "        optimal_batch_size = 1  # Very conservative fallback for FLUX\n",
        "        print(f\"Using very conservative batch size for FLUX: {optimal_batch_size}\")\n",
        "else:\n",
        "    optimal_batch_size = batch_size\n",
        "    print(f\"‚úÖ Using configured batch size: {optimal_batch_size}\")\n",
        "\n",
        "# Update configuration with hardware-adaptive settings\n",
        "try:\n",
        "    config['processing']['batch_size'] = optimal_batch_size\n",
        "    config['hardware'] = hardware_info\n",
        "    \n",
        "    # Add FLUX-specific optimizations\n",
        "    config['optimizations'] = {\n",
        "        'use_xformers': 'xformers>=0.0.22' in installed_packages,\n",
        "        'use_flash_attention': 'flash-attn>=2.3.0' in installed_packages,\n",
        "        'gradient_checkpointing': hardware_info['flux_compatibility'] in ['minimal', 'insufficient'],\n",
        "        'cpu_offload': hardware_info['flux_compatibility'] == 'insufficient'\n",
        "    }\n",
        "    \n",
        "    # Save updated configuration\n",
        "    import json\n",
        "    with open(\"config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    \n",
        "    print(\"‚úÖ Configuration updated with hardware-adaptive settings\")\n",
        "    print(f\"üìä Optimizations enabled:\")\n",
        "    for opt, enabled in config['optimizations'].items():\n",
        "        status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
        "        print(f\"  {status} {opt}: {enabled}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not save updated configuration: {e}\")\n",
        "\n",
        "# Display memory optimization suggestions\n",
        "try:\n",
        "    from utils.memory_optimizer import MemoryOptimizer\n",
        "    optimizer = MemoryOptimizer()\n",
        "    suggestions = optimizer.suggest_memory_optimizations()\n",
        "    \n",
        "    if suggestions:\n",
        "        print(\"\\nüí° Memory Optimization Suggestions:\")\n",
        "        for i, suggestion in enumerate(suggestions[:3], 1):\n",
        "            print(f\"  {i}. {suggestion}\")\n",
        "            \n",
        "    # Add FLUX-specific suggestions\n",
        "    print(\"\\nüöÄ FLUX.1-dev Specific Recommendations:\")\n",
        "    if hardware_info['flux_compatibility'] == 'insufficient':\n",
        "        print(\"  ‚ö†Ô∏è Consider using CPU offloading for model components\")\n",
        "        print(\"  ‚ö†Ô∏è Use gradient checkpointing to reduce memory usage\")\n",
        "        print(\"  ‚ö†Ô∏è Process images one at a time (batch_size=1)\")\n",
        "    elif hardware_info['flux_compatibility'] == 'minimal':\n",
        "        print(\"  üí° Enable gradient checkpointing for memory savings\")\n",
        "        print(\"  üí° Use mixed precision if supported\")\n",
        "        print(\"  üí° Consider smaller image resolutions initially\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Hardware is well-suited for FLUX.1-dev\")\n",
        "        print(\"  ‚úÖ Can use standard batch sizes and full precision if needed\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not generate optimization suggestions: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Validate Configuration and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate the updated configuration\n",
        "print(\"üîç Validating configuration and dependencies...\")\n",
        "\n",
        "try:\n",
        "    # Validate configuration structure\n",
        "    validation_result = validate_config(config)\n",
        "    if validation_result['valid']:\n",
        "        print(\"‚úÖ Configuration validation passed\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Configuration validation issues: {validation_result['errors']}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not validate configuration: {e}\")\n",
        "\n",
        "# Test critical imports for new models\n",
        "critical_imports = {\n",
        "    'torch': 'PyTorch',\n",
        "    'transformers': 'Transformers (for Depth-Anything-V2)',\n",
        "    'diffusers': 'Diffusers (for FLUX.1-dev)',\n",
        "    'timm': 'TIMM (for Depth-Anything-V2 backbone)',\n",
        "    'einops': 'Einops (for tensor operations)'\n",
        "}\n",
        "\n",
        "print(\"\\nüß™ Testing critical imports:\")\n",
        "import_results = {}\n",
        "\n",
        "for module, description in critical_imports.items():\n",
        "    try:\n",
        "        __import__(module)\n",
        "        print(f\"‚úÖ {description}: Available\")\n",
        "        import_results[module] = True\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå {description}: Failed - {e}\")\n",
        "        import_results[module] = False\n",
        "\n",
        "# Test optional performance imports\n",
        "optional_imports = {\n",
        "    'xformers': 'XFormers (memory optimization)',\n",
        "    'bitsandbytes': 'BitsAndBytes (8-bit optimization)'\n",
        "}\n",
        "\n",
        "print(\"\\nüîß Testing optional performance imports:\")\n",
        "for module, description in optional_imports.items():\n",
        "    try:\n",
        "        __import__(module)\n",
        "        print(f\"‚úÖ {description}: Available\")\n",
        "        import_results[module] = True\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è {description}: Not available - {e}\")\n",
        "        import_results[module] = False\n",
        "\n",
        "# Check model accessibility (without downloading)\n",
        "print(\"\\nü§ñ Checking model accessibility:\")\n",
        "model_checks = {\n",
        "    'depth_model': config['models']['depth_model'],\n",
        "    'base_model': config['models']['base_model'],\n",
        "    'controlnet_model': config['models']['controlnet_model']\n",
        "}\n",
        "\n",
        "for model_type, model_name in model_checks.items():\n",
        "    try:\n",
        "        # Just check if we can create the model config (doesn't download)\n",
        "        if 'depth-anything' in model_name.lower():\n",
        "            print(f\"‚úÖ {model_type}: {model_name} (Depth-Anything-V2 format recognized)\")\n",
        "        elif 'flux' in model_name.lower():\n",
        "            print(f\"‚úÖ {model_type}: {model_name} (FLUX format recognized)\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {model_type}: {model_name} (format not specifically recognized)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {model_type}: {model_name} - {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Setup Results and Environment Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect comprehensive environment information\n",
        "environment_info = {\n",
        "    'platform': platform.system(),\n",
        "    'python_version': sys.version.split()[0],\n",
        "    'torch_version': torch.__version__,\n",
        "    'cuda_available': torch.cuda.is_available(),\n",
        "    'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
        "    'optimal_batch_size': optimal_batch_size,\n",
        "    'directories_created': created_dirs,\n",
        "    'installed_packages': installed_packages,\n",
        "    'failed_packages': failed_packages,\n",
        "    'import_results': import_results,\n",
        "    'hardware_info': hardware_info\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    environment_info.update({\n",
        "        'gpu_name': torch.cuda.get_device_name(0),\n",
        "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3),\n",
        "        'cuda_version': torch.version.cuda\n",
        "    })\n",
        "\n",
        "# Save setup results\n",
        "setup_results = {\n",
        "    'environment_info': environment_info,\n",
        "    'configuration': config,\n",
        "    'setup_status': 'completed',\n",
        "    'installation_metrics': installation_metrics,\n",
        "    'model_compatibility': {\n",
        "        'depth_anything_v2_ready': import_results.get('timm', False) and import_results.get('einops', False),\n",
        "        'flux_ready': import_results.get('diffusers', False) and hardware_info['flux_compatibility'] != 'none',\n",
        "        'performance_optimized': import_results.get('xformers', False) or import_results.get('flash-attn', False)\n",
        "    }\n",
        "}\n",
        "\n",
        "setup_metadata = {\n",
        "    'setup_time': '2024-01-12T00:00:00Z',  # This would be actual timestamp\n",
        "    'notebook_version': '2.0_enhanced_flux_depth_anything_v2',\n",
        "    'utilities_version': '1.0',\n",
        "    'models_supported': ['Depth-Anything-V2-Large', 'FLUX.1-dev', 'FLUX-ControlNet-Depth']\n",
        "}\n",
        "\n",
        "try:\n",
        "    success = save_stage_results(setup_results, \"setup\", setup_metadata)\n",
        "    if success:\n",
        "        print(\"‚úÖ Setup results saved successfully\")\n",
        "        print(f\"üìÅ Results saved to: {output_dir / 'setup'}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not save setup results\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error saving setup results: {e}\")\n",
        "\n",
        "# Display final setup summary\n",
        "print(\"\\nüéâ Enhanced Setup and Installation Completed!\")\n",
        "print(f\"\\nüìä Setup Summary:\")\n",
        "print(f\"  ü§ñ Models: Depth-Anything-V2 + FLUX.1-dev ready\")\n",
        "print(f\"  üíæ GPU Memory: {hardware_info.get('gpu_memory_gb', 0):.1f} GB ({hardware_info['flux_compatibility']} for FLUX)\")\n",
        "print(f\"  üîß Batch Size: {optimal_batch_size}\")\n",
        "print(f\"  ‚ö° Mixed Precision: {config['models']['mixed_precision']}\")\n",
        "print(f\"  üì¶ Packages: {installation_metrics['installed_successfully']}/{installation_metrics['total_packages']} installed\")\n",
        "\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"1. üìÇ Add your real beach images to the data/real/images/ directory\")\n",
        "print(\"2. ‚ñ∂Ô∏è Run notebook 02: Data Import and Preprocessing\")\n",
        "print(\"3. üîÑ Continue with notebook 03: Depth-Anything-V2 Extraction\")\n",
        "print(\"4. üé® Proceed to notebook 05: FLUX ControlNet Synthetic Generation\")\n",
        "\n",
        "if failed_packages:\n",
        "    print(\"\\n‚ö†Ô∏è Note: Some packages failed to install. The pipeline may still work, but performance could be affected.\")\n",
        "    \n",
        "if hardware_info['flux_compatibility'] in ['minimal', 'insufficient']:\n",
        "    print(\"\\nüí° Hardware Recommendation: Consider upgrading GPU memory for optimal FLUX.1-dev performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up large variables to free memory\n",
        "large_variables = [\n",
        "    'setup_results',\n",
        "    'environment_info',\n",
        "    'installation_metrics'\n",
        "]\n",
        "\n",
        "try:\n",
        "    cleanup_variables(large_variables)\n",
        "    print(\"‚úÖ Memory cleanup completed\")\n",
        "    \n",
        "    # Show final memory status\n",
        "    final_memory = monitor_memory()\n",
        "    print(f\"üíª Final system memory usage: {final_memory.get('system_percent', 0):.1f}%\")\n",
        "    if 'gpu_percent' in final_memory:\n",
        "        print(f\"üéÆ Final GPU memory usage: {final_memory.get('gpu_percent', 0):.1f}%\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Memory cleanup warning: {e}\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for Depth-Anything-V2 and FLUX.1-dev pipeline!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}