{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUUmqfGsglhJ"
      },
      "source": [
        "# SwellSight Wave Analysis Model - Model Evaluation and Validation\n",
        "\n",
        "This notebook provides comprehensive evaluation and validation of the trained SwellSight wave analysis model.\n",
        "\n",
        "## Overview\n",
        "- Load trained model and evaluation data\n",
        "- Perform comprehensive model validation\n",
        "- Generate performance metrics and visualizations\n",
        "- Analyze model predictions on test data\n",
        "- Create deployment readiness assessment\n",
        "- Generate final pipeline summary\n",
        "\n",
        "## Prerequisites\n",
        "- Complete execution of all previous notebooks (01-07)\n",
        "- Trained model available from notebook 06\n",
        "- Test dataset prepared and validated\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iZwJNZIglhL"
      },
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HTTeX7gglhL",
        "outputId": "6e60e342-e624-45a8-a82f-5270a2afcf23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úì Google Drive mounted successfully\n",
            "‚úì Project directory found: /content/drive/MyDrive/SwellSight\n",
            "‚úì Added /content/drive/MyDrive/SwellSight to sys.path\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if running in Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "\n",
        "    try:\n",
        "        # Attempt 1: Standard mount\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úì Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Standard mount failed: {e}\")\n",
        "\n",
        "        # Attempt 2: Force remount with extended timeout (robust fallback)\n",
        "        print(\"Trying force remount with extended timeout...\")\n",
        "        try:\n",
        "            drive.mount('/content/drive', force_remount=True, timeout_ms=300000)\n",
        "            print(\"‚úì Force remount successful\")\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå Critical failure mounting drive: {e2}\")\n",
        "            raise\n",
        "\n",
        "    # Verify the specific project path exists\n",
        "    # Adjust this path if your folder structure changes\n",
        "    PROJECT_PATH = Path('/content/drive/MyDrive/SwellSight')\n",
        "    if PROJECT_PATH.exists():\n",
        "        print(f\"‚úì Project directory found: {PROJECT_PATH}\")\n",
        "        # Add project path to sys.path for module imports\n",
        "        if str(PROJECT_PATH) not in sys.path:\n",
        "            sys.path.append(str(PROJECT_PATH))\n",
        "            print(f\"‚úì Added {PROJECT_PATH} to sys.path\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Project directory not found at: {PROJECT_PATH}\")\n",
        "else:\n",
        "    print(\"Not running in Google Colab. Skipping Drive mount.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c17d822"
      },
      "source": [
        "Let's inspect the `swellsight` directory to understand its structure and check for necessary `__init__.py` files, which are crucial for Python to recognize it as a package. This will help us diagnose the `ModuleNotFoundError`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fccb71fe",
        "outputId": "fb4a6342-7e5a-4a05-c7af-94c831e585b8"
      },
      "source": [
        "import os\n",
        "\n",
        "project_path = '/content/drive/MyDrive/SwellSight'\n",
        "swellsight_path = os.path.join(project_path, 'final_model.pth')\n",
        "\n",
        "print(f\"Contents of {swellsight_path}:\")\n",
        "if os.path.exists(swellsight_path):\n",
        "    for root, dirs, files in os.walk(swellsight_path):\n",
        "        level = root.replace(swellsight_path, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print(f'{subindent}{f}')\n",
        "else:\n",
        "    print(f\"Directory not found: {swellsight_path}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/drive/MyDrive/SwellSight/final_model.pth:\n",
            "Directory not found: /content/drive/MyDrive/SwellSight/final_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwy7xYiAglhM",
        "outputId": "957b1cd2-ef41-4276-a861-62f66637d9c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "print(\"‚úì Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZAlxZ3IglhN",
        "outputId": "ab97ce31-d6b8-42d1-b3f7-46bdf5c0b790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration loaded from user-specified file: /content/drive/MyDrive/SwellSight/data/metadata/pipeline_config.json\n",
            "‚úì Configuration processing complete.\n",
            "  Final checkpoints_path: /content/drive/MyDrive/SwellSight/checkpoints\n",
            "  Final metadata_path: /content/drive/MyDrive/SwellSight/data/metadata\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "try:\n",
        "    # Check if SWELLSIGHT_CONFIG is already defined (e.g., from a previous notebook)\n",
        "    CONFIG = globals().get('SWELLSIGHT_CONFIG')\n",
        "\n",
        "    if CONFIG is None:\n",
        "        # User specified a specific config file\n",
        "        user_config_file = Path('/content/drive/MyDrive/SwellSight/data/metadata/pipeline_config.json')\n",
        "\n",
        "        if user_config_file.exists():\n",
        "            with open(user_config_file, 'r') as f:\n",
        "                CONFIG = json.load(f)\n",
        "            print(f\"‚úì Configuration loaded from user-specified file: {user_config_file}\")\n",
        "        else:\n",
        "            # Fallback to session_config.json or default if user_config_file not found\n",
        "            session_config_file = Path('/content/drive/MyDrive/SwellSight/session_config.json') if IN_COLAB else Path('SwellSight/session_config.json')\n",
        "            if session_config_file.exists():\n",
        "                with open(session_config_file, 'r') as f:\n",
        "                    CONFIG = json.load(f)\n",
        "                print(f\"‚úì Configuration loaded from session_config.json: {session_config_file}\")\n",
        "            else:\n",
        "                # Default configuration if no config file is found\n",
        "                CONFIG = {\n",
        "                    'paths': {\n",
        "                        'checkpoints_path': '/content/drive/MyDrive/SwellSight/checkpoints',\n",
        "                        'metadata_path': '/content/drive/MyDrive/SwellSight/data/metadata',\n",
        "                        'real_data_path': 'data/real',\n",
        "                        'test_output_path': 'test_output'\n",
        "                    }\n",
        "                }\n",
        "                print(\"‚úì Default configuration applied.\")\n",
        "\n",
        "    # Ensure CONFIG['paths'] exists and update default paths if not explicitly set in config file\n",
        "    if 'paths' not in CONFIG:\n",
        "        CONFIG['paths'] = {}\n",
        "    # Ensure user-provided checkpoints and metadata paths are set if not present\n",
        "    if 'checkpoints_path' not in CONFIG['paths']:\n",
        "        CONFIG['paths']['checkpoints_path'] = '/content/drive/MyDrive/SwellSight/checkpoints'\n",
        "    if 'metadata_path' not in CONFIG['paths']:\n",
        "        CONFIG['paths']['metadata_path'] = '/content/drive/MyDrive/SwellSight/data/metadata'\n",
        "    if 'real_data_path' not in CONFIG['paths']:\n",
        "        CONFIG['paths']['real_data_path'] = 'data/real'\n",
        "    if 'test_output_path' not in CONFIG['paths']:\n",
        "        CONFIG['paths']['test_output_path'] = 'test_output'\n",
        "\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger('SwellSight_Evaluation')\n",
        "\n",
        "    print(\"‚úì Configuration processing complete.\")\n",
        "    print(f\"  Final checkpoints_path: {CONFIG['paths']['checkpoints_path']}\")\n",
        "    print(f\"  Final metadata_path: {CONFIG['paths']['metadata_path']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error loading configuration: {e}\")\n",
        "    # Fallback to a minimal CONFIG if there's an error\n",
        "    CONFIG = {\n",
        "        'paths': {\n",
        "            'checkpoints_path': '/content/drive/MyDrive/SwellSight/checkpoints',\n",
        "            'metadata_path': '/content/drive/MyDrive/SwellSight/data/metadata',\n",
        "            'real_data_path': 'data/real',\n",
        "            'test_output_path': 'test_output'\n",
        "        }\n",
        "    }\n",
        "    print(\"‚ö†Ô∏è  Fallback minimal configuration applied due to error.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvsbeVrbglhN"
      },
      "source": [
        "## 2. Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "PCOIT8q2glhO",
        "outputId": "667b2942-417d-4c22-a0a3-2ae87ba38448"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'swellsight'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-212139215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mswellsight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwave_analysis_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaveAnalysisModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mswellsight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Load model from checkpoint file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'swellsight'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from swellsight.models.wave_analysis_model import WaveAnalysisModel\n",
        "from swellsight.config.model_config import ModelConfig\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path: Path):\n",
        "    \"\"\"Load model from checkpoint file.\"\"\"\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "    if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
        "        metadata = ckpt.get('metadata', {})\n",
        "        cfg_dict = metadata.get('model_config', {})\n",
        "        config = ModelConfig.from_dict(cfg_dict) if cfg_dict else ModelConfig()\n",
        "        model = WaveAnalysisModel(config)\n",
        "        model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
        "        return model, metadata, ckpt.get('training_history', {})\n",
        "\n",
        "    if isinstance(ckpt, nn.Module):\n",
        "        return ckpt, {}, {}\n",
        "\n",
        "    config = ModelConfig()\n",
        "    model = WaveAnalysisModel(config)\n",
        "    try:\n",
        "        model.load_state_dict(ckpt)\n",
        "        return model, {}, {}\n",
        "    except:\n",
        "        return None, {}, {}\n",
        "\n",
        "# Find and load specific model\n",
        "trained_model = None\n",
        "model_metadata = {}\n",
        "training_history = {}\n",
        "\n",
        "checkpoints_dir = Path(CONFIG['paths'].get('checkpoints_path', 'checkpoints'))\n",
        "model_file = checkpoints_dir / 'final_model.pth' # Explicitly use final_model.pth\n",
        "\n",
        "if model_file.exists():\n",
        "    print(f\"Loading specific model: {model_file}\")\n",
        "    trained_model, model_metadata, training_history = load_model_from_checkpoint(model_file)\n",
        "    if trained_model:\n",
        "        trained_model.eval()\n",
        "        print(\"‚úì Model loaded and set to evaluation mode\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to load model from \" + str(model_file))\n",
        "else:\n",
        "    print(f\"‚ùå Specified model file not found: {model_file}\")\n",
        "    print(f\"  Looked in directory: {checkpoints_dir}\")\n",
        "    print(f\"  Available files: {list(checkpoints_dir.glob('*')) if checkpoints_dir.exists() else 'Directory does not exist.'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TvpKNoOglhP"
      },
      "source": [
        "## 3. Prepare Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBDN2mZcglhP"
      },
      "outputs": [],
      "source": [
        "class WaveTestDataset(Dataset):\n",
        "    def __init__(self, images_dir: Path, annotations: dict = None, input_size=(768, 768)):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.files = sorted([p for p in self.images_dir.glob('**/*')\n",
        "                           if p.suffix.lower() in ('.jpg', '.jpeg', '.png')])\n",
        "        self.annotations = annotations or {}\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize(input_size),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image_tensor = self.transform(image)\n",
        "        label = self.annotations.get(path.name)\n",
        "        return {\n",
        "            'image': image_tensor,\n",
        "            'path': str(path),\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "# Load annotations\n",
        "annotations = {}\n",
        "annotation_files = [\n",
        "    Path(CONFIG['paths'].get('metadata_path', 'data/metadata')) / 'test_annotations.json',\n",
        "    Path('test_output') / 'pipeline_results_20251231_230407.json'\n",
        "]\n",
        "\n",
        "for ann_file in annotation_files:\n",
        "    if ann_file.exists():\n",
        "        try:\n",
        "            with open(ann_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list):\n",
        "                    for item in data:\n",
        "                        if 'file_name' in item:\n",
        "                            annotations[item['file_name']] = item\n",
        "                elif isinstance(data, dict):\n",
        "                    annotations.update(data)\n",
        "            print(f\"‚úì Loaded annotations from {ann_file}\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = None\n",
        "test_images_path = Path(CONFIG['paths'].get('test_output_path', 'data/real'))\n",
        "if test_images_path.exists():\n",
        "    test_dataset = WaveTestDataset(test_images_path, annotations)\n",
        "    print(f\"‚úì Test dataset created with {len(test_dataset)} images\")\n",
        "else:\n",
        "    print(f\"‚ùå Test images directory not found: {test_images_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ihXCsOglhQ"
      },
      "source": [
        "## 4. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54rsJajRglhQ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset, batch_size=8, device='cpu'):\n",
        "    \"\"\"Evaluate model on test dataset.\"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    paths = []\n",
        "\n",
        "    model.to(device)\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Evaluating'):\n",
        "            images = batch['image'].to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Extract predictions\n",
        "            heights = outputs['height'].squeeze(-1).cpu().numpy()\n",
        "            wave_types = outputs['wave_type'].argmax(dim=1).cpu().numpy()\n",
        "            directions = outputs['direction'].argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(batch['path'])):\n",
        "                pred = {\n",
        "                    'height': float(heights[i]),\n",
        "                    'wave_type': int(wave_types[i]),\n",
        "                    'direction': int(directions[i])\n",
        "                }\n",
        "                predictions.append(pred)\n",
        "                paths.append(batch['path'][i])\n",
        "                labels.append(batch['label'][i])\n",
        "\n",
        "    inference_time = time.time() - start_time\n",
        "    return paths, predictions, labels, {'inference_time': inference_time, 'num_samples': len(dataset)}\n",
        "\n",
        "# Run evaluation if model and dataset are available\n",
        "evaluation_results = {}\n",
        "if trained_model and test_dataset and len(test_dataset) > 0:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Running evaluation on {device}...\")\n",
        "\n",
        "    paths, predictions, labels, metrics = evaluate_model(trained_model, test_dataset, device=device)\n",
        "\n",
        "    evaluation_results = {\n",
        "        'paths': paths,\n",
        "        'predictions': predictions,\n",
        "        'labels': labels,\n",
        "        'metrics': metrics\n",
        "    }\n",
        "\n",
        "    print(f\"‚úì Evaluation completed in {metrics['inference_time']:.2f}s\")\n",
        "    print(f\"  Average inference time: {metrics['inference_time']/metrics['num_samples']:.3f}s per image\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot run evaluation - model or dataset not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDilcdEgglhR"
      },
      "source": [
        "## 5. Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYlyjDIQglhR"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(predictions, labels):\n",
        "    \"\"\"Compute performance metrics when ground truth labels are available.\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Separate predictions and labels by task\n",
        "    heights_true, heights_pred = [], []\n",
        "    wave_types_true, wave_types_pred = [], []\n",
        "    directions_true, directions_pred = [], []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        if label is None:\n",
        "            continue\n",
        "\n",
        "        if isinstance(label, dict):\n",
        "            if 'height' in label and label['height'] is not None:\n",
        "                heights_true.append(float(label['height']))\n",
        "                heights_pred.append(float(pred['height']))\n",
        "\n",
        "            if 'wave_type' in label and label['wave_type'] is not None:\n",
        "                wave_types_true.append(int(label['wave_type']))\n",
        "                wave_types_pred.append(int(pred['wave_type']))\n",
        "\n",
        "            if 'direction' in label and label['direction'] is not None:\n",
        "                directions_true.append(int(label['direction']))\n",
        "                directions_pred.append(int(pred['direction']))\n",
        "\n",
        "    # Height regression metrics\n",
        "    if heights_true:\n",
        "        metrics['height'] = {\n",
        "            'mse': float(mean_squared_error(heights_true, heights_pred)),\n",
        "            'mae': float(mean_absolute_error(heights_true, heights_pred)),\n",
        "            'r2': float(r2_score(heights_true, heights_pred)),\n",
        "            'samples': len(heights_true)\n",
        "        }\n",
        "\n",
        "    # Wave type classification metrics\n",
        "    if wave_types_true:\n",
        "        metrics['wave_type'] = {\n",
        "            'accuracy': float(accuracy_score(wave_types_true, wave_types_pred)),\n",
        "            'f1_macro': float(f1_score(wave_types_true, wave_types_pred, average='macro')),\n",
        "            'samples': len(wave_types_true)\n",
        "        }\n",
        "\n",
        "    # Direction classification metrics\n",
        "    if directions_true:\n",
        "        metrics['direction'] = {\n",
        "            'accuracy': float(accuracy_score(directions_true, directions_pred)),\n",
        "            'f1_macro': float(f1_score(directions_true, directions_pred, average='macro')),\n",
        "            'samples': len(directions_true)\n",
        "        }\n",
        "\n",
        "    return metrics, (heights_true, heights_pred), (wave_types_true, wave_types_pred), (directions_true, directions_pred)\n",
        "\n",
        "# Compute metrics if evaluation was successful\n",
        "performance_metrics = {}\n",
        "if evaluation_results:\n",
        "    metrics, height_data, wave_data, dir_data = compute_metrics(\n",
        "        evaluation_results['predictions'],\n",
        "        evaluation_results['labels']\n",
        "    )\n",
        "    performance_metrics = metrics\n",
        "\n",
        "    print(\"üìä Performance Metrics:\")\n",
        "    for task, task_metrics in metrics.items():\n",
        "        print(f\"\\n{task.upper()}:\")\n",
        "        for metric, value in task_metrics.items():\n",
        "            if metric != 'samples':\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {metric}: {value}\")\n",
        "else:\n",
        "    print(\"‚ùå No evaluation results available for metrics computation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9edfvk4glhT"
      },
      "source": [
        "## 6. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMJ5iq6LglhT"
      },
      "outputs": [],
      "source": [
        "# Create visualizations if we have evaluation results\n",
        "if evaluation_results and performance_metrics:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Height scatter plot\n",
        "    if 'height' in performance_metrics:\n",
        "        heights_true, heights_pred = height_data\n",
        "        axes[0].scatter(heights_true, heights_pred, alpha=0.6)\n",
        "        min_h, max_h = min(heights_true), max(heights_true)\n",
        "        axes[0].plot([min_h, max_h], [min_h, max_h], 'r--', alpha=0.8)\n",
        "        axes[0].set_xlabel('True Height (m)')\n",
        "        axes[0].set_ylabel('Predicted Height (m)')\n",
        "        axes[0].set_title(f'Height Prediction\\nR¬≤ = {performance_metrics[\"height\"][\"r2\"]:.3f}')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[0].text(0.5, 0.5, 'No height labels\\navailable', ha='center', va='center', transform=axes[0].transAxes)\n",
        "        axes[0].set_title('Height Prediction')\n",
        "\n",
        "    # Wave type confusion matrix\n",
        "    if 'wave_type' in performance_metrics:\n",
        "        wave_true, wave_pred = wave_data\n",
        "        cm = confusion_matrix(wave_true, wave_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "        axes[1].set_xlabel('Predicted')\n",
        "        axes[1].set_ylabel('True')\n",
        "        axes[1].set_title(f'Wave Type\\nAccuracy = {performance_metrics[\"wave_type\"][\"accuracy\"]:.3f}')\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'No wave type labels\\navailable', ha='center', va='center', transform=axes[1].transAxes)\n",
        "        axes[1].set_title('Wave Type Classification')\n",
        "\n",
        "    # Direction confusion matrix\n",
        "    if 'direction' in performance_metrics:\n",
        "        dir_true, dir_pred = dir_data\n",
        "        cm = confusion_matrix(dir_true, dir_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[2])\n",
        "        axes[2].set_xlabel('Predicted')\n",
        "        axes[2].set_ylabel('True')\n",
        "        axes[2].set_title(f'Direction\\nAccuracy = {performance_metrics[\"direction\"][\"accuracy\"]:.3f}')\n",
        "    else:\n",
        "        axes[2].text(0.5, 0.5, 'No direction labels\\navailable', ha='center', va='center', transform=axes[2].transAxes)\n",
        "        axes[2].set_title('Direction Classification')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå No evaluation results available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68gQjfu5glhU"
      },
      "source": [
        "## 7. Deployment Readiness Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1NWuM_CglhU"
      },
      "outputs": [],
      "source": [
        "def assess_deployment_readiness(model, model_metadata, performance_metrics):\n",
        "    \"\"\"Assess if model is ready for deployment.\"\"\"\n",
        "    checks = {}\n",
        "\n",
        "    if model is not None:\n",
        "        # Model size check\n",
        "        param_count = sum(p.numel() for p in model.parameters())\n",
        "        checks['model_parameters'] = param_count\n",
        "        checks['model_size_mb'] = param_count * 4 / (1024 * 1024)  # Assuming float32\n",
        "\n",
        "        # JIT compilation test\n",
        "        try:\n",
        "            model.eval()\n",
        "            sample_input = torch.randn(1, 3, 768, 768)\n",
        "            traced_model = torch.jit.trace(model, sample_input)\n",
        "            checks['jit_compatible'] = True\n",
        "        except Exception as e:\n",
        "            checks['jit_compatible'] = False\n",
        "            checks['jit_error'] = str(e)\n",
        "\n",
        "        # Inference speed test\n",
        "        try:\n",
        "            model.eval()\n",
        "            sample_input = torch.randn(1, 3, 768, 768)\n",
        "\n",
        "            # Warmup\n",
        "            for _ in range(3):\n",
        "                _ = model(sample_input)\n",
        "\n",
        "            # Timing\n",
        "            start_time = time.time()\n",
        "            for _ in range(10):\n",
        "                _ = model(sample_input)\n",
        "            avg_inference_time = (time.time() - start_time) / 10\n",
        "\n",
        "            checks['avg_inference_time_s'] = avg_inference_time\n",
        "            checks['inference_fps'] = 1.0 / avg_inference_time\n",
        "        except Exception as e:\n",
        "            checks['inference_speed_error'] = str(e)\n",
        "\n",
        "    # Performance thresholds (adjust based on requirements)\n",
        "    deployment_ready = True\n",
        "    issues = []\n",
        "\n",
        "    if performance_metrics:\n",
        "        if 'height' in performance_metrics:\n",
        "            if performance_metrics['height']['r2'] < 0.7:\n",
        "                issues.append(f\"Height R¬≤ too low: {performance_metrics['height']['r2']:.3f} < 0.7\")\n",
        "                deployment_ready = False\n",
        "\n",
        "        if 'wave_type' in performance_metrics:\n",
        "            if performance_metrics['wave_type']['accuracy'] < 0.8:\n",
        "                issues.append(f\"Wave type accuracy too low: {performance_metrics['wave_type']['accuracy']:.3f} < 0.8\")\n",
        "                deployment_ready = False\n",
        "\n",
        "        if 'direction' in performance_metrics:\n",
        "            if performance_metrics['direction']['accuracy'] < 0.8:\n",
        "                issues.append(f\"Direction accuracy too low: {performance_metrics['direction']['accuracy']:.3f} < 0.8\")\n",
        "                deployment_ready = False\n",
        "\n",
        "    if checks.get('avg_inference_time_s', 0) > 1.0:\n",
        "        issues.append(f\"Inference too slow: {checks['avg_inference_time_s']:.3f}s > 1.0s\")\n",
        "        deployment_ready = False\n",
        "\n",
        "    checks['deployment_ready'] = deployment_ready\n",
        "    checks['issues'] = issues\n",
        "\n",
        "    return checks\n",
        "\n",
        "# Run deployment assessment\n",
        "deployment_assessment = assess_deployment_readiness(trained_model, model_metadata, performance_metrics)\n",
        "\n",
        "print(\"üöÄ Deployment Readiness Assessment:\")\n",
        "print(f\"\\nModel Statistics:\")\n",
        "if 'model_parameters' in deployment_assessment:\n",
        "    print(f\"  Parameters: {deployment_assessment['model_parameters']:,}\")\n",
        "    print(f\"  Model size: {deployment_assessment['model_size_mb']:.1f} MB\")\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "if 'avg_inference_time_s' in deployment_assessment:\n",
        "    print(f\"  Inference time: {deployment_assessment['avg_inference_time_s']:.3f}s\")\n",
        "    print(f\"  Throughput: {deployment_assessment['inference_fps']:.1f} FPS\")\n",
        "\n",
        "print(f\"\\nCompatibility:\")\n",
        "print(f\"  JIT compatible: {deployment_assessment.get('jit_compatible', 'Unknown')}\")\n",
        "\n",
        "print(f\"\\nDeployment Status: {'‚úÖ READY' if deployment_assessment['deployment_ready'] else '‚ùå NOT READY'}\")\n",
        "if deployment_assessment['issues']:\n",
        "    print(\"\\nIssues to address:\")\n",
        "    for issue in deployment_assessment['issues']:\n",
        "        print(f\"  - {issue}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09L_oNkhglhV"
      },
      "source": [
        "## 8. Save Results and Generate Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHbN4v9SglhV"
      },
      "outputs": [],
      "source": [
        "# Compile final results\n",
        "final_results = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'model_metadata': model_metadata,\n",
        "    'training_history': training_history,\n",
        "    'evaluation_metrics': evaluation_results.get('metrics', {}),\n",
        "    'performance_metrics': performance_metrics,\n",
        "    'deployment_assessment': deployment_assessment,\n",
        "    'pipeline_summary': {\n",
        "        'goals': [\n",
        "            'Load trained model and evaluation data',\n",
        "            'Perform comprehensive model validation',\n",
        "            'Generate performance metrics and visualizations',\n",
        "            'Analyze model predictions on test data',\n",
        "            'Create deployment readiness assessment',\n",
        "            'Generate final pipeline summary'\n",
        "        ],\n",
        "        'completed': []\n",
        "    }\n",
        "}\n",
        "\n",
        "# Track completed goals\n",
        "if trained_model is not None:\n",
        "    final_results['pipeline_summary']['completed'].append('Load trained model and evaluation data')\n",
        "\n",
        "if performance_metrics:\n",
        "    final_results['pipeline_summary']['completed'].extend([\n",
        "        'Perform comprehensive model validation',\n",
        "        'Generate performance metrics and visualizations',\n",
        "        'Analyze model predictions on test data'\n",
        "    ])\n",
        "\n",
        "if deployment_assessment:\n",
        "    final_results['pipeline_summary']['completed'].append('Create deployment readiness assessment')\n",
        "\n",
        "final_results['pipeline_summary']['completed'].append('Generate final pipeline summary')\n",
        "\n",
        "# Save results\n",
        "output_dir = Path(CONFIG['paths'].get('test_output_path', 'test_output'))\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_file = output_dir / f'model_evaluation_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Evaluation complete! Results saved to: {output_file}\")\n",
        "print(f\"\\nüìã Pipeline Summary:\")\n",
        "print(f\"  Goals completed: {len(final_results['pipeline_summary']['completed'])}/{len(final_results['pipeline_summary']['goals'])}\")\n",
        "for goal in final_results['pipeline_summary']['completed']:\n",
        "    print(f\"  ‚úì {goal}\")\n",
        "\n",
        "# Store results in global variable for next notebooks\n",
        "EVALUATION_RESULTS = final_results\n",
        "print(\"\\n‚úì Results stored in EVALUATION_RESULTS variable for future use\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}