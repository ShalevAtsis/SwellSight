{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHkio_Kw8ruM"
   },
   "source": [
    "# SwellSight Real-to-Synthetic Pipeline - Enhanced Data Import and Preprocessing\n",
    "\n",
    "This enhanced notebook handles the import and preprocessing of real beach camera images with comprehensive data validation, quality checks, memory optimization, and robust error handling.\n",
    "\n",
    "## Overview\n",
    "Enhanced features include:\n",
    "- **Comprehensive Data Validation**: Image quality validation with resolution, format, and corruption detection\n",
    "- **Memory-Aware Batch Processing**: Dynamic batch sizing based on available memory\n",
    "- **Robust Error Handling**: Retry logic for file operations with exponential backoff\n",
    "- **Progress Tracking**: Progress bars with memory usage display\n",
    "- **Quality Summary Reporting**: Detailed quality statistics and recommendations\n",
    "- **Standardized Data Format**: Pipeline integration with shared utility functions\n",
    "\n",
    "## Pipeline Integration\n",
    "This notebook implements enhanced data preparation:\n",
    "1. **Configuration Loading**: Load shared configuration with validation\n",
    "2. **Image Discovery**: Find all supported image formats with validation\n",
    "3. **Quality Validation**: Comprehensive image quality assessment\n",
    "4. **Memory Optimization**: Dynamic batch sizing and memory monitoring\n",
    "5. **Error Recovery**: Robust error handling with retry mechanisms\n",
    "6. **Data Standardization**: Prepare data in standardized format for next stages\n",
    "\n",
    "## Prerequisites\n",
    "- Complete execution of `01_Setup_and_Installation_Enhanced.ipynb`\n",
    "- Real beach images available in configured data directory\n",
    "- Shared utility functions properly installed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTA04Gkq8ruR"
   },
   "source": [
    "## 1. Load Configuration and Initialize Enhanced Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7pbYWDg8ruU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageStat\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from collections import Counter\n",
    "\n",
    "# Import enhanced utility functions\n",
    "try:\n",
    "    from utils.config_manager import ConfigManager\n",
    "    from utils.data_validator import DataValidator\n",
    "    from utils.memory_optimizer import MemoryOptimizer\n",
    "    from utils.error_handler import ErrorHandler\n",
    "    from utils.progress_tracker import ProgressTracker\n",
    "    from utils.data_flow_manager import DataFlowManager\n",
    "    print(\"‚úì Enhanced utility functions loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing utility functions: {e}\")\n",
    "    print(\"Please ensure utils/ directory is in your Python path\")\n",
    "    raise\n",
    "\n",
    "# Check environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úì Google Drive mounted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Drive mount failed: {e}\")\n",
    "        try:\n",
    "            drive.mount('/content/drive', force_remount=True, timeout_ms=300000)\n",
    "            print(\"‚úì Force remount successful\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Critical failure mounting drive: {e2}\")\n",
    "            raise\n",
    "\n",
    "# Initialize enhanced components\n",
    "print(\"\\nüîß Initializing enhanced pipeline components...\")\n",
    "\n",
    "try:\n",
    "    # Initialize configuration manager\n",
    "    config_manager = ConfigManager()\n",
    "    CONFIG = config_manager.load_config()\n",
    "    \n",
    "    # Initialize other components\n",
    "    data_validator = DataValidator(quality_threshold=CONFIG.get('processing', {}).get('quality_threshold', 0.7))\n",
    "    memory_optimizer = MemoryOptimizer(safety_margin=0.1)\n",
    "    error_handler = ErrorHandler(max_retries=3, backoff_factor=2.0)\n",
    "    progress_tracker = ProgressTracker()\n",
    "    data_flow_manager = DataFlowManager()\n",
    "    \n",
    "    print(\"‚úì Enhanced components initialized successfully\")\n",
    "    \n",
    "    # Validate dependencies\n",
    "    dependency_status = data_flow_manager.check_dependencies('data_preprocessing')\n",
    "    if not dependency_status['all_satisfied']:\n",
    "        print(f\"‚ö†Ô∏è  Missing dependencies: {dependency_status['missing_dependencies']}\")\n",
    "        print(\"Please complete the setup notebook first\")\n",
    "    else:\n",
    "        print(\"‚úì All dependencies satisfied\")\n",
    "    \n",
    "    # Set up paths from configuration\n",
    "    REAL_IMAGES_PATH = Path(CONFIG['paths']['real_images_path'])\n",
    "    OUTPUT_PATH = Path(CONFIG['paths']['output_path'])\n",
    "    \n",
    "    print(f\"\\nüìÅ Configuration loaded:\")\n",
    "    print(f\"   Session ID: {CONFIG['session']['session_id']}\")\n",
    "    print(f\"   Real images path: {REAL_IMAGES_PATH}\")\n",
    "    print(f\"   Output path: {OUTPUT_PATH}\")\n",
    "    print(f\"   Quality threshold: {data_validator.quality_threshold}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize enhanced components: {e}\")\n",
    "    print(\"Falling back to basic configuration loading...\")\n",
    "    \n",
    "    # Fallback to basic configuration loading\n",
    "    try:\n",
    "        if IN_COLAB:\n",
    "            config_file = Path('/content/drive/MyDrive/SwellSight/config.json')\n",
    "        else:\n",
    "            config_file = Path('config.json')\n",
    "        \n",
    "        with open(config_file, 'r') as f:\n",
    "            CONFIG = json.load(f)\n",
    "        \n",
    "        # Initialize basic components\n",
    "        data_validator = DataValidator()\n",
    "        memory_optimizer = MemoryOptimizer()\n",
    "        \n",
    "        REAL_IMAGES_PATH = Path(CONFIG['paths']['real_images_path'])\n",
    "        OUTPUT_PATH = Path(CONFIG['paths']['output_path'])\n",
    "        \n",
    "        print(\"‚úì Basic configuration loaded successfully\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Critical error loading configuration: {e2}\")\n",
    "        raise\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(OUTPUT_PATH / 'data_preprocessing.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\nüöÄ Enhanced data preprocessing notebook ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDO5Qt_YgVSB"
   },
   "source": [
    "## 2. Enhanced Image Discovery with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yITD7KYGgVSC"
   },
   "outputs": [],
   "source": [
    "# Enhanced image discovery with comprehensive validation\n",
    "print(\"üîç Enhanced image discovery and validation...\")\n",
    "\n",
    "# Check if input directory exists\n",
    "if not REAL_IMAGES_PATH.exists():\n",
    "    print(f\"‚ùå Input directory does not exist: {REAL_IMAGES_PATH}\")\n",
    "    print(\"\\nüìù To proceed, please:\")\n",
    "    print(f\"   1. Create the directory: {REAL_IMAGES_PATH}\")\n",
    "    print(\"   2. Add your real beach camera images\")\n",
    "    print(\"   3. Re-run this notebook\")\n",
    "    raise FileNotFoundError(f\"Input directory not found: {REAL_IMAGES_PATH}\")\n",
    "\n",
    "# Get supported formats from validator\n",
    "supported_extensions = list(data_validator.SUPPORTED_FORMATS)\n",
    "print(f\"\\nüîé Searching for images with extensions: {supported_extensions}\")\n",
    "\n",
    "# Discover image files with error handling\n",
    "def discover_images_with_retry():\n",
    "    \"\"\"Discover images with retry logic for network drives\"\"\"\n",
    "    def _discover():\n",
    "        image_paths = []\n",
    "        for ext in supported_extensions:\n",
    "            # Search for both lowercase and uppercase extensions\n",
    "            lower_files = list(REAL_IMAGES_PATH.glob(f'*{ext.lower()}'))\n",
    "            upper_files = list(REAL_IMAGES_PATH.glob(f'*{ext.upper()}'))\n",
    "            image_paths.extend(lower_files + upper_files)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        return sorted(list(set(image_paths)))\n",
    "    \n",
    "    return error_handler.retry_with_backoff(_discover)\n",
    "\n",
    "try:\n",
    "    image_paths = discover_images_with_retry()\n",
    "    \n",
    "    # Count by extension\n",
    "    extension_counts = {}\n",
    "    for path in image_paths:\n",
    "        ext = path.suffix.lower()\n",
    "        extension_counts[ext] = extension_counts.get(ext, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Discovery Results:\")\n",
    "    for ext, count in extension_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"   ‚úÖ {ext}: {count} files\")\n",
    "        else:\n",
    "            print(f\"   ‚ö™ {ext}: 0 files\")\n",
    "    \n",
    "    print(f\"\\n   Total images found: {len(image_paths)}\")\n",
    "    print(f\"   Input directory: {REAL_IMAGES_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during image discovery: {e}\")\n",
    "    print(f\"‚ùå Image discovery failed: {e}\")\n",
    "    raise\n",
    "\n",
    "if not image_paths:\n",
    "    print(f\"\\n‚ùå No image files found in {REAL_IMAGES_PATH}\")\n",
    "    print(f\"\\nüìù Supported formats: {', '.join(supported_extensions)}\")\n",
    "    print(\"\\nüí° Please add real beach camera images to the input directory\")\n",
    "    raise ValueError(\"No images found for processing\")\n",
    "\n",
    "# Apply memory-aware batch sizing for processing limit\n",
    "max_images = CONFIG['processing'].get('max_images_per_session', 500)\n",
    "memory_info = memory_optimizer.monitor_memory_usage()\n",
    "\n",
    "# Estimate memory requirements and adjust batch size if needed\n",
    "estimated_memory_per_image = memory_optimizer.estimate_image_memory_usage()\n",
    "available_memory = memory_info.get('system_available_gb', 4) * 1024**3  # Convert to bytes\n",
    "memory_based_limit = int(available_memory * 0.5 / estimated_memory_per_image)  # Use 50% of available memory\n",
    "\n",
    "effective_limit = min(max_images, memory_based_limit, len(image_paths))\n",
    "\n",
    "if len(image_paths) > effective_limit:\n",
    "    print(f\"\\n‚ö†Ô∏è  Limiting processing to {effective_limit} images\")\n",
    "    print(f\"   Found: {len(image_paths)} images\")\n",
    "    print(f\"   Config limit: {max_images}\")\n",
    "    print(f\"   Memory-based limit: {memory_based_limit}\")\n",
    "    print(f\"   Available memory: {memory_info.get('system_available_gb', 0):.1f}GB\")\n",
    "    image_paths = image_paths[:effective_limit]\n",
    "\n",
    "print(f\"\\n‚úÖ Final image count for processing: {len(image_paths)}\")\n",
    "\n",
    "# Display sample filenames\n",
    "print(f\"\\nüìã Sample filenames:\")\n",
    "for i, path in enumerate(image_paths[:5]):\n",
    "    print(f\"   {i+1}. {path.name}\")\n",
    "if len(image_paths) > 5:\n",
    "    print(f\"   ... and {len(image_paths) - 5} more\")\n",
    "\n",
    "# Memory optimization suggestions\n",
    "suggestions = memory_optimizer.suggest_memory_optimizations(memory_info)\n",
    "if suggestions:\n",
    "    print(f\"\\nüí° Memory optimization suggestions:\")\n",
    "    for suggestion in suggestions[:3]:  # Show top 3\n",
    "        print(f\"   ‚Ä¢ {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baeANcWxgVSJ"
   },
   "source": [
    "## 3. Comprehensive Image Quality Assessment with Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djxx6sHggVSP"
   },
   "outputs": [],
   "source": [
    "# Enhanced image quality assessment with memory-aware batch processing\n",
    "print(\"üîç Comprehensive image quality assessment with memory optimization...\")\n",
    "\n",
    "# Calculate optimal batch size for processing\n",
    "optimal_batch_size = memory_optimizer.get_optimal_batch_size(\n",
    "    item_size=estimated_memory_per_image,\n",
    "    max_batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Processing Configuration:\")\n",
    "print(f\"   Total images: {len(image_paths)}\")\n",
    "print(f\"   Optimal batch size: {optimal_batch_size}\")\n",
    "print(f\"   Quality threshold: {data_validator.quality_threshold}\")\n",
    "print(f\"   Estimated memory per image: {estimated_memory_per_image / (1024*1024):.1f}MB\")\n",
    "\n",
    "# Initialize tracking variables\n",
    "image_metadata = []\n",
    "valid_images = []\n",
    "invalid_images = []\n",
    "processing_errors = []\n",
    "quality_stats = {\n",
    "    'total_size_mb': 0,\n",
    "    'resolutions': [],\n",
    "    'aspect_ratios': [],\n",
    "    'formats': {},\n",
    "    'color_modes': {},\n",
    "    'quality_scores': [],\n",
    "    'brightness_scores': [],\n",
    "    'contrast_scores': [],\n",
    "    'sharpness_scores': []\n",
    "}\n",
    "\n",
    "# Process images in memory-optimized batches with progress tracking\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "batch_count = 0\n",
    "\n",
    "with memory_optimizer.memory_monitor(log_usage=True) as monitor:\n",
    "    # Create progress tracker\n",
    "    progress = progress_tracker.create_progress_bar(\n",
    "        total=len(image_paths),\n",
    "        description=\"Analyzing images\",\n",
    "        show_memory=True\n",
    "    )\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, len(image_paths), optimal_batch_size):\n",
    "        batch_end = min(batch_start + optimal_batch_size, len(image_paths))\n",
    "        batch_paths = image_paths[batch_start:batch_end]\n",
    "        batch_count += 1\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing batch {batch_count} ({len(batch_paths)} images)...\")\n",
    "        \n",
    "        # Process each image in the batch\n",
    "        batch_results = []\n",
    "        \n",
    "        for image_path in batch_paths:\n",
    "            try:\n",
    "                # Validate image quality with comprehensive checks\n",
    "                def validate_image():\n",
    "                    return data_validator.validate_image_quality(str(image_path))\n",
    "                \n",
    "                # Use error handler for robust validation\n",
    "                validation_result = error_handler.retry_with_backoff(validate_image)\n",
    "                \n",
    "                # Extract metadata from validation result\n",
    "                metadata = {\n",
    "                    'filename': image_path.name,\n",
    "                    'path': str(image_path),\n",
    "                    'valid': validation_result['valid'],\n",
    "                    'quality_score': validation_result['score'],\n",
    "                    'issues': validation_result['issues'],\n",
    "                    **validation_result['metrics']\n",
    "                }\n",
    "                \n",
    "                image_metadata.append(metadata)\n",
    "                batch_results.append(metadata)\n",
    "                \n",
    "                if validation_result['valid']:\n",
    "                    valid_images.append(image_path)\n",
    "                    \n",
    "                    # Update quality statistics\n",
    "                    metrics = validation_result['metrics']\n",
    "                    quality_stats['total_size_mb'] += metrics.get('file_size', 0) / (1024*1024)\n",
    "                    quality_stats['resolutions'].append(f\"{metrics.get('width', 0)}x{metrics.get('height', 0)}\")\n",
    "                    quality_stats['aspect_ratios'].append(metrics.get('width', 1) / max(metrics.get('height', 1), 1))\n",
    "                    quality_stats['quality_scores'].append(validation_result['score'])\n",
    "                    quality_stats['brightness_scores'].append(metrics.get('brightness_mean', 0))\n",
    "                    quality_stats['contrast_scores'].append(metrics.get('contrast', 0))\n",
    "                    quality_stats['sharpness_scores'].append(metrics.get('sharpness', 0))\n",
    "                    \n",
    "                    # Count formats and modes\n",
    "                    format_name = metrics.get('format', 'Unknown')\n",
    "                    mode = metrics.get('mode', 'Unknown')\n",
    "                    quality_stats['formats'][format_name] = quality_stats['formats'].get(format_name, 0) + 1\n",
    "                    quality_stats['color_modes'][mode] = quality_stats['color_modes'].get(mode, 0) + 1\n",
    "                    \n",
    "                else:\n",
    "                    invalid_images.append(image_path)\n",
    "                    logger.warning(f\"Invalid image {image_path.name}: {validation_result['issues']}\")\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "                # Update progress with memory info\n",
    "                current_memory = memory_optimizer.monitor_memory_usage()\n",
    "                progress_tracker.update_progress(\n",
    "                    progress, \n",
    "                    processed_count, \n",
    "                    additional_info=f\"Memory: {current_memory.get('system_percent', 0):.1f}%\"\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    'filename': image_path.name,\n",
    "                    'error': str(e),\n",
    "                    'batch': batch_count\n",
    "                }\n",
    "                processing_errors.append(error_info)\n",
    "                logger.error(f\"Error processing {image_path.name}: {e}\")\n",
    "                \n",
    "                # Continue with next image\n",
    "                processed_count += 1\n",
    "                progress_tracker.update_progress(progress, processed_count)\n",
    "        \n",
    "        # Memory cleanup after each batch\n",
    "        memory_optimizer.cleanup_variables([batch_results])\n",
    "        \n",
    "        # Check memory usage and adjust if needed\n",
    "        current_memory = memory_optimizer.monitor_memory_usage()\n",
    "        if current_memory.get('system_percent', 0) > 85:\n",
    "            print(f\"\\n‚ö†Ô∏è  High memory usage detected: {current_memory.get('system_percent', 0):.1f}%\")\n",
    "            suggestions = memory_optimizer.suggest_memory_optimizations(current_memory)\n",
    "            for suggestion in suggestions[:2]:\n",
    "                print(f\"   üí° {suggestion}\")\n",
    "    \n",
    "    progress_tracker.close_progress_bar(progress)\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Quality assessment completed!\")\n",
    "print(f\"   Processing time: {processing_time:.1f} seconds\")\n",
    "print(f\"   Valid images: {len(valid_images)}\")\n",
    "print(f\"   Invalid images: {len(invalid_images)}\")\n",
    "print(f\"   Processing errors: {len(processing_errors)}\")\n",
    "print(f\"   Success rate: {len(valid_images)/len(image_paths)*100:.1f}%\")\n",
    "\n",
    "# Report processing errors if any\n",
    "if processing_errors:\n",
    "    print(f\"\\n‚ö†Ô∏è  Processing errors encountered:\")\n",
    "    for error in processing_errors[:3]:  # Show first 3\n",
    "        print(f\"   - {error['filename']}: {error['error']}\")\n",
    "    if len(processing_errors) > 3:\n",
    "        print(f\"   ... and {len(processing_errors) - 3} more errors\")\n",
    "\n",
    "# Report invalid images if any\n",
    "if invalid_images:\n",
    "    print(f\"\\n‚ö†Ô∏è  Invalid images found:\")\n",
    "    for img_path in invalid_images[:3]:  # Show first 3\n",
    "        print(f\"   - {img_path.name}\")\n",
    "    if len(invalid_images) > 3:\n",
    "        print(f\"   ... and {len(invalid_images) - 3} more\")\n",
    "\n",
    "# Final memory cleanup\n",
    "memory_optimizer.cleanup_variables([batch_results])\n",
    "final_memory = memory_optimizer.monitor_memory_usage()\n",
    "print(f\"\\nüíæ Final memory usage: {final_memory.get('system_percent', 0):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHO5MyD0gVSg"
   },
   "source": [
    "## 4. Enhanced Data Statistics and Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mh-Udn8JgVSh"
   },
   "outputs": [],
   "source": [
    "# Enhanced data statistics with comprehensive quality analysis\n",
    "if not valid_images:\n",
    "    print(\"‚ùå No valid images found. Cannot proceed with analysis.\")\n",
    "    raise ValueError(\"No valid images available for processing\")\n",
    "\n",
    "print(\"üìä Generating comprehensive data statistics and quality analysis...\")\n",
    "\n",
    "# Basic dataset statistics\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   Total valid images: {len(valid_images)}\")\n",
    "print(f\"   Total size: {quality_stats['total_size_mb']:.1f} MB\")\n",
    "print(f\"   Average size per image: {quality_stats['total_size_mb']/len(valid_images):.2f} MB\")\n",
    "print(f\"   Processing time: {processing_time:.1f} seconds\")\n",
    "print(f\"   Images per second: {len(image_paths)/processing_time:.1f}\")\n",
    "\n",
    "# Resolution analysis with detailed statistics\n",
    "resolution_counts = Counter(quality_stats['resolutions'])\n",
    "print(f\"\\nüìê Resolution Distribution:\")\n",
    "for resolution, count in resolution_counts.most_common(10):\n",
    "    percentage = count / len(valid_images) * 100\n",
    "    print(f\"   {resolution}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "# Format and color mode distribution\n",
    "print(f\"\\nüñºÔ∏è  Format Distribution:\")\n",
    "for format_name, count in quality_stats['formats'].items():\n",
    "    percentage = count / len(valid_images) * 100\n",
    "    print(f\"   {format_name}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüé® Color Mode Distribution:\")\n",
    "for mode, count in quality_stats['color_modes'].items():\n",
    "    percentage = count / len(valid_images) * 100\n",
    "    print(f\"   {mode}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "# Enhanced quality metrics with statistical analysis\n",
    "quality_scores = quality_stats['quality_scores']\n",
    "brightness_scores = quality_stats['brightness_scores']\n",
    "contrast_scores = quality_stats['contrast_scores']\n",
    "sharpness_scores = quality_stats['sharpness_scores']\n",
    "aspect_ratios = quality_stats['aspect_ratios']\n",
    "\n",
    "print(f\"\\nüí° Enhanced Quality Metrics:\")\n",
    "print(f\"   Quality Score - Mean: {np.mean(quality_scores):.3f}, Std: {np.std(quality_scores):.3f}, Range: [{np.min(quality_scores):.3f}, {np.max(quality_scores):.3f}]\")\n",
    "print(f\"   Brightness - Mean: {np.mean(brightness_scores):.1f}, Std: {np.std(brightness_scores):.1f}, Range: [{np.min(brightness_scores):.1f}, {np.max(brightness_scores):.1f}]\")\n",
    "print(f\"   Contrast - Mean: {np.mean(contrast_scores):.3f}, Std: {np.std(contrast_scores):.3f}, Range: [{np.min(contrast_scores):.3f}, {np.max(contrast_scores):.3f}]\")\n",
    "print(f\"   Sharpness - Mean: {np.mean(sharpness_scores):.3f}, Std: {np.std(sharpness_scores):.3f}, Range: [{np.min(sharpness_scores):.3f}, {np.max(sharpness_scores):.3f}]\")\n",
    "print(f\"   Aspect Ratio - Mean: {np.mean(aspect_ratios):.3f}, Std: {np.std(aspect_ratios):.3f}, Range: [{np.min(aspect_ratios):.3f}, {np.max(aspect_ratios):.3f}]\")\n",
    "\n",
    "# Quality distribution analysis\n",
    "high_quality_count = sum(1 for score in quality_scores if score >= 0.8)\n",
    "medium_quality_count = sum(1 for score in quality_scores if 0.5 <= score < 0.8)\n",
    "low_quality_count = sum(1 for score in quality_scores if score < 0.5)\n",
    "\n",
    "print(f\"\\nüéØ Quality Distribution:\")\n",
    "print(f\"   High quality (‚â•0.8): {high_quality_count} images ({high_quality_count/len(valid_images)*100:.1f}%)\")\n",
    "print(f\"   Medium quality (0.5-0.8): {medium_quality_count} images ({medium_quality_count/len(valid_images)*100:.1f}%)\")\n",
    "print(f\"   Low quality (<0.5): {low_quality_count} images ({low_quality_count/len(valid_images)*100:.1f}%)\")\n",
    "\n",
    "# Enhanced quality assessment with specific recommendations\n",
    "print(f\"\\nüéØ Enhanced Quality Assessment:\")\n",
    "\n",
    "# Brightness analysis\n",
    "avg_brightness = np.mean(brightness_scores)\n",
    "brightness_std = np.std(brightness_scores)\n",
    "if avg_brightness < 50:\n",
    "    print(f\"   ‚ö†Ô∏è  Images appear dark (avg: {avg_brightness:.1f}) - may affect depth estimation quality\")\n",
    "    print(f\"      üí° Consider brightness adjustment or gamma correction\")\n",
    "elif avg_brightness > 200:\n",
    "    print(f\"   ‚ö†Ô∏è  Images appear bright (avg: {avg_brightness:.1f}) - may have overexposure issues\")\n",
    "    print(f\"      üí° Consider exposure adjustment or histogram equalization\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Brightness levels good for depth estimation (avg: {avg_brightness:.1f})\")\n",
    "\n",
    "if brightness_std > 50:\n",
    "    print(f\"   ‚ö†Ô∏è  High brightness variation (std: {brightness_std:.1f}) - inconsistent lighting conditions\")\n",
    "    print(f\"      üí° Consider normalization or adaptive processing\")\n",
    "\n",
    "# Contrast analysis\n",
    "avg_contrast = np.mean(contrast_scores)\n",
    "if avg_contrast < 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  Low contrast detected (avg: {avg_contrast:.3f}) - may reduce depth map quality\")\n",
    "    print(f\"      üí° Consider contrast enhancement or CLAHE\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Contrast levels adequate for depth estimation (avg: {avg_contrast:.3f})\")\n",
    "\n",
    "# Sharpness analysis\n",
    "avg_sharpness = np.mean(sharpness_scores)\n",
    "if avg_sharpness < 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  Low sharpness detected (avg: {avg_sharpness:.3f}) - images may be blurred\")\n",
    "    print(f\"      üí° Consider sharpening filters or deblurring\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Sharpness levels good (avg: {avg_sharpness:.3f})\")\n",
    "\n",
    "# Aspect ratio analysis\n",
    "aspect_std = np.std(aspect_ratios)\n",
    "if aspect_std > 0.5:\n",
    "    print(f\"   ‚ö†Ô∏è  High aspect ratio variation (std: {aspect_std:.3f}) - consider consistent cropping\")\n",
    "    print(f\"      üí° Standardize aspect ratios for better model performance\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Consistent aspect ratios across dataset (std: {aspect_std:.3f})\")\n",
    "\n",
    "# Overall dataset quality assessment\n",
    "avg_quality = np.mean(quality_scores)\n",
    "quality_threshold = data_validator.quality_threshold\n",
    "\n",
    "print(f\"\\nüèÜ Overall Dataset Assessment:\")\n",
    "if avg_quality >= quality_threshold:\n",
    "    print(f\"   ‚úÖ Dataset quality is good (avg: {avg_quality:.3f} ‚â• {quality_threshold})\")\n",
    "    print(f\"   üöÄ Ready for depth estimation processing\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Dataset quality below threshold (avg: {avg_quality:.3f} < {quality_threshold})\")\n",
    "    print(f\"   üîß Consider quality improvements before proceeding\")\n",
    "\n",
    "# Memory usage summary\n",
    "current_memory = memory_optimizer.monitor_memory_usage()\n",
    "print(f\"\\nüíæ Memory Usage Summary:\")\n",
    "print(f\"   System memory: {current_memory.get('system_percent', 0):.1f}% used\")\n",
    "print(f\"   Available memory: {current_memory.get('system_available_gb', 0):.1f}GB\")\n",
    "if current_memory.get('gpu_total_gb', 0) > 0:\n",
    "    print(f\"   GPU memory: {current_memory.get('gpu_percent', 0):.1f}% used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bugZ9ZEQgVSq"
   },
   "source": [
    "## 5. Enhanced Visual Quality Analysis with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Qde_BbggVSr"
   },
   "outputs": [],
   "source": [
    "# Enhanced visual quality analysis with robust error handling\n",
    "print(\"üìä Creating enhanced visual quality analysis...\")\n",
    "\n",
    "try:\n",
    "    # Set up the plotting environment\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Enhanced SwellSight Dataset Quality Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Quality Score Distribution\n",
    "    axes[0, 0].hist(quality_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(np.mean(quality_scores), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(quality_scores):.3f}')\n",
    "    axes[0, 0].axvline(quality_threshold, color='orange', linestyle='--', \n",
    "                      label=f'Threshold: {quality_threshold}')\n",
    "    axes[0, 0].set_title('Quality Score Distribution')\n",
    "    axes[0, 0].set_xlabel('Quality Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Brightness Distribution\n",
    "    axes[0, 1].hist(brightness_scores, bins=20, alpha=0.7, color='gold', edgecolor='black')\n",
    "    axes[0, 1].axvline(np.mean(brightness_scores), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(brightness_scores):.1f}')\n",
    "    axes[0, 1].set_title('Brightness Distribution')\n",
    "    axes[0, 1].set_xlabel('Brightness (0-255)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Contrast Distribution\n",
    "    axes[0, 2].hist(contrast_scores, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 2].axvline(np.mean(contrast_scores), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(contrast_scores):.3f}')\n",
    "    axes[0, 2].set_title('Contrast Distribution')\n",
    "    axes[0, 2].set_xlabel('Contrast (0-1)')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sharpness Distribution\n",
    "    axes[1, 0].hist(sharpness_scores, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1, 0].axvline(np.mean(sharpness_scores), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(sharpness_scores):.3f}')\n",
    "    axes[1, 0].set_title('Sharpness Distribution')\n",
    "    axes[1, 0].set_xlabel('Sharpness')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Resolution Distribution (Top 10)\n",
    "    top_resolutions = resolution_counts.most_common(10)\n",
    "    if top_resolutions:\n",
    "        res_names = [res[0] for res in top_resolutions]\n",
    "        res_counts = [res[1] for res in top_resolutions]\n",
    "        \n",
    "        axes[1, 1].bar(range(len(res_names)), res_counts, alpha=0.7, color='mediumpurple')\n",
    "        axes[1, 1].set_title('Top 10 Resolutions')\n",
    "        axes[1, 1].set_xlabel('Resolution')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "        axes[1, 1].set_xticks(range(len(res_names)))\n",
    "        axes[1, 1].set_xticklabels(res_names, rotation=45, ha='right')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Quality vs Brightness Scatter Plot\n",
    "    scatter = axes[1, 2].scatter(brightness_scores, quality_scores, alpha=0.6, \n",
    "                                c=contrast_scores, cmap='viridis', s=20)\n",
    "    axes[1, 2].set_title('Quality vs Brightness (colored by Contrast)')\n",
    "    axes[1, 2].set_xlabel('Brightness')\n",
    "    axes[1, 2].set_ylabel('Quality Score')\n",
    "    axes[1, 2].axhline(quality_threshold, color='red', linestyle='--', alpha=0.7, \n",
    "                      label=f'Quality Threshold: {quality_threshold}')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar for scatter plot\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 2])\n",
    "    cbar.set_label('Contrast')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot with error handling\n",
    "    try:\n",
    "        plot_path = OUTPUT_PATH / 'quality_analysis.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Quality analysis plot saved: {plot_path}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save plot: {e}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating visual analysis: {e}\")\n",
    "    print(f\"‚ö†Ô∏è  Could not create visual analysis: {e}\")\n",
    "    print(\"Continuing with text-based analysis...\")\n",
    "\n",
    "# Create summary statistics table\n",
    "try:\n",
    "    summary_stats = {\n",
    "        'Metric': ['Quality Score', 'Brightness', 'Contrast', 'Sharpness', 'Aspect Ratio'],\n",
    "        'Mean': [np.mean(quality_scores), np.mean(brightness_scores), np.mean(contrast_scores), \n",
    "                np.mean(sharpness_scores), np.mean(aspect_ratios)],\n",
    "        'Std': [np.std(quality_scores), np.std(brightness_scores), np.std(contrast_scores), \n",
    "               np.std(sharpness_scores), np.std(aspect_ratios)],\n",
    "        'Min': [np.min(quality_scores), np.min(brightness_scores), np.min(contrast_scores), \n",
    "               np.min(sharpness_scores), np.min(aspect_ratios)],\n",
    "        'Max': [np.max(quality_scores), np.max(brightness_scores), np.max(contrast_scores), \n",
    "               np.max(sharpness_scores), np.max(aspect_ratios)]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df = summary_df.round(3)\n",
    "    \n",
    "    print(\"\\nüìã Quality Metrics Summary Table:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary table\n",
    "    try:\n",
    "        summary_path = OUTPUT_PATH / 'quality_summary.csv'\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"\\n‚úÖ Quality summary saved: {summary_path}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save summary table: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating summary statistics: {e}\")\n",
    "    print(f\"‚ö†Ô∏è  Could not create summary statistics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Standardized Data Format and Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare standardized data format for pipeline integration\n",
    "print(\"üì¶ Preparing standardized data format for pipeline integration...\")\n",
    "\n",
    "try:\n",
    "    # Create standardized data structure\n",
    "    processed_data = {\n",
    "        'valid_image_paths': [str(path) for path in valid_images],\n",
    "        'invalid_image_paths': [str(path) for path in invalid_images],\n",
    "        'image_metadata': image_metadata,\n",
    "        'quality_statistics': {\n",
    "            'total_images_processed': len(image_paths),\n",
    "            'valid_images_count': len(valid_images),\n",
    "            'invalid_images_count': len(invalid_images),\n",
    "            'success_rate': len(valid_images) / len(image_paths),\n",
    "            'average_quality_score': float(np.mean(quality_scores)) if quality_scores else 0.0,\n",
    "            'quality_score_std': float(np.std(quality_scores)) if quality_scores else 0.0,\n",
    "            'average_brightness': float(np.mean(brightness_scores)) if brightness_scores else 0.0,\n",
    "            'average_contrast': float(np.mean(contrast_scores)) if contrast_scores else 0.0,\n",
    "            'average_sharpness': float(np.mean(sharpness_scores)) if sharpness_scores else 0.0,\n",
    "            'total_size_mb': quality_stats['total_size_mb'],\n",
    "            'resolution_distribution': dict(resolution_counts.most_common(10)),\n",
    "            'format_distribution': quality_stats['formats'],\n",
    "            'color_mode_distribution': quality_stats['color_modes']\n",
    "        },\n",
    "        'processing_info': {\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'images_per_second': len(image_paths) / processing_time,\n",
    "            'batch_size_used': optimal_batch_size,\n",
    "            'memory_usage': memory_optimizer.monitor_memory_usage(),\n",
    "            'quality_threshold': data_validator.quality_threshold,\n",
    "            'processing_errors': processing_errors\n",
    "        },\n",
    "        'recommendations': {\n",
    "            'dataset_ready_for_next_stage': avg_quality >= quality_threshold and len(valid_images) > 0,\n",
    "            'suggested_improvements': [],\n",
    "            'memory_optimizations': memory_optimizer.suggest_memory_optimizations()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add specific recommendations based on analysis\n",
    "    recommendations = processed_data['recommendations']['suggested_improvements']\n",
    "    \n",
    "    if avg_brightness < 50:\n",
    "        recommendations.append(\"Consider brightness adjustment - images appear dark\")\n",
    "    elif avg_brightness > 200:\n",
    "        recommendations.append(\"Consider exposure adjustment - images appear overexposed\")\n",
    "    \n",
    "    if avg_contrast < 0.1:\n",
    "        recommendations.append(\"Consider contrast enhancement - low contrast detected\")\n",
    "    \n",
    "    if avg_sharpness < 0.1:\n",
    "        recommendations.append(\"Consider sharpening filters - low sharpness detected\")\n",
    "    \n",
    "    if aspect_std > 0.5:\n",
    "        recommendations.append(\"Consider standardizing aspect ratios for consistency\")\n",
    "    \n",
    "    if len(invalid_images) > len(valid_images) * 0.1:  # More than 10% invalid\n",
    "        recommendations.append(\"High number of invalid images - review input data quality\")\n",
    "    \n",
    "    # Create metadata for data flow manager\n",
    "    stage_metadata = {\n",
    "        'processing_time_seconds': processing_time,\n",
    "        'input_count': len(image_paths),\n",
    "        'output_count': len(valid_images),\n",
    "        'success_rate': len(valid_images) / len(image_paths),\n",
    "        'quality_metrics': {\n",
    "            'mean_quality_score': float(np.mean(quality_scores)) if quality_scores else 0.0,\n",
    "            'min_quality_score': float(np.min(quality_scores)) if quality_scores else 0.0,\n",
    "            'max_quality_score': float(np.max(quality_scores)) if quality_scores else 0.0\n",
    "        },\n",
    "        'errors': [\n",
    "            {\n",
    "                'type': 'processing_error',\n",
    "                'count': len(processing_errors),\n",
    "                'examples': [error['filename'] for error in processing_errors[:3]]\n",
    "            },\n",
    "            {\n",
    "                'type': 'invalid_image',\n",
    "                'count': len(invalid_images),\n",
    "                'examples': [path.name for path in invalid_images[:3]]\n",
    "            }\n",
    "        ] if (processing_errors or invalid_images) else [],\n",
    "        'outputs': {\n",
    "            'processed_images': 'data_preprocessing_results.json',\n",
    "            'quality_report': 'quality_summary.csv',\n",
    "            'quality_plot': 'quality_analysis.png'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results using data flow manager\n",
    "    success = data_flow_manager.save_stage_results(\n",
    "        data=processed_data,\n",
    "        stage_name='data_preprocessing',\n",
    "        metadata=stage_metadata\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"‚úÖ Data preprocessing results saved successfully\")\n",
    "        print(f\"   Stage: data_preprocessing\")\n",
    "        print(f\"   Valid images: {len(valid_images)}\")\n",
    "        print(f\"   Quality score: {avg_quality:.3f}\")\n",
    "        print(f\"   Ready for next stage: {processed_data['recommendations']['dataset_ready_for_next_stage']}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Warning: Could not save results to data flow manager\")\n",
    "        print(\"Results are still available in memory for this session\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(f\"\\nüéØ Final Processing Summary:\")\n",
    "    print(f\"   Total images processed: {len(image_paths)}\")\n",
    "    print(f\"   Valid images: {len(valid_images)} ({len(valid_images)/len(image_paths)*100:.1f}%)\")\n",
    "    print(f\"   Average quality score: {avg_quality:.3f}\")\n",
    "    print(f\"   Processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"   Memory usage: {final_memory.get('system_percent', 0):.1f}%\")\n",
    "    \n",
    "    if processed_data['recommendations']['dataset_ready_for_next_stage']:\n",
    "        print(f\"\\nüöÄ Dataset is ready for depth extraction stage!\")\n",
    "        print(f\"   You can now proceed to notebook 03: Depth-Anything-V2 Extraction\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Dataset may need improvements before proceeding:\")\n",
    "        for rec in recommendations[:3]:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error preparing standardized data format: {e}\")\n",
    "    print(f\"‚ùå Error preparing data for pipeline: {e}\")\n",
    "    print(\"Results are available in memory but may not be properly formatted for next stage\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced data preprocessing completed successfully!\")\n",
    "print(f\"üìä All results saved and ready for pipeline integration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}