{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "augmentation_header"
   },
   "source": [
    "# SwellSight Pipeline - Enhanced Data Augmentation System\n",
    "\n",
    "This enhanced notebook implements FLUX.1-dev optimized augmentation parameter generation with improved validation, memory optimization, and progress tracking.\n",
    "\n",
    "## Key Improvements\n",
    "- **FLUX.1-dev Optimization**: Parameter ranges and distributions optimized for FLUX.1-dev model\n",
    "- **Enhanced Validation**: Comprehensive parameter validation with quality checks\n",
    "- **Memory Optimization**: Dynamic memory monitoring and efficient parameter generation\n",
    "- **Progress Tracking**: Real-time progress with memory usage display\n",
    "- **Configuration Snapshots**: Reproducibility through configuration preservation\n",
    "- **Parameter Diversity Analysis**: Advanced diversity metrics and reporting\n",
    "\n",
    "## Pipeline Integration\n",
    "This notebook implements Step 4 of the enhanced pipeline:\n",
    "1. **Load Configuration**: Enhanced config loading with hardware adaptation\n",
    "2. **Parameter System**: FLUX-optimized parameter generator with validation\n",
    "3. **Batch Generation**: Memory-efficient parameter set creation\n",
    "4. **Quality Control**: Advanced parameter validation and quality checks\n",
    "5. **Diversity Analysis**: Comprehensive parameter space coverage analysis\n",
    "6. **Export & Snapshot**: Results export with configuration preservation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import enhanced utilities\n",
    "sys.path.append('.')\n",
    "from utils.config_manager import ConfigManager, load_config, validate_config\n",
    "from utils.data_flow_manager import DataFlowManager\n",
    "from utils.memory_optimizer import MemoryOptimizer, monitor_memory\n",
    "from utils.progress_tracker import ProgressTracker, create_progress_bar\n",
    "from utils.error_handler import ErrorHandler, retry_with_backoff\n",
    "from utils.data_validator import DataValidator\n",
    "\n",
    "# Configure enhanced logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üîÑ Loading enhanced configuration and utilities...\")\n",
    "\n",
    "# Initialize enhanced components\n",
    "config_manager = ConfigManager()\n",
    "data_flow_manager = DataFlowManager()\n",
    "memory_optimizer = MemoryOptimizer()\n",
    "progress_tracker = ProgressTracker()\n",
    "error_handler = ErrorHandler()\n",
    "data_validator = DataValidator()\n",
    "\n",
    "print(\"‚úÖ Enhanced utilities initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enhanced configuration with hardware adaptation\n",
    "print(\"üîß Loading enhanced configuration...\")\n",
    "\n",
    "try:\n",
    "    # Load configuration with hardware detection and adaptation\n",
    "    PIPELINE_CONFIG = config_manager.load_config()\n",
    "    \n",
    "    # Validate configuration\n",
    "    validation_result = validate_config(PIPELINE_CONFIG)\n",
    "    if not validation_result['valid']:\n",
    "        logger.warning(\"Configuration validation issues found:\")\n",
    "        for error in validation_result['errors']:\n",
    "            logger.warning(f\"  - {error}\")\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        logger.info(\"Configuration warnings:\")\n",
    "        for warning in validation_result['warnings']:\n",
    "            logger.info(f\"  - {warning}\")\n",
    "    \n",
    "    # Load previous stage results\n",
    "    depth_results = data_flow_manager.load_previous_results(\n",
    "        stage_name=\"depth_extraction\",\n",
    "        required_files=[\"depth_maps.json\", \"depth_quality.json\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Configuration and previous results loaded successfully\")\n",
    "    \n",
    "    # Display configuration summary\n",
    "    print(f\"\\nüìã Enhanced Pipeline Configuration:\")\n",
    "    print(f\"   Pipeline: {PIPELINE_CONFIG['pipeline']['name']} v{PIPELINE_CONFIG['pipeline']['version']}\")\n",
    "    print(f\"   Batch size: {PIPELINE_CONFIG['processing']['batch_size']}\")\n",
    "    print(f\"   Quality threshold: {PIPELINE_CONFIG['processing']['quality_threshold']}\")\n",
    "    print(f\"   Memory limit: {PIPELINE_CONFIG['processing']['memory_limit_gb']:.1f}GB\")\n",
    "    \n",
    "    # Display hardware info if available\n",
    "    if 'detected_hardware' in PIPELINE_CONFIG:\n",
    "        hw_info = PIPELINE_CONFIG['detected_hardware']\n",
    "        print(f\"\\nüñ•Ô∏è  Detected Hardware:\")\n",
    "        print(f\"   GPU Available: {hw_info['gpu_available']}\")\n",
    "        if hw_info['gpu_available']:\n",
    "            print(f\"   GPU: {hw_info['gpu_name']}\")\n",
    "            print(f\"   GPU Memory: {hw_info['gpu_memory_gb']:.1f}GB\")\n",
    "            print(f\"   FLUX Compatibility: {hw_info['flux_compatibility']}\")\n",
    "        print(f\"   CPU Cores: {hw_info['cpu_count']}\")\n",
    "        print(f\"   System Memory: {hw_info['memory_gb']:.1f}GB\")\n",
    "    \n",
    "    # Display depth extraction results\n",
    "    if depth_results:\n",
    "        print(f\"\\nüìä Depth Extraction Results:\")\n",
    "        print(f\"   Images processed: {len(depth_results.get('images', []))}\")\n",
    "        print(f\"   Average quality: {depth_results.get('average_quality', 0):.3f}\")\n",
    "        print(f\"   Ready for augmentation: {len(depth_results.get('images', []))} images\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading configuration: {e}\")\n",
    "    print(\"‚ùå Failed to load configuration. Please check previous pipeline stages.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FLUX-Optimized Parameter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLUXOptimizedParameterSystem:\n",
    "    \"\"\"\n",
    "    Enhanced parameter system optimized for FLUX.1-dev model\n",
    "    with comprehensive validation and quality checks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.flux_config = config['models']['base_model']\n",
    "        self.controlnet_config = config['models']['controlnet_model']\n",
    "        \n",
    "        # FLUX.1-dev optimized parameter ranges\n",
    "        self.parameter_ranges = {\n",
    "            'guidance_scale': {\n",
    "                'min': 1.0,\n",
    "                'max': 5.0,  # FLUX works better with lower guidance\n",
    "                'default': 3.5,\n",
    "                'distribution': 'normal',\n",
    "                'std': 0.8\n",
    "            },\n",
    "            'num_inference_steps': {\n",
    "                'values': [20, 25, 28, 30, 35],  # FLUX optimal step counts\n",
    "                'default': 28,\n",
    "                'weights': [0.1, 0.2, 0.4, 0.2, 0.1]  # Prefer 28 steps\n",
    "            },\n",
    "            'controlnet_conditioning_scale': {\n",
    "                'min': 0.4,\n",
    "                'max': 0.8,  # FLUX ControlNet works better with moderate conditioning\n",
    "                'default': 0.6,\n",
    "                'distribution': 'uniform'\n",
    "            },\n",
    "            'control_guidance_start': {\n",
    "                'min': 0.0,\n",
    "                'max': 0.2,\n",
    "                'default': 0.0,\n",
    "                'distribution': 'uniform'\n",
    "            },\n",
    "            'control_guidance_end': {\n",
    "                'min': 0.8,\n",
    "                'max': 1.0,\n",
    "                'default': 1.0,\n",
    "                'distribution': 'uniform'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Enhanced prompt variations for wave/ocean scenes\n",
    "        self.prompt_variations = [\n",
    "            \"ocean waves crashing on beach, natural lighting, photorealistic\",\n",
    "            \"powerful sea waves, dramatic coastline, high detail\",\n",
    "            \"coastal waves breaking, golden hour lighting, cinematic\",\n",
    "            \"surf waves rolling to shore, clear blue water, realistic\",\n",
    "            \"ocean swells approaching beach, natural colors, detailed\",\n",
    "            \"breaking waves with white foam, sunny day, sharp focus\",\n",
    "            \"sea waves in motion, dynamic water, professional photography\",\n",
    "            \"beach waves at sunset, warm lighting, high resolution\",\n",
    "            \"turbulent ocean waves, stormy atmosphere, dramatic\",\n",
    "            \"gentle waves lapping shore, peaceful scene, soft lighting\"\n",
    "        ]\n",
    "        \n",
    "        # Parameter validation rules\n",
    "        self.validation_rules = {\n",
    "            'guidance_scale': {\n",
    "                'min_value': 0.5,\n",
    "                'max_value': 10.0,\n",
    "                'recommended_min': 1.0,\n",
    "                'recommended_max': 5.0\n",
    "            },\n",
    "            'num_inference_steps': {\n",
    "                'min_value': 10,\n",
    "                'max_value': 50,\n",
    "                'recommended_min': 20,\n",
    "                'recommended_max': 35\n",
    "            },\n",
    "            'controlnet_conditioning_scale': {\n",
    "                'min_value': 0.1,\n",
    "                'max_value': 2.0,\n",
    "                'recommended_min': 0.4,\n",
    "                'recommended_max': 0.8\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_random_parameters(self) -> dict:\n",
    "        \"\"\"Generate FLUX-optimized random parameters\"\"\"\n",
    "        try:\n",
    "            parameters = {}\n",
    "            \n",
    "            # Guidance scale (normal distribution around optimal value)\n",
    "            guidance_config = self.parameter_ranges['guidance_scale']\n",
    "            if guidance_config['distribution'] == 'normal':\n",
    "                guidance = np.random.normal(guidance_config['default'], guidance_config['std'])\n",
    "                guidance = np.clip(guidance, guidance_config['min'], guidance_config['max'])\n",
    "            else:\n",
    "                guidance = np.random.uniform(guidance_config['min'], guidance_config['max'])\n",
    "            parameters['guidance_scale'] = float(guidance)\n",
    "            \n",
    "            # Inference steps (weighted choice)\n",
    "            steps_config = self.parameter_ranges['num_inference_steps']\n",
    "            steps = np.random.choice(steps_config['values'], p=steps_config['weights'])\n",
    "            parameters['num_inference_steps'] = int(steps)\n",
    "            \n",
    "            # ControlNet conditioning scale\n",
    "            conditioning_config = self.parameter_ranges['controlnet_conditioning_scale']\n",
    "            conditioning = np.random.uniform(conditioning_config['min'], conditioning_config['max'])\n",
    "            parameters['controlnet_conditioning_scale'] = float(conditioning)\n",
    "            \n",
    "            # Control guidance timing\n",
    "            start_config = self.parameter_ranges['control_guidance_start']\n",
    "            end_config = self.parameter_ranges['control_guidance_end']\n",
    "            parameters['control_guidance_start'] = float(np.random.uniform(start_config['min'], start_config['max']))\n",
    "            parameters['control_guidance_end'] = float(np.random.uniform(end_config['min'], end_config['max']))\n",
    "            \n",
    "            # Random seed for reproducibility\n",
    "            parameters['seed'] = int(np.random.randint(0, 2**32 - 1))\n",
    "            \n",
    "            # Prompt variation\n",
    "            parameters['prompt_variation'] = np.random.choice(self.prompt_variations)\n",
    "            \n",
    "            return parameters\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating parameters: {e}\")\n",
    "            return self._get_default_parameters()\n",
    "    \n",
    "    def validate_parameters(self, parameters: dict) -> dict:\n",
    "        \"\"\"Validate parameter values and provide quality assessment\"\"\"\n",
    "        validation_result = {\n",
    "            'valid': True,\n",
    "            'issues': [],\n",
    "            'warnings': [],\n",
    "            'quality_score': 1.0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            quality_scores = []\n",
    "            \n",
    "            for param_name, value in parameters.items():\n",
    "                if param_name in self.validation_rules:\n",
    "                    rules = self.validation_rules[param_name]\n",
    "                    \n",
    "                    # Check absolute bounds\n",
    "                    if value < rules['min_value'] or value > rules['max_value']:\n",
    "                        validation_result['valid'] = False\n",
    "                        validation_result['issues'].append(\n",
    "                            f\"{param_name} value {value} outside valid range [{rules['min_value']}, {rules['max_value']}]\"\n",
    "                        )\n",
    "                        quality_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # Check recommended bounds\n",
    "                    if value < rules['recommended_min'] or value > rules['recommended_max']:\n",
    "                        validation_result['warnings'].append(\n",
    "                            f\"{param_name} value {value} outside recommended range [{rules['recommended_min']}, {rules['recommended_max']}]\"\n",
    "                        )\n",
    "                        quality_scores.append(0.7)  # Reduced quality but still valid\n",
    "                    else:\n",
    "                        quality_scores.append(1.0)  # Optimal quality\n",
    "            \n",
    "            # Calculate overall quality score\n",
    "            if quality_scores:\n",
    "                validation_result['quality_score'] = np.mean(quality_scores)\n",
    "            \n",
    "            return validation_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error validating parameters: {e}\")\n",
    "            return {\n",
    "                'valid': False,\n",
    "                'issues': [f\"Validation error: {str(e)}\"],\n",
    "                'warnings': [],\n",
    "                'quality_score': 0.0\n",
    "            }\n",
    "    \n",
    "    def _get_default_parameters(self) -> dict:\n",
    "        \"\"\"Get default parameters as fallback\"\"\"\n",
    "        return {\n",
    "            'guidance_scale': self.parameter_ranges['guidance_scale']['default'],\n",
    "            'num_inference_steps': self.parameter_ranges['num_inference_steps']['default'],\n",
    "            'controlnet_conditioning_scale': self.parameter_ranges['controlnet_conditioning_scale']['default'],\n",
    "            'control_guidance_start': self.parameter_ranges['control_guidance_start']['default'],\n",
    "            'control_guidance_end': self.parameter_ranges['control_guidance_end']['default'],\n",
    "            'seed': 42,\n",
    "            'prompt_variation': self.prompt_variations[0]\n",
    "        }\n",
    "\n",
    "# Initialize FLUX-optimized parameter system\n",
    "print(\"üéõÔ∏è  Initializing FLUX-optimized parameter system...\")\n",
    "\n",
    "try:\n",
    "    param_system = FLUXOptimizedParameterSystem(PIPELINE_CONFIG)\n",
    "    \n",
    "    # Test parameter generation\n",
    "    test_params = param_system.generate_random_parameters()\n",
    "    validation_result = param_system.validate_parameters(test_params)\n",
    "    \n",
    "    print(\"‚úÖ FLUX-optimized parameter system initialized successfully!\")\n",
    "    print(f\"   Parameter validation: {'‚úÖ Valid' if validation_result['valid'] else '‚ùå Invalid'}\")\n",
    "    print(f\"   Quality score: {validation_result['quality_score']:.3f}\")\n",
    "    print(f\"   Sample parameters: {test_params}\")\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        print(f\"   Warnings: {len(validation_result['warnings'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize parameter system: {e}\")\n",
    "    print(\"‚ùå Parameter system initialization failed\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory-Optimized Parameter Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-optimized batch parameter generation\n",
    "print(\"üé≤ Starting memory-optimized parameter generation...\")\n",
    "\n",
    "# Get images ready for augmentation\n",
    "ready_images = depth_results.get('images', [])\n",
    "if not ready_images:\n",
    "    print(\"‚ùå No images ready for augmentation. Please check previous pipeline stages.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get synthetic per real from config\n",
    "synthetic_per_real = PIPELINE_CONFIG['processing'].get('synthetic_per_real', 3)\n",
    "total_parameter_sets = len(ready_images) * synthetic_per_real\n",
    "\n",
    "print(f\"\\nüìä Generation Configuration:\")\n",
    "print(f\"   Images to process: {len(ready_images)}\")\n",
    "print(f\"   Synthetic variations per image: {synthetic_per_real}\")\n",
    "print(f\"   Total parameter sets to generate: {total_parameter_sets}\")\n",
    "\n",
    "# Monitor memory usage\n",
    "initial_memory = monitor_memory()\n",
    "print(f\"\\nüíæ Initial Memory Usage:\")\n",
    "print(f\"   System: {initial_memory.get('system_percent', 0):.1f}%\")\n",
    "if 'gpu_percent' in initial_memory:\n",
    "    print(f\"   GPU: {initial_memory.get('gpu_percent', 0):.1f}%\")\n",
    "\n",
    "# Calculate optimal batch size for parameter generation\n",
    "param_memory_per_item = 1024  # Estimated bytes per parameter set\n",
    "optimal_batch_size = memory_optimizer.get_optimal_batch_size(\n",
    "    item_size=param_memory_per_item,\n",
    "    max_batch_size=min(len(ready_images), 100)\n",
    ")\n",
    "\n",
    "print(f\"\\nüîß Memory Optimization:\")\n",
    "print(f\"   Optimal batch size: {optimal_batch_size}\")\n",
    "print(f\"   Estimated memory per parameter set: {param_memory_per_item} bytes\")\n",
    "\n",
    "# Initialize tracking variables\n",
    "all_augmentation_params = []\n",
    "parameter_statistics = defaultdict(list)\n",
    "generation_metadata = []\n",
    "validation_summary = {\n",
    "    'total_generated': 0,\n",
    "    'valid_parameters': 0,\n",
    "    'invalid_parameters': 0,\n",
    "    'quality_warnings': 0,\n",
    "    'average_quality_score': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nüöÄ Starting parameter generation with memory monitoring...\")\n",
    "\n",
    "# Use memory monitoring context\n",
    "with memory_optimizer.memory_monitor(log_usage=True) as monitor:\n",
    "    # Process images in batches for memory efficiency\n",
    "    for batch_start in tqdm(range(0, len(ready_images), optimal_batch_size), \n",
    "                           desc=\"Processing batches\", unit=\"batch\"):\n",
    "        \n",
    "        batch_end = min(batch_start + optimal_batch_size, len(ready_images))\n",
    "        batch_images = ready_images[batch_start:batch_end]\n",
    "        \n",
    "        # Generate parameters for current batch\n",
    "        batch_params = []\n",
    "        \n",
    "        for i, image_path in enumerate(batch_images):\n",
    "            image_path_obj = Path(image_path)\n",
    "            global_index = batch_start + i\n",
    "            \n",
    "            # Generate multiple parameter sets for this image\n",
    "            for j in range(synthetic_per_real):\n",
    "                try:\n",
    "                    # Generate parameters\n",
    "                    augmentation_params = param_system.generate_random_parameters()\n",
    "                    \n",
    "                    # Validate parameters\n",
    "                    validation_result = param_system.validate_parameters(augmentation_params)\n",
    "                    \n",
    "                    # Create parameter metadata\n",
    "                    param_metadata = {\n",
    "                        'image_path': image_path,\n",
    "                        'image_name': image_path_obj.name,\n",
    "                        'variation_index': j + 1,\n",
    "                        'total_variations': synthetic_per_real,\n",
    "                        'global_index': validation_summary['total_generated'],\n",
    "                        'parameters': augmentation_params,\n",
    "                        'validation': validation_result,\n",
    "                        'generated_timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    batch_params.append(param_metadata)\n",
    "                    \n",
    "                    # Update validation summary\n",
    "                    validation_summary['total_generated'] += 1\n",
    "                    if validation_result['valid']:\n",
    "                        validation_summary['valid_parameters'] += 1\n",
    "                    else:\n",
    "                        validation_summary['invalid_parameters'] += 1\n",
    "                    \n",
    "                    validation_summary['quality_warnings'] += len(validation_result['warnings'])\n",
    "                    \n",
    "                    # Collect statistics for numeric parameters\n",
    "                    for key, value in augmentation_params.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            parameter_statistics[key].append(value)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to generate parameters for {image_path_obj.name} variation {j+1}: {e}\")\n",
    "                    validation_summary['invalid_parameters'] += 1\n",
    "                    continue\n",
    "        \n",
    "        # Add batch to main collection\n",
    "        all_augmentation_params.extend(batch_params)\n",
    "        \n",
    "        # Monitor memory usage during processing\n",
    "        current_memory = monitor_memory()\n",
    "        if current_memory.get('system_percent', 0) > 85:\n",
    "            logger.warning(f\"High memory usage detected: {current_memory.get('system_percent', 0):.1f}%\")\n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "        \n",
    "        # Clear batch variables to free memory\n",
    "        del batch_params\n",
    "        gc.collect()\n",
    "\n",
    "# Calculate final statistics\n",
    "if validation_summary['total_generated'] > 0:\n",
    "    # Calculate average quality score\n",
    "    quality_scores = [param['validation']['quality_score'] for param in all_augmentation_params \n",
    "                     if 'validation' in param]\n",
    "    if quality_scores:\n",
    "        validation_summary['average_quality_score'] = np.mean(quality_scores)\n",
    "\n",
    "print(f\"\\n‚úÖ Parameter generation completed!\")\n",
    "print(f\"\\nüìä Generation Results:\")\n",
    "print(f\"   Total parameter sets generated: {validation_summary['total_generated']}\")\n",
    "print(f\"   Valid parameter sets: {validation_summary['valid_parameters']}\")\n",
    "print(f\"   Invalid parameter sets: {validation_summary['invalid_parameters']}\")\n",
    "print(f\"   Quality warnings: {validation_summary['quality_warnings']}\")\n",
    "print(f\"   Average quality score: {validation_summary['average_quality_score']:.3f}\")\n",
    "print(f\"   Success rate: {validation_summary['valid_parameters']/validation_summary['total_generated']*100:.1f}%\")\n",
    "\n",
    "# Final memory check\n",
    "final_memory = monitor_memory()\n",
    "print(f\"\\nüíæ Final Memory Usage:\")\n",
    "print(f\"   System: {final_memory.get('system_percent', 0):.1f}%\")\n",
    "if 'gpu_percent' in final_memory:\n",
    "    print(f\"   GPU: {final_memory.get('gpu_percent', 0):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Parameter Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced parameter diversity analysis with enhanced metrics\n",
    "print(\"üìä Performing advanced parameter diversity analysis...\")\n",
    "\n",
    "if not parameter_statistics:\n",
    "    print(\"‚ùå No parameter statistics available for analysis.\")\n",
    "else:\n",
    "    print(f\"\\nüìà Enhanced Parameter Statistics:\")\n",
    "    \n",
    "    diversity_metrics = {}\n",
    "    \n",
    "    # Analyze each parameter type with enhanced metrics\n",
    "    for param_name, values in parameter_statistics.items():\n",
    "        if values:\n",
    "            values_array = np.array(values)\n",
    "            \n",
    "            # Basic statistics\n",
    "            mean_val = np.mean(values_array)\n",
    "            std_val = np.std(values_array)\n",
    "            min_val = np.min(values_array)\n",
    "            max_val = np.max(values_array)\n",
    "            median_val = np.median(values_array)\n",
    "            \n",
    "            # Advanced diversity metrics\n",
    "            unique_values = len(np.unique(values_array))\n",
    "            total_values = len(values_array)\n",
    "            uniqueness_ratio = unique_values / total_values\n",
    "            \n",
    "            # Coefficient of variation (normalized standard deviation)\n",
    "            cv = std_val / mean_val if mean_val != 0 else 0\n",
    "            \n",
    "            # Range utilization (how much of the possible range is used)\n",
    "            param_range = max_val - min_val\n",
    "            if param_name in param_system.parameter_ranges:\n",
    "                config_range = param_system.parameter_ranges[param_name]\n",
    "                if 'min' in config_range and 'max' in config_range:\n",
    "                    theoretical_range = config_range['max'] - config_range['min']\n",
    "                    range_utilization = param_range / theoretical_range if theoretical_range > 0 else 0\n",
    "                else:\n",
    "                    range_utilization = 1.0  # For discrete parameters\n",
    "            else:\n",
    "                range_utilization = 1.0\n",
    "            \n",
    "            # Store metrics\n",
    "            diversity_metrics[param_name] = {\n",
    "                'mean': mean_val,\n",
    "                'std': std_val,\n",
    "                'min': min_val,\n",
    "                'max': max_val,\n",
    "                'median': median_val,\n",
    "                'range': param_range,\n",
    "                'unique_values': unique_values,\n",
    "                'total_values': total_values,\n",
    "                'uniqueness_ratio': uniqueness_ratio,\n",
    "                'coefficient_of_variation': cv,\n",
    "                'range_utilization': range_utilization\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n   {param_name}:\")\n",
    "            print(f\"     Mean: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
    "            print(f\"     Range: [{min_val:.3f}, {max_val:.3f}] (Median: {median_val:.3f})\")\n",
    "            print(f\"     Unique values: {unique_values}/{total_values} ({uniqueness_ratio*100:.1f}%)\")\n",
    "            print(f\"     Coefficient of variation: {cv:.3f}\")\n",
    "            print(f\"     Range utilization: {range_utilization*100:.1f}%\")\n",
    "            \n",
    "            # Diversity assessment with enhanced criteria\n",
    "            diversity_score = (uniqueness_ratio + range_utilization + min(cv, 1.0)) / 3.0\n",
    "            \n",
    "            if diversity_score > 0.8:\n",
    "                print(f\"     Diversity: ‚úÖ Excellent ({diversity_score:.3f})\")\n",
    "            elif diversity_score > 0.6:\n",
    "                print(f\"     Diversity: ‚úÖ Good ({diversity_score:.3f})\")\n",
    "            elif diversity_score > 0.4:\n",
    "                print(f\"     Diversity: ‚ö†Ô∏è  Moderate ({diversity_score:.3f})\")\n",
    "            else:\n",
    "                print(f\"     Diversity: ‚ùå Poor ({diversity_score:.3f})\")\n",
    "\n",
    "# Enhanced parameter combination analysis\n",
    "print(f\"\\nüéØ Enhanced Parameter Combination Analysis:\")\n",
    "if all_augmentation_params:\n",
    "    # Create parameter combination signatures\n",
    "    param_signatures = set()\n",
    "    quality_distribution = defaultdict(int)\n",
    "    \n",
    "    for param_set in all_augmentation_params:\n",
    "        params = param_set['parameters']\n",
    "        validation = param_set.get('validation', {})\n",
    "        \n",
    "        if isinstance(params, dict):\n",
    "            # Create signature from numeric parameters (rounded for reasonable uniqueness)\n",
    "            numeric_params = {}\n",
    "            for k, v in params.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    if isinstance(v, float):\n",
    "                        numeric_params[k] = round(v, 3)  # Round floats to 3 decimal places\n",
    "                    else:\n",
    "                        numeric_params[k] = v\n",
    "            \n",
    "            param_signature = tuple(sorted(numeric_params.items()))\n",
    "            param_signatures.add(param_signature)\n",
    "            \n",
    "            # Track quality distribution\n",
    "            quality_score = validation.get('quality_score', 0.0)\n",
    "            quality_bin = f\"{quality_score:.1f}\"\n",
    "            quality_distribution[quality_bin] += 1\n",
    "    \n",
    "    unique_combinations = len(param_signatures)\n",
    "    total_combinations = len(all_augmentation_params)\n",
    "    uniqueness_rate = unique_combinations / total_combinations if total_combinations > 0 else 0\n",
    "    \n",
    "    print(f\"   Total parameter sets: {total_combinations}\")\n",
    "    print(f\"   Unique combinations: {unique_combinations}\")\n",
    "    print(f\"   Duplicate combinations: {total_combinations - unique_combinations}\")\n",
    "    print(f\"   Uniqueness rate: {uniqueness_rate*100:.1f}%\")\n",
    "    \n",
    "    # Quality distribution analysis\n",
    "    print(f\"\\nüìä Quality Score Distribution:\")\n",
    "    for quality_bin in sorted(quality_distribution.keys(), key=float, reverse=True):\n",
    "        count = quality_distribution[quality_bin]\n",
    "        percentage = count / total_combinations * 100\n",
    "        print(f\"   Quality {quality_bin}: {count} sets ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Overall diversity assessment\n",
    "    if uniqueness_rate > 0.95:\n",
    "        print(f\"\\n   ‚úÖ Excellent parameter diversity ({uniqueness_rate*100:.1f}% unique)\")\n",
    "    elif uniqueness_rate > 0.85:\n",
    "        print(f\"\\n   ‚úÖ Very good parameter diversity ({uniqueness_rate*100:.1f}% unique)\")\n",
    "    elif uniqueness_rate > 0.7:\n",
    "        print(f\"\\n   ‚úÖ Good parameter diversity ({uniqueness_rate*100:.1f}% unique)\")\n",
    "    elif uniqueness_rate > 0.5:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Moderate parameter diversity ({uniqueness_rate*100:.1f}% unique)\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚ùå Poor parameter diversity ({uniqueness_rate*100:.1f}% unique)\")\n",
    "        print(f\"   üí° Consider increasing parameter variation ranges\")\n",
    "\n",
    "# Enhanced diversity recommendations\n",
    "print(f\"\\nüí° Enhanced Diversity Recommendations:\")\n",
    "recommendations = []\n",
    "\n",
    "if diversity_metrics:\n",
    "    # Check individual parameter diversity\n",
    "    low_diversity_params = []\n",
    "    for param_name, metrics in diversity_metrics.items():\n",
    "        diversity_score = (metrics['uniqueness_ratio'] + metrics['range_utilization'] + \n",
    "                         min(metrics['coefficient_of_variation'], 1.0)) / 3.0\n",
    "        if diversity_score < 0.5:\n",
    "            low_diversity_params.append(param_name)\n",
    "    \n",
    "    if low_diversity_params:\n",
    "        recommendations.append(f\"‚ö†Ô∏è  Consider increasing variation for: {', '.join(low_diversity_params)}\")\n",
    "    else:\n",
    "        recommendations.append(\"‚úÖ All parameters show good individual diversity\")\n",
    "    \n",
    "    # Check overall combination diversity\n",
    "    if uniqueness_rate < 0.8:\n",
    "        recommendations.append(\"üí° Consider adjusting parameter ranges to increase combination diversity\")\n",
    "    else:\n",
    "        recommendations.append(\"‚úÖ Excellent parameter combination diversity\")\n",
    "    \n",
    "    # Check quality distribution\n",
    "    high_quality_ratio = sum(count for quality_bin, count in quality_distribution.items() \n",
    "                           if float(quality_bin) >= 0.8) / total_combinations\n",
    "    \n",
    "    if high_quality_ratio < 0.7:\n",
    "        recommendations.append(\"‚ö†Ô∏è  Consider adjusting parameter ranges to improve quality scores\")\n",
    "    else:\n",
    "        recommendations.append(\"‚úÖ Good quality score distribution\")\n",
    "    \n",
    "    # Check synthetic variations\n",
    "    if validation_summary['total_generated'] < len(ready_images) * 2:\n",
    "        recommendations.append(\"üí° Consider increasing synthetic_per_real for more diversity\")\n",
    "    else:\n",
    "        recommendations.append(\"‚úÖ Good number of synthetic variations planned\")\n",
    "\n",
    "# Display recommendations\n",
    "for recommendation in recommendations:\n",
    "    print(f\"   {recommendation}\")\n",
    "\n",
    "# Store diversity analysis results\n",
    "diversity_analysis_results = {\n",
    "    'parameter_metrics': diversity_metrics,\n",
    "    'combination_analysis': {\n",
    "        'total_combinations': total_combinations,\n",
    "        'unique_combinations': unique_combinations,\n",
    "        'uniqueness_rate': uniqueness_rate,\n",
    "        'quality_distribution': dict(quality_distribution)\n",
    "    },\n",
    "    'recommendations': recommendations,\n",
    "    'overall_diversity_score': uniqueness_rate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration Snapshot and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration snapshot for reproducibility\n",
    "print(\"üíæ Creating configuration snapshot for reproducibility...\")\n",
    "\n",
    "try:\n",
    "    # Create comprehensive configuration snapshot\n",
    "    config_snapshot = {\n",
    "        'snapshot_info': {\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'pipeline_stage': 'Data Augmentation System',\n",
    "            'notebook_version': 'Enhanced v2.0',\n",
    "            'total_parameter_sets': validation_summary['total_generated']\n",
    "        },\n",
    "        'pipeline_config': PIPELINE_CONFIG,\n",
    "        'parameter_system_config': {\n",
    "            'parameter_ranges': param_system.parameter_ranges,\n",
    "            'validation_rules': param_system.validation_rules,\n",
    "            'prompt_variations': param_system.prompt_variations\n",
    "        },\n",
    "        'generation_settings': {\n",
    "            'synthetic_per_real': synthetic_per_real,\n",
    "            'optimal_batch_size': optimal_batch_size,\n",
    "            'memory_optimization_enabled': True,\n",
    "            'validation_enabled': True\n",
    "        },\n",
    "        'hardware_snapshot': PIPELINE_CONFIG.get('detected_hardware', {}),\n",
    "        'validation_summary': validation_summary,\n",
    "        'diversity_analysis': diversity_analysis_results\n",
    "    }\n",
    "    \n",
    "    # Save configuration snapshot\n",
    "    snapshot_path = Path(PIPELINE_CONFIG['paths']['data_dir']) / 'metadata' / 'config_snapshots'\n",
    "    snapshot_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snapshot_file = snapshot_path / f'augmentation_config_snapshot_{timestamp}.json'\n",
    "    \n",
    "    with open(snapshot_file, 'w') as f:\n",
    "        json.dump(config_snapshot, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Configuration snapshot saved: {snapshot_file}\")\n",
    "    \n",
    "    # Also save a \"latest\" snapshot for easy access\n",
    "    latest_snapshot_file = snapshot_path / 'augmentation_config_latest.json'\n",
    "    with open(latest_snapshot_file, 'w') as f:\n",
    "        json.dump(config_snapshot, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Latest configuration snapshot: {latest_snapshot_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create configuration snapshot: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Configuration snapshot creation failed, but processing can continue\")\n",
    "\n",
    "# Export enhanced augmentation results\n",
    "print(f\"\\nüíæ Exporting enhanced augmentation results...\")\n",
    "\n",
    "try:\n",
    "    # Filter valid parameter sets for export\n",
    "    valid_param_sets = [\n",
    "        param_set for param_set in all_augmentation_params\n",
    "        if param_set.get('validation', {}).get('valid', False)\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìä Export Preparation:\")\n",
    "    print(f\"   Total parameter sets generated: {len(all_augmentation_params)}\")\n",
    "    print(f\"   Valid parameter sets for export: {len(valid_param_sets)}\")\n",
    "    print(f\"   Filtered out (invalid): {len(all_augmentation_params) - len(valid_param_sets)}\")\n",
    "    \n",
    "    # Create comprehensive augmentation report\n",
    "    augmentation_report = {\n",
    "        \"augmentation_info\": {\n",
    "            \"pipeline_stage\": \"Enhanced Data Augmentation System\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"notebook_version\": \"Enhanced v2.0\",\n",
    "            \"source_images\": len(ready_images),\n",
    "            \"synthetic_per_real_target\": synthetic_per_real,\n",
    "            \"parameter_system_type\": \"FLUXOptimizedParameterSystem\",\n",
    "            \"flux_optimized\": True,\n",
    "            \"memory_optimized\": True\n",
    "        },\n",
    "        \"generation_summary\": {\n",
    "            \"total_parameter_sets_generated\": validation_summary['total_generated'],\n",
    "            \"valid_parameter_sets\": validation_summary['valid_parameters'],\n",
    "            \"invalid_parameter_sets\": validation_summary['invalid_parameters'],\n",
    "            \"quality_warnings\": validation_summary['quality_warnings'],\n",
    "            \"average_quality_score\": validation_summary['average_quality_score'],\n",
    "            \"generation_success_rate_percent\": validation_summary['valid_parameters'] / validation_summary['total_generated'] * 100 if validation_summary['total_generated'] > 0 else 0\n",
    "        },\n",
    "        \"parameter_statistics\": {\n",
    "            param_name: {\n",
    "                \"mean\": float(metrics['mean']),\n",
    "                \"std\": float(metrics['std']),\n",
    "                \"min\": float(metrics['min']),\n",
    "                \"max\": float(metrics['max']),\n",
    "                \"median\": float(metrics['median']),\n",
    "                \"unique_values\": metrics['unique_values'],\n",
    "                \"uniqueness_ratio\": metrics['uniqueness_ratio'],\n",
    "                \"range_utilization\": metrics['range_utilization']\n",
    "            } for param_name, metrics in diversity_metrics.items()\n",
    "        },\n",
    "        \"diversity_analysis\": diversity_analysis_results,\n",
    "        \"memory_optimization\": {\n",
    "            \"optimal_batch_size_used\": optimal_batch_size,\n",
    "            \"initial_memory_usage\": initial_memory,\n",
    "            \"final_memory_usage\": final_memory\n",
    "        },\n",
    "        \"configuration_snapshot_path\": str(snapshot_file) if 'snapshot_file' in locals() else None\n",
    "    }\n",
    "    \n",
    "    # Save augmentation report\n",
    "    metadata_path = Path(PIPELINE_CONFIG['paths']['data_dir']) / 'metadata'\n",
    "    metadata_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    augmentation_report_path = metadata_path / 'augmentation_report_enhanced.json'\n",
    "    with open(augmentation_report_path, 'w') as f:\n",
    "        json.dump(augmentation_report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced augmentation report saved: {augmentation_report_path}\")\n",
    "    \n",
    "    # Create enhanced ControlNet generation batch\n",
    "    controlnet_batch = {\n",
    "        \"batch_info\": {\n",
    "            \"stage\": \"FLUX ControlNet Synthetic Generation\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"source_stage\": \"Enhanced Data Augmentation System\",\n",
    "            \"total_parameter_sets\": len(valid_param_sets),\n",
    "            \"ready_for_processing\": len(valid_param_sets) > 0,\n",
    "            \"flux_optimized\": True,\n",
    "            \"enhanced_validation\": True\n",
    "        },\n",
    "        \"generation_parameters\": valid_param_sets,\n",
    "        \"augmentation_summary\": {\n",
    "            \"source_images\": len(ready_images),\n",
    "            \"target_synthetic_per_real\": synthetic_per_real,\n",
    "            \"actual_parameter_sets\": len(valid_param_sets),\n",
    "            \"validation_passed\": validation_summary['invalid_parameters'] == 0,\n",
    "            \"average_quality_score\": validation_summary['average_quality_score'],\n",
    "            \"diversity_score\": diversity_analysis_results['overall_diversity_score']\n",
    "        },\n",
    "        \"flux_optimization\": {\n",
    "            \"parameter_ranges_optimized\": True,\n",
    "            \"guidance_scale_range\": param_system.parameter_ranges['guidance_scale'],\n",
    "            \"inference_steps_optimized\": param_system.parameter_ranges['num_inference_steps'],\n",
    "            \"controlnet_conditioning_optimized\": param_system.parameter_ranges['controlnet_conditioning_scale']\n",
    "        },\n",
    "        \"pipeline_config\": PIPELINE_CONFIG,\n",
    "        \"configuration_snapshot_path\": str(snapshot_file) if 'snapshot_file' in locals() else None\n",
    "    }\n",
    "    \n",
    "    controlnet_batch_path = metadata_path / 'controlnet_batch_enhanced.json'\n",
    "    with open(controlnet_batch_path, 'w') as f:\n",
    "        json.dump(controlnet_batch, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced ControlNet batch saved: {controlnet_batch_path}\")\n",
    "    \n",
    "    # Update pipeline configuration with augmentation info\n",
    "    PIPELINE_CONFIG['augmentation_info'] = {\n",
    "        'completed': True,\n",
    "        'enhanced_version': True,\n",
    "        'flux_optimized': True,\n",
    "        'parameter_sets_generated': len(valid_param_sets),\n",
    "        'source_images': len(ready_images),\n",
    "        'average_quality_score': validation_summary['average_quality_score'],\n",
    "        'diversity_score': diversity_analysis_results['overall_diversity_score'],\n",
    "        'augmentation_report_path': str(augmentation_report_path),\n",
    "        'controlnet_batch_path': str(controlnet_batch_path),\n",
    "        'config_snapshot_path': str(snapshot_file) if 'snapshot_file' in locals() else None,\n",
    "        'last_updated': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save updated config\n",
    "    config_manager.save_config(PIPELINE_CONFIG)\n",
    "    print(f\"‚úÖ Pipeline configuration updated\")\n",
    "    \n",
    "    print(f\"\\nüìã Export Summary:\")\n",
    "    print(f\"   Enhanced augmentation report: {augmentation_report_path}\")\n",
    "    print(f\"   Enhanced ControlNet batch: {controlnet_batch_path}\")\n",
    "    print(f\"   Configuration snapshot: {snapshot_file if 'snapshot_file' in locals() else 'Not created'}\")\n",
    "    print(f\"   Parameter sets ready for FLUX ControlNet: {len(valid_param_sets)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during export: {e}\")\n",
    "    print(f\"‚ùå Export failed: {e}\")\n",
    "    # Save partial results\n",
    "    error_handler.save_partial_results(\n",
    "        {'parameter_sets': all_augmentation_params, 'validation_summary': validation_summary},\n",
    "        'augmentation_export_error'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Enhanced Pipeline Status and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced pipeline status check and final validation\n",
    "print(\"üîç Performing enhanced pipeline status check...\")\n",
    "\n",
    "# Comprehensive status checks\n",
    "status_checks = {\n",
    "    \"Enhanced Configuration Loaded\": 'PIPELINE_CONFIG' in locals() and PIPELINE_CONFIG is not None,\n",
    "    \"FLUX Parameter System Initialized\": 'param_system' in locals() and isinstance(param_system, FLUXOptimizedParameterSystem),\n",
    "    \"Parameters Generated\": len(all_augmentation_params) > 0,\n",
    "    \"Parameters Validated\": validation_summary['total_generated'] > 0,\n",
    "    \"Valid Parameters Available\": validation_summary['valid_parameters'] > 0,\n",
    "    \"Diversity Analysis Complete\": 'diversity_analysis_results' in locals(),\n",
    "    \"Memory Optimization Applied\": 'optimal_batch_size' in locals(),\n",
    "    \"Configuration Snapshot Created\": 'snapshot_file' in locals(),\n",
    "    \"Results Exported\": 'controlnet_batch_path' in locals() and Path(controlnet_batch_path).exists(),\n",
    "    \"Pipeline Config Updated\": 'augmentation_info' in PIPELINE_CONFIG\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced Pipeline Status Check:\")\n",
    "print(\"=\" * 60)\n",
    "all_ready = True\n",
    "for check, status in status_checks.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{check:<40}: {status_icon}\")\n",
    "    if not status:\n",
    "        all_ready = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "if all_ready and validation_summary['valid_parameters'] > 0:\n",
    "    print(\"üéâ ENHANCED DATA AUGMENTATION COMPLETED SUCCESSFULLY!\")\n",
    "    \n",
    "    print(f\"\\nüìä Final Enhanced Summary:\")\n",
    "    print(f\"   ‚úÖ {validation_summary['valid_parameters']} valid FLUX-optimized parameter sets\")\n",
    "    print(f\"   üìà Parameter diversity: {diversity_analysis_results['overall_diversity_score']*100:.1f}% unique combinations\")\n",
    "    print(f\"   üéØ Average quality score: {validation_summary['average_quality_score']:.3f}\")\n",
    "    print(f\"   üöÄ Ready for FLUX ControlNet generation\")\n",
    "    print(f\"   üìÅ Source images: {len(ready_images)}\")\n",
    "    print(f\"   üíæ Memory optimization: Batch size {optimal_batch_size}\")\n",
    "    \n",
    "    # Enhanced quality assessment\n",
    "    print(f\"\\nüéØ Enhanced Quality Assessment:\")\n",
    "    if validation_summary['invalid_parameters'] == 0:\n",
    "        print(f\"   ‚úÖ All parameter sets passed validation\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {validation_summary['invalid_parameters']} parameter sets failed validation\")\n",
    "    \n",
    "    if validation_summary['average_quality_score'] >= 0.9:\n",
    "        print(f\"   ‚úÖ Excellent parameter quality (score: {validation_summary['average_quality_score']:.3f})\")\n",
    "    elif validation_summary['average_quality_score'] >= 0.8:\n",
    "        print(f\"   ‚úÖ Very good parameter quality (score: {validation_summary['average_quality_score']:.3f})\")\n",
    "    elif validation_summary['average_quality_score'] >= 0.7:\n",
    "        print(f\"   ‚úÖ Good parameter quality (score: {validation_summary['average_quality_score']:.3f})\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Parameter quality could be improved (score: {validation_summary['average_quality_score']:.3f})\")\n",
    "    \n",
    "    if validation_summary['quality_warnings'] == 0:\n",
    "        print(f\"   ‚úÖ No quality warnings - all parameters in optimal ranges\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {validation_summary['quality_warnings']} quality warnings (parameters valid but suboptimal)\")\n",
    "    \n",
    "    # Enhanced diversity assessment\n",
    "    diversity_score = diversity_analysis_results['overall_diversity_score']\n",
    "    print(f\"\\nüé≤ Enhanced Diversity Assessment:\")\n",
    "    if diversity_score > 0.95:\n",
    "        print(f\"   ‚úÖ Exceptional parameter diversity ({diversity_score*100:.1f}% unique)\")\n",
    "    elif diversity_score > 0.85:\n",
    "        print(f\"   ‚úÖ Excellent parameter diversity ({diversity_score*100:.1f}% unique)\")\n",
    "    elif diversity_score > 0.7:\n",
    "        print(f\"   ‚úÖ Very good parameter diversity ({diversity_score*100:.1f}% unique)\")\n",
    "    elif diversity_score > 0.5:\n",
    "        print(f\"   ‚úÖ Good parameter diversity ({diversity_score*100:.1f}% unique)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Parameter diversity could be improved ({diversity_score*100:.1f}% unique)\")\n",
    "    \n",
    "    # FLUX optimization status\n",
    "    print(f\"\\nüîß FLUX.1-dev Optimization Status:\")\n",
    "    print(f\"   ‚úÖ Parameter ranges optimized for FLUX.1-dev\")\n",
    "    print(f\"   ‚úÖ Guidance scale range: {param_system.parameter_ranges['guidance_scale']['min']:.1f}-{param_system.parameter_ranges['guidance_scale']['max']:.1f}\")\n",
    "    print(f\"   ‚úÖ Inference steps optimized: {param_system.parameter_ranges['num_inference_steps']['values']}\")\n",
    "    print(f\"   ‚úÖ ControlNet conditioning optimized\")\n",
    "    \n",
    "    # Memory optimization status\n",
    "    print(f\"\\nüíæ Memory Optimization Status:\")\n",
    "    print(f\"   ‚úÖ Dynamic batch sizing applied (size: {optimal_batch_size})\")\n",
    "    print(f\"   ‚úÖ Memory monitoring enabled\")\n",
    "    print(f\"   ‚úÖ Garbage collection optimized\")\n",
    "    \n",
    "    # Reproducibility status\n",
    "    print(f\"\\nüì∏ Reproducibility Status:\")\n",
    "    if 'snapshot_file' in locals():\n",
    "        print(f\"   ‚úÖ Configuration snapshot created\")\n",
    "        print(f\"   ‚úÖ All settings preserved for reproduction\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Configuration snapshot creation failed\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready to proceed to enhanced notebook:\")\n",
    "    print(f\"   üìì 05_FLUX_ControlNet_Synthetic_Generation_Enhanced.ipynb\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå ENHANCED DATA AUGMENTATION INCOMPLETE\")\n",
    "    print(\"\\nüîß Issues to resolve:\")\n",
    "    for check, status in status_checks.items():\n",
    "        if not status:\n",
    "            print(f\"   - {check}\")\n",
    "    \n",
    "    if validation_summary['valid_parameters'] == 0:\n",
    "        print(f\"\\nüí° Enhanced troubleshooting suggestions:\")\n",
    "        recovery_instructions = error_handler.provide_recovery_instructions(\n",
    "            \"parameter_generation\", \"augmentation\"\n",
    "        )\n",
    "        for instruction in recovery_instructions[:5]:\n",
    "            print(f\"   - {instruction}\")\n",
    "\n",
    "print(f\"\\nüìö Enhanced Pipeline Progress:\")\n",
    "print(f\"   ‚úÖ 01_Setup_and_Installation_Enhanced.ipynb\")\n",
    "print(f\"   ‚úÖ 02_Data_Import_and_Preprocessing_Enhanced.ipynb\")\n",
    "print(f\"   ‚úÖ 03_Depth_Anything_V2_Extraction_Enhanced.ipynb\")\n",
    "print(f\"   ‚úÖ 04_Data_Augmentation_System_Enhanced.ipynb (current)\")\n",
    "print(f\"   ‚è≥ 05_FLUX_ControlNet_Synthetic_Generation_Enhanced.ipynb (next)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Save enhanced execution log\n",
    "execution_log = {\n",
    "    \"notebook\": \"04_Data_Augmentation_System_Enhanced.ipynb\",\n",
    "    \"version\": \"Enhanced v2.0\",\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"status\": \"completed\" if all_ready else \"incomplete\",\n",
    "    \"flux_optimized\": True,\n",
    "    \"memory_optimized\": True,\n",
    "    \"enhanced_features\": [\n",
    "        \"FLUX.1-dev parameter optimization\",\n",
    "        \"Advanced parameter validation\",\n",
    "        \"Memory-efficient generation\",\n",
    "        \"Enhanced diversity analysis\",\n",
    "        \"Configuration snapshots\",\n",
    "        \"Comprehensive quality assessment\"\n",
    "    ],\n",
    "    \"summary\": {\n",
    "        \"parameter_sets_generated\": validation_summary['total_generated'],\n",
    "        \"valid_parameter_sets\": validation_summary['valid_parameters'],\n",
    "        \"validation_success_rate\": validation_summary['valid_parameters'] / validation_summary['total_generated'] * 100 if validation_summary['total_generated'] > 0 else 0,\n",
    "        \"average_quality_score\": validation_summary['average_quality_score'],\n",
    "        \"parameter_diversity_rate\": diversity_analysis_results['overall_diversity_score'] * 100,\n",
    "        \"memory_optimization_batch_size\": optimal_batch_size,\n",
    "        \"ready_for_flux_controlnet\": validation_summary['valid_parameters'] > 0\n",
    "    },\n",
    "    \"hardware_info\": PIPELINE_CONFIG.get('detected_hardware', {}),\n",
    "    \"memory_usage\": {\n",
    "        \"initial\": initial_memory,\n",
    "        \"final\": final_memory\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save execution log\n",
    "logs_path = Path(PIPELINE_CONFIG['paths']['data_dir']) / 'logs'\n",
    "logs_path.mkdir(parents=True, exist_ok=True)\n",
    "log_path = logs_path / f'augmentation_enhanced_execution_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(execution_log, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìù Enhanced execution log saved: {log_path}\")\n",
    "\n",
    "# Display final memory cleanup\n",
    "print(f\"\\nüßπ Performing final memory cleanup...\")\n",
    "memory_optimizer.cleanup_variables([all_augmentation_params, parameter_statistics, diversity_metrics])\n",
    "final_cleanup_memory = monitor_memory()\n",
    "print(f\"   Final memory usage: System {final_cleanup_memory.get('system_percent', 0):.1f}%\")\n",
    "if 'gpu_percent' in final_cleanup_memory:\n",
    "    print(f\"   GPU: {final_cleanup_memory.get('gpu_percent', 0):.1f}%\")\n",
    "\n",
    "print(f\"\\nüéâ Enhanced Data Augmentation System completed successfully!\")"
   ]
  }
 ]
}